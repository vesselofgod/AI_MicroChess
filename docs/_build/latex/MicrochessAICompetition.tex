%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional
  \DeclareUnicodeCharacter{"00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{"2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{"2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{"2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{"251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{"2572}{\textbackslash}
 \else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{2572}{\textbackslash}
 \fi
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Sonny]{fncychap}
\usepackage{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{이전 페이지에서 계속}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\usepackage{kotex}

\title{Microchess AI Competition Documentation}
\date{2018년 05월 02일}
\release{0.1.0}
\author{Hyunsoo Park}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{출시}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}

\begin{itemize}
\item {} 
이 플랫폼은 Microchess를 플레이하는 AI 개발 경진대회에 사용하기 위한 목적으로 개발되었다.

\item {} 
이 플랫폼은 크게 Microchss 게임과 예제 AI들 그리고 실행 스크립트로 구성되어 있으며, Windows 10 환경에서 python 3.5 (anaconda)를 사용하여 개발 테스트 되어있다.

\item {} 
플랫폼이 python 으로 구현되어 있기 때문에 python 으로 AI를 구현하는 것이 권장되지만, 다른 언어로 구현한 AI를 사용할 수 도 있다.

\end{itemize}


\chapter{빠른시작}
\label{\detokenize{index:chess-ai-compeitition-platform}}\label{\detokenize{index:id1}}

\section{설치방법}
\label{\detokenize{index:id2}}
이 문서에서는 플랫폼 개발환경과 같은 Windows 10 + python 3.5 (anaconda)를 기준으로 설명한다.
하지만, 대부분의 구성요소들이 순수 python 으로 구현되어 있기 때문에, 다른 운영체제에서 큰 문제없이 실행
가능하다.

\sphinxstylestrong{설치 순서}
\begin{enumerate}
\item {} 
플랫폼 다운로드 및 압축해제

\item {} 
anaconda 다운로드 및 설치 (\sphinxurl{https://www.anaconda.com/download/})

\item {} 
가상환경 생성 및 활성화:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
(base) C:\PYGZbs{}Users\PYGZbs{}user\PYGZbs{}MicrochessAICompetition\PYGZgt{} conda create \textendash{}n mchess python=3.5
(base) C:\PYGZbs{}Users\PYGZbs{}user\PYGZbs{}MicrochessAICompetition\PYGZgt{} activate mchess
(mchess) C:\PYGZbs{}Users\PYGZbs{}user\PYGZbs{}MicrochessAICompetition\PYGZgt{}
\end{sphinxVerbatim}

\item {} 
주요 모듈 설치:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{conda} \PYG{n}{install} \PYG{n}{numpy} \PYG{n}{scipy} \PYG{n}{ipython} \PYG{n}{tqdm} \PYG{n}{pyyaml} \PYG{n}{mkl} \PYG{n}{matplotlib}
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{conda} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{n}{c} \PYG{n}{CogSci} \PYG{n}{pygame}
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{pip} \PYG{n}{install} \PYG{n}{sqlitedict}
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{pip} \PYG{n}{install} \PYG{n}{visdom}  \PYG{c+c1}{\PYGZsh{} visdom 설치}
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{conda} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{n}{c} \PYG{n}{peterjc123} \PYG{n}{pytorch}\PYG{o}{\PYGZhy{}}\PYG{n}{cpu}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{o}{.}\PYG{l+m+mi}{1}  \PYG{c+c1}{\PYGZsh{} pytorch 설치 (CPU, Windows 용)}
\end{sphinxVerbatim}

\end{enumerate}


\section{AI 구현}
\label{\detokenize{index:ai}}
AI를 구현할 때는 BaseAgent (agents/\_\_init\_\_.py)를 상속받아, reset, act, close 세 가지 메소드를 구현한다.
가장 간단한 예인 Random AI (agents/basic/random\_agent.py)는 다음과 같이 구현한다.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{random}
\PYG{k+kn}{from} \PYG{n+nn}{agents} \PYG{k+kn}{import} \PYG{n}{BaseAgent}
    

\PYG{k}{class} \PYG{n+nc}{RandomAgent}\PYG{p}{(}\PYG{n}{BaseAgent}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Random AI}

\PYG{l+s+sd}{    \PYGZhy{} 무작위로 행동을 결정하는 예제}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{k}{def} \PYG{n+nf}{reset}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{act}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{moves} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{state}\PYG{o}{.}\PYG{n}{legal\PYGZus{}moves}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{moves}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{close}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{pass}
\end{sphinxVerbatim}

이 세 가지 메소드는 플랫폼에의해 호출된다.

reset와 close는 AI의 초기화와 종료를 처리한다. 게임을 시작할 때와 끝낼 때 한번씩만 실행된다.
act는 AI의 턴마다 한번씩 실행된다. 현재 게임상태(state)를 입력받아, 다음 수(move)를 출력해야한다.


\section{AI끼리 게임 플레이}
\label{\detokenize{index:id3}}
AI끼리 게임을 플레이할 때는 scripts/run\_game.py 을 사용한다. Random AI (white)와 One Step Search AI (black)끼리 플레이할 때는 다음과 같이 실행한다.:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{scripts}\PYG{o}{/}\PYG{n}{run\PYGZus{}game}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{white}\PYG{o}{=}\PYG{n}{agents}\PYG{o}{.}\PYG{n}{basic}\PYG{o}{.}\PYG{n}{random\PYGZus{}agent}\PYG{o}{.}\PYG{n}{RandomAgent} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{black}\PYG{o}{=}\PYG{n}{agents}\PYG{o}{.}\PYG{n}{search}\PYG{o}{.}\PYG{n}{one\PYGZus{}step\PYGZus{}search\PYGZus{}agent}\PYG{o}{.}\PYG{n}{OneStepSearchAgent}
\end{sphinxVerbatim}

white와 black에 AI의 경로를 지정해 주면 게임을 바로 실행하고, 결과를 출력해 준다.

One Step AI는 다음 수의 결과를 고려하는 간단한 AI이다. 강력한 AI는 아니지만, Random AI는 쉽게 이길 수 있다.
실행한 결과는 다음과 같이 나타난다.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
[2018\PYGZhy{}04\PYGZhy{}23 15:40:07,079 DEBUG] White: agents.basic.random\PYGZus{}agent.RandomAgent\PYGZhy{}True
[2018\PYGZhy{}04\PYGZhy{}23 15:40:07,079 DEBUG] White: board value 0.500
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
k n b r
p . . .
. . . .
. . . P
R B N K
[2018\PYGZhy{}04\PYGZhy{}23 15:40:07,093 DEBUG] White: 400.0 sec. remain
[2018\PYGZhy{}04\PYGZhy{}23 15:40:07,093 DEBUG] White: move b1d3
[2018\PYGZhy{}04\PYGZhy{}23 15:40:07,093 DEBUG] 8/8/8/knbr4/p7/3B4/3P4/R1NK4 b Kk \PYGZhy{} 1 1
...  (생략)
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,443 DEBUG] White: move a1a2
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,443 DEBUG] 8/8/8/8/k7/8/K7/2b5 b \PYGZhy{} \PYGZhy{} 0 14
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,443 DEBUG] [0, 1]
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,443 DEBUG] Black Win
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
. . . .
k . . .
. . . .
K . . .
. . b .
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,449 DEBUG] Score: [0.000 1.000]
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,449 DEBUG] turns: 26
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,449 DEBUG] Game end: True
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,449 DEBUG] checkmate: False
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,449 DEBUG] Stalemate: False
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,449 DEBUG] Insufficient material: True
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,449 DEBUG] 57 moves: False
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,449 DEBUG] 5\PYGZhy{}fold: False
[2018\PYGZhy{}04\PYGZhy{}23 18:05:27,450 DEBUG] Black win: [0, 1]
\end{sphinxVerbatim}

4\textasciitilde{}8번째 줄 처럼 매 턴마다, 게임 상태를 텍스트 상태로 출력한다. white는 대문자로 표시하고, black은 소문자로 표시한다.

23번째 줄부터 게임 결과를 출력한 것이다. Score의 첫번째 숫자는 white의 점수를 나타내고, 두 번째 숫자는 black의 점수를 나타낸다.
(특별한 경우가 아니면 플랫폼의 나머지 부분에서도 white, black 순서로 출력함)
게임에 승리하면 1.0, 패배하면 0.0, 그리고 비기면 0.5점을 얻는다.

일반 Chess (Microchess 포함)와 달리 이 플랫폼에서는 승패가 결정되지 않으면,
게임이 종료되었을 때 남아있는 기물의 점수를 계산하여 최종 승패를 판단한다.
따라서, 기존 무승부 조건을 만족한 상태에서 기물의 점수도 동일한 경우만 무승부가 가능하다.

turns는 전체 게임의 턴 수를 보여주고, 그 다음 줄의 game end는 게임이 정상적으로 종료되었는지 여부를 알려준다.
그 다음부터는 게임이 종료된 이유(checkmate, stalemate, 등)를 알려준다.

AI 끼리의 성능을 평가할 때는 benchmark 옵션을 사용한다. benchmark 옵션을 사용하면, 지정된 white와 black 옵션과 상관없이
총 20게임 (지정된 white와 black으로 10게임, white와 black을 뒤집어서 10게임)을 플레이하고 전체 승률을 출력해준다.


\section{AI와 게임 플레이}
\label{\detokenize{index:id4}}
개발한 AI의 성능을 평가하기 위해 직접 게임을 플레이해봐야 할 필요가 있다.
다음과 같이 플랫폼을 실행하면 직접 게임을 플레이 해 볼 수 있다.:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{scripts}\PYG{o}{/}\PYG{n}{run\PYGZus{}game}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{white}\PYG{o}{=}\PYG{n}{human} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{black}\PYG{o}{=}\PYG{n}{one\PYGZus{}step\PYGZus{}search}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{microchess-human}.png}
\caption{게임 플레이 인터페이스}\label{\detokenize{index:play-interface}}\label{\detokenize{index:id6}}\end{figure}

일반 AI대신 human (agents.basic.human.Player)을 인자로 주면, 게임을 직접 플레이 할 수 있는 인터페이스 {\hyperref[\detokenize{index:play-interface}]{\sphinxcrossref{\DUrole{std,std-ref}{게임 플레이 인터페이스}}}} 가 활성화 된다.


\section{게시판}
\label{\detokenize{index:id5}}\begin{itemize}
\item {} \begin{description}
\item[{경진대회 운영 게시판}] \leavevmode\begin{itemize}
\item {} 
\sphinxurl{https://groups.google.com/forum/\#!forum/microchess\_ai}

\end{itemize}

\end{description}

\end{itemize}


\chapter{Tutorial}
\label{\detokenize{index:tutorial}}

\section{Microchess AI 경진대회}
\label{\detokenize{01-microchess_ai_competition::doc}}\label{\detokenize{01-microchess_ai_competition:microchess-ai}}
본 경진대회는 Game AI에 대한 학생들의 흥미를 유발하기 위해 기획되었다.
따라서, 학생들에게 주어진 각종 제약(시간, 컴퓨팅 자원)안에서 AI를 구현하고 실험하기에 너무 어렵지 않도록,
규모가 작을 필요가 있었다. 하지만 너무 쉽게 해결할 수 없도록 너무 단순하지는 않아야 했다.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{microchess}.png}
\caption{Microchess}\label{\detokenize{01-microchess_ai_competition:microchess}}\label{\detokenize{01-microchess_ai_competition:id14}}\end{figure}

{\hyperref[\detokenize{01-microchess_ai_competition:microchess}]{\sphinxcrossref{\DUrole{std,std-ref}{Microchess}}}} 는 일반 체스의 일부분 만을 사용하는 미니 체스 중 하나로써,
기존 체스에 비해 작은 탐색 공간을 가지고 있으면서도, 모든 경우의 수를 따져보는 것은 거의 불가능했기 때문에
경진대회 종목으로 선정되었다. 기존 체스는 대략 \(35^{80}\) 가지 경우의 수를 가지고 있는 것으로
알려져 있다. 반면 Microchess는 훨씬 작은 \(10^{20}\) 규모로 추정된다.
비록 문제의 규모는 매우 축소되었지만, 모든 경우의 수를 따지는 것은 천문학적인 규모이기 때문에,
다음 수를 결정하기 위해서는 보다 효율적인 방법이 필요하다.

Microchess는 일반 체스가 가지는 특징(완전 정보공개, 불확실성 없음)을 가지지만, 일부 다른 점이 있다.
\begin{itemize}
\item {} 
\(4 \times 5\) 미니체스

\item {} 
탐색공간이 매우 크지는 않지만, 쉽게 해결은 어려움

\item {} 
플레이어는 퀸을 제외한 기물을 하나씩 가지고 있음

\item {} 
기물의 움직임은 일반 체스와 동일

\item {} 
Castling 가능

\end{itemize}


\subsection{경진대회 규칙}
\label{\detokenize{01-microchess_ai_competition:id1}}
이 경진대회에서는 대부분 일반 체스의 규칙을 따르지만, 게임 시간을 절약하고, 무승부 %
\begin{footnote}[1]\sphinxAtStartFootnote
일반 체스는 무승부 비율이 높음 약 30\textasciitilde{}35 \%
%
\end{footnote} 를 줄이기 위해,
AI 경진대회에 적합하게 몇 가지 규칙을 변경하였다.

일반 체스는 무승부가 전체 게임의 30\%가 넘을 정도로 무승부가 많은 게임이다. (Black의 승률보다 높음)
Microchess에서 무승부 비율은 명확하게 조사된 바가 없지만, 무승부 확률이 동일하거나, 높은 것으로 추정된다.
일정 수준의 성능을 보이는 AI들끼리의 게임은 대부분 무승부로 끝나는 경향이 있다.
따라서, 이 경진대회에서는 보다 쉽게 승패를 가릴 수 있도록 무승부 조건을 강화하였다.

또한, 게임 플레이 데이터로 의사결정 모델을 학습할 때, 무승부가 되었을 때는 유용한 정보를 알아내기 어렵기 때문에,
무승부 비율이 줄어들면 기계학습 알고리즘을 사용하여 AI를 학습할 때도 도움이된다.
\begin{itemize}
\item {} \begin{description}
\item[{최대 80턴 으로 제한}] \leavevmode\begin{itemize}
\item {} 
대부분의 게임은 50턴 이내에서 종료됨

\item {} 
AI 들의 경우 무의미한 수를 반복하면서 지나치게 오랬동안 게임을 할 때도 있기 때문에 최대 턴에 제한을 둠

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{플레이어마다 전체 게임 400초 시간제한을 둠}] \leavevmode\begin{itemize}
\item {} 
턴 마다 시간제한은 없지만, 한 게임에서 400초를 모두 소모하면 패배

\item {} 
플레이어가 한 턴에 약 10초를 소모할 것이라고 가정해서 400초 시간제한을 둔 것

\item {} \begin{description}
\item[{전체 400초를 초과하지 않는 범위내에서 플레이어가 임의로 조정가능함}] \leavevmode\begin{itemize}
\item {} 
최대 80턴까지 게임하면 전체 게임 시간은 800초

\item {} 
400초 = (80턴 \(\times\) 10초) / 2플레이어

\item {} 
한 턴에 10초 가정

\end{itemize}

\end{description}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{승리/패배 조건}] \leavevmode\begin{itemize}
\item {} 
기존 체스의 승리조건을 만족하면 승리(예. 상대 킹을 체크메이트)

\item {} 
최대 턴을 초과하면, 남은 기물에 따라 점수를 계산하여 높은 쪽이 승리함

\item {} 
기존 무승부 규칙 %
\begin{footnote}[2]\sphinxAtStartFootnote
무승부 규칙: 기물이 부족해 체크메이트 불가, 어떤 기물도 이동 불가, 50턴 동안 폰을 이동하지 않거나, 기물이 잡히지 않음, 5번 이상 똑같은 수 반복, \sphinxurl{https://en.wikipedia.org/wiki/Chess\#Draw}
%
\end{footnote} 에 따라 무승부가 되면, 남은 기물의 점수를 계산하여 승패를 판단함

\item {} 
최대 턴 초과 또는 무승부로 게임 종료 후, 기물 점수도 동일한 경우에만 무승부로 판단함

\item {} 
한 플레이어가 먼저 400초 시간제한을 초과하면 패배로 판단함

\end{itemize}

\end{description}

\end{itemize}


\subsection{점수계산}
\label{\detokenize{01-microchess_ai_competition:id4}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{score-table}.png}
\caption{점수 표}\label{\detokenize{01-microchess_ai_competition:score-table}}\label{\detokenize{01-microchess_ai_competition:id15}}\end{figure}

무승부로 게임이 종료되면, {\hyperref[\detokenize{01-microchess_ai_competition:score-table}]{\sphinxcrossref{\DUrole{std,std-ref}{점수 표}}}} %
\begin{footnote}[3]\sphinxAtStartFootnote
사용하는 전략에 따라, 기물의 점수에 이견이 있지만, 많은 Chess AI들이 이 테이블의 점수를 따름
%
\end{footnote} 을 참조하여 남은 기물의 점수를 계산하여 게임의 승패를 판단한다.

다음은 {\hyperref[\detokenize{01-microchess_ai_competition:score-example}]{\sphinxcrossref{\DUrole{std,std-ref}{점수 계산 예}}}} 를 보여준다.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{microchess-score}.png}
\caption{점수 계산 예}\label{\detokenize{01-microchess_ai_competition:score-example}}\label{\detokenize{01-microchess_ai_competition:id16}}\end{figure}
\begin{itemize}
\item {} 
백: ♔(K)+♖(R)+♘(N)+♙(P)= 4+5+3+1 = 13

\item {} 
흑: ♚(k)+♟(p)= 4+1 = 5

\end{itemize}


\subsection{주의사항}
\label{\detokenize{01-microchess_ai_competition:id6}}\begin{itemize}
\item {} \begin{description}
\item[{실격패 조건}] \leavevmode\begin{itemize}
\item {} 
400초를 모두 모소하는 경우

\item {} 
프로그램이 종료될 수준의 예외/에러 발생

\item {} 
유효하지 않은 수를 둘 경우

\item {} \begin{description}
\item[{부정행위를 할 경우}] \leavevmode\begin{itemize}
\item {} 
파일 시스템에서 자신의 AI에게 허용되지 않은 경로에 접근하는 경우

\item {} 
네트워크에 연결하는 경우

\item {} 
상대 AI 혹은 경진대회 시스템에 영향을 주기 위해 별도의 thread, process 를 실행하는 경우

\end{itemize}

\end{description}

\end{itemize}

\end{description}

\end{itemize}


\subsection{운영 게시판}
\label{\detokenize{01-microchess_ai_competition:id7}}\begin{itemize}
\item {} \begin{description}
\item[{Google Groups를 이용}] \leavevmode\begin{itemize}
\item {} 
\sphinxurl{https://groups.google.com/forum/\#!forum/microchess\_ai}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{운영목적}] \leavevmode\begin{itemize}
\item {} 
공지사항 전달

\item {} 
질의응답 게시판

\item {} 
과제제출용

\end{itemize}

\end{description}

\end{itemize}


\subsection{최종 우승자 결정}
\label{\detokenize{01-microchess_ai_competition:id8}}
Double Round-Robin Tournament (Full League)
\begin{itemize}
\item {} 
모든 AI는 서로 흑과 백으로 여러 번 %
\begin{footnote}[4]\sphinxAtStartFootnote
정확한 경기횟수는 미정, 일반 체스는 두 번 이지만 AI 체스에서는 두 번만으로 평가가 어려울 수 있음
%
\end{footnote} 경기를 진행

\item {} \begin{description}
\item[{가장 많은 점수를 획득한 AI가 우승}] \leavevmode\begin{itemize}
\item {} 
점수: 승(+1), 패(0), 무승부(+1/2)

\end{itemize}

\end{description}

\end{itemize}

\newpage


\section{Microchess AI 플랫폼}
\label{\detokenize{02-microchess_ai_platform::doc}}\label{\detokenize{02-microchess_ai_platform:microchess-ai}}

\subsection{개요}
\label{\detokenize{02-microchess_ai_platform:id1}}\begin{itemize}
\item {} \begin{description}
\item[{Microchess 플랫폼 구현}] \leavevmode\begin{itemize}
\item {} 
일반 체스 플랫폼인 python-chess %
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxurl{https://github.com/niklasf/python-chess}
%
\end{footnote} 을 수정하여 구현

\end{itemize}

\end{description}

\item {} 
Python으로 AI를 작성할 수 있는 기반을 제공함

\item {} \begin{description}
\item[{주요 AI 예제 네 가지 제공}] \leavevmode\begin{itemize}
\item {} 
Random AI, 탐색 기반 AI, MCTS AI, Self Learning AI

\end{itemize}

\end{description}

\item {} 
사람과 AI 체스 플레이 가능한 인터페이스 제공

\item {} \begin{description}
\item[{개발 및 테스트 환경}] \leavevmode\begin{itemize}
\item {} 
python 3.5 (anaconda)

\item {} 
Windows 10

\end{itemize}

\end{description}

\end{itemize}

다른 python 버전이나, 배포판, 다른 OS 에서 심각한 문제가 발생할 가능성은 적지만,
이 문서는 위 환경을 기준으로 작성되었다.


\subsection{디렉토리 구조}
\label{\detokenize{02-microchess_ai_platform:id3}}
\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
MicrochessAICompetition
 ├── chess: Microchess 용으로 수정한 python\PYGZhy{}chess
 ├── scripts: 실행 스크립트 및 기타 파일
 └── agents
         ├── basic: 기본 AI 예제
         ├── search: 탐색 기반 AI, MCTS 예제
         ├── self\PYGZus{}learning: 학습 AI 예제
         └── stockfish: 다른 언어로 구현된 AI 사용 예제
\end{sphinxVerbatim}


\subsection{설치방법}
\label{\detokenize{02-microchess_ai_platform:id4}}\begin{enumerate}
\item {} 
플랫폼 다운로드 및 압축해제

\item {} 
anaconda 다운로드 및 설치 (\sphinxurl{https://www.anaconda.com/download/})

\item {} 
가상환경 생성 및 활성화:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
(base) C:\PYGZbs{}Users\PYGZbs{}user\PYGZbs{}MicrochessAICompetition\PYGZgt{} conda create \textendash{}n mchess python=3.5
(base) C:\PYGZbs{}Users\PYGZbs{}user\PYGZbs{}MicrochessAICompetition\PYGZgt{} activate mchess
(mchess) C:\PYGZbs{}Users\PYGZbs{}user\PYGZbs{}MicrochessAICompetition\PYGZgt{}
\end{sphinxVerbatim}

\item {} 
주요 모듈 설치:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{conda} \PYG{n}{install} \PYG{n}{numpy} \PYG{n}{scipy} \PYG{n}{ipython} \PYG{n}{tqdm} \PYG{n}{pyyaml} \PYG{n}{mkl} \PYG{n}{matplotlib}
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{conda} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{n}{c} \PYG{n}{CogSci} \PYG{n}{pygame}
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{pip} \PYG{n}{install} \PYG{n}{sqlitedict}
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{pip} \PYG{n}{install} \PYG{n}{visdom}  \PYG{c+c1}{\PYGZsh{} visdom 설치}
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{conda} \PYG{n}{install} \PYG{o}{\PYGZhy{}}\PYG{n}{c} \PYG{n}{peterjc123} \PYG{n}{pytorch}\PYG{o}{\PYGZhy{}}\PYG{n}{cpu}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{o}{.}\PYG{l+m+mi}{0}  \PYG{c+c1}{\PYGZsh{} pytorch 설치 (CPU, Windows 용)}
\end{sphinxVerbatim}

\end{enumerate}
\begin{itemize}
\item {} \begin{description}
\item[{visdom은 AI 디버깅 및 데이터 시각화 용으로 사용함}] \leavevmode\begin{itemize}
\item {} 
MCTS AI와 Self Learning AI에서 사용

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{pytorch는 비공식 Windows + CPU 버전(\sphinxurl{https://github.com/peterjc123/pytorch-scripts}) 사용}] \leavevmode\begin{itemize}
\item {} 
최신 버전인 0.4.0 버전은 정식으로 Windows를 지원하지만, 개발에 사용한 0.3.1 버전은 정식으로 Windows를 지원되지 않음

\item {} 
0.3.1 버전과 0.4.0 버전의 대부분은 호환되지만, 0.4.0을 이 플랫폼에서 검증하지 못했음

\item {} 
CUDA 지원버전을 사용할 수 있지만, CUDA 사용가능한 부분이 적어서 속도향상이 크지 않고, 학습 성능 검증이 부족함

\end{itemize}

\end{description}

\end{itemize}


\subsection{실행 방법}
\label{\detokenize{02-microchess_ai_platform:id5}}
예제 AI 실행
\begin{itemize}
\item {} \begin{description}
\item[{Random AI vs. One Step Search AI}] \leavevmode\begin{itemize}
\item {} 
Random AI 위치: agents/basic/random\_agent.py

\item {} 
One Step Search AI: agents/search/bruteforce\_search\_agent.py

\end{itemize}

\end{description}

\end{itemize}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{python} \PYG{n}{scripts}\PYG{o}{/}\PYG{n}{run\PYGZus{}game}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{white}\PYG{o}{=}\PYG{n}{agents}\PYG{o}{.}\PYG{n}{basic}\PYG{o}{.}\PYG{n}{random\PYGZus{}agent}\PYG{o}{.}\PYG{n}{RandomAgent} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{black}\PYG{o}{=}\PYG{n}{agents}\PYG{o}{.}\PYG{n}{basic}\PYG{o}{.}\PYG{n}{bruteforce\PYGZus{}search\PYGZus{}agent}\PYG{o}{.}\PYG{n}{OneStepSearchAgent}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{execution-cmd}.png}
\caption{fen \sphinxfootnotemark[2] 표기법으로 보드 상태 출력}\label{\detokenize{02-microchess_ai_platform:fen-example}}\label{\detokenize{02-microchess_ai_platform:id9}}\end{figure}
%
\begin{footnotetext}[2]\sphinxAtStartFootnote
\sphinxurl{https://en.wikipedia.org/wiki/Forsyth-Edwards\_Notation}
%
\end{footnotetext}\ignorespaces \begin{itemize}
\item {} 
Jupyter qtconsole 에서 실행

\end{itemize}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{jupyter} \PYG{n}{qtconsole}
\PYG{n}{In} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{:} \PYG{o}{\PYGZpc{}}\PYG{n}{run} \PYG{n}{scripts}\PYG{o}{/}\PYG{n}{run\PYGZus{}game}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{white}\PYG{o}{=}\PYG{n}{agents}\PYG{o}{.}\PYG{n}{basic}\PYG{o}{.}\PYG{n}{random\PYGZus{}agent}\PYG{o}{.}\PYG{n}{RandomAgent} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{black}\PYG{o}{=}\PYG{n}{agents}\PYG{o}{.}\PYG{n}{se} \PYG{n}{arch}\PYG{o}{.}\PYG{n}{bruteforce\PYGZus{}search\PYGZus{}agent}\PYG{o}{.}\PYG{n}{OneStepSearchAgent}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{execution-qtconsole}.png}
\caption{Qt Console 화면}\label{\detokenize{02-microchess_ai_platform:qt-console-example}}\label{\detokenize{02-microchess_ai_platform:id10}}\end{figure}

\sphinxstylestrong{Random AI vs. Random AI}

AI끼리 게임을 플레이할 때는 scripts/run\_game.py 을 사용한다. 두 Random AI끼리 플레이할 때는 다음과 같이 실행한다.:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{scripts}\PYG{o}{/}\PYG{n}{run\PYGZus{}game}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{white}\PYG{o}{=}\PYG{n}{agents}\PYG{o}{.}\PYG{n}{basic}\PYG{o}{.}\PYG{n}{random\PYGZus{}agent}\PYG{o}{.}\PYG{n}{RandomAgent} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{black}\PYG{o}{=}\PYG{n}{agents}\PYG{o}{.}\PYG{n}{basic}\PYG{o}{.}\PYG{n}{random\PYGZus{}agent}\PYG{o}{.}\PYG{n}{RandomAgent}
\end{sphinxVerbatim}

White와 black에 AI의 경로를 지정해 주면 게임을 실행하고, 결과를 출력해 준다.

예제 AI는 AI의 경로를 run\_game.py의 agents에 미리 저장해 두었기 때문에, 전체 경로를 지정할 필요없이 다음과 같이 짧은 이름으로 실행 가능하다.:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{scripts}\PYG{o}{/}\PYG{n}{run\PYGZus{}game}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{white}\PYG{o}{=}\PYG{n}{random} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{black}\PYG{o}{=}\PYG{n}{random}
\end{sphinxVerbatim}

\def\sphinxLiteralBlockLabel{\label{\detokenize{02-microchess_ai_platform:id11}}}
\sphinxSetupCaptionForVerbatim{예제 AI의 이름과 경로}
\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
    \PYG{n}{agents} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}
        \PYG{n}{human}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.basic.human.Player}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{malfunction}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.basic.debug\PYGZus{}agent.MalfunctionAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{first\PYGZus{}move}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.basic.debug\PYGZus{}agent.FirstMoveAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{random}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.basic.random\PYGZus{}agent.RandomAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{one\PYGZus{}step\PYGZus{}search}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.search.one\PYGZus{}step\PYGZus{}search\PYGZus{}agent.OneStepSearchAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{two\PYGZus{}step\PYGZus{}search}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.search.two\PYGZus{}step\PYGZus{}search\PYGZus{}agent.TwoStepSearchAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{greedy}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.search.two\PYGZus{}step\PYGZus{}search\PYGZus{}agent.TwoStepSearchAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{negamax}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.search.negamax\PYGZus{}search\PYGZus{}agent.NegamaxSearchAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{abp\PYGZus{}negamax}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.search.abp\PYGZus{}negamax\PYGZus{}search\PYGZus{}agent.ABPNegamaxSearchAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{mcts}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.search.mcts\PYGZus{}agent.MCTSAgent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{mcts\PYGZus{}dev}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.search.mcts\PYGZus{}agent.MCTSAgentDev}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{self\PYGZus{}learning}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.self\PYGZus{}learning.agent.Agent}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{stockfish}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{agents.stockfish.agent.Stockfish}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{)}
\end{sphinxVerbatim}

Random AI와 One Step Search AI의 게임은 다음과 같이 실행 가능하다.:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{scripts}\PYG{o}{/}\PYG{n}{run\PYGZus{}game}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{white}\PYG{o}{=}\PYG{n}{random} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{black}\PYG{o}{=}\PYG{n}{one\PYGZus{}step\PYGZus{}search}
\end{sphinxVerbatim}

One Step AI는 다음 수의 결과를 고려하는 간단한 AI이다. 강력하지는 않지만, Random AI는 쉽게 이길 수 있다.
실행한 결과는 다음과 같이 나타난다.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
[2018\PYGZhy{}04\PYGZhy{}23 15:40:07,079 DEBUG] White: agents.basic.random\PYGZus{}agent.RandomAgent\PYGZhy{}True
[2018\PYGZhy{}04\PYGZhy{}23 15:40:07,079 DEBUG] White: board value 0.500
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
k n b r
p . . .
. . . .
. . . P
R B N K
[2018\PYGZhy{}04\PYGZhy{}23 15:40:07,093 DEBUG] White: 400.0 sec. remain
[2018\PYGZhy{}04\PYGZhy{}23 15:40:07,093 DEBUG] White: move b1d3
[2018\PYGZhy{}04\PYGZhy{}23 15:40:07,093 DEBUG] 8/8/8/knbr4/p7/3B4/3P4/R1NK4 b Kk \PYGZhy{} 1 1
...  (생략)
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,171 DEBUG] White: move d2c1
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,172 DEBUG] 8/8/8/8/3k4/8/8/2K5 b \PYGZhy{} \PYGZhy{} 0 37
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,172 DEBUG] [0.5, 0.5]
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,172 DEBUG] draw
\PYGZlt{}IPython.core.display.SVG object\PYGZgt{}
. . . .
. . . k
. . . .
. . . .
. . K .
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,178 DEBUG] Score: [0.500 0.500]
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,178 DEBUG] turns: 72
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,178 DEBUG] Game end: True
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,179 DEBUG] checkmate: False
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,179 DEBUG] Stalemate: False
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,179 DEBUG] Insufficient material: True
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,179 DEBUG] 57 moves: False
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,179 DEBUG] 5\PYGZhy{}fold: False
[2018\PYGZhy{}04\PYGZhy{}23 15:30:05,179 DEBUG] Draw: [0.5, 0.5]
\end{sphinxVerbatim}

4\textasciitilde{}8번째 줄 처럼 매 턴마다, 게임 상태를 텍스트 상태로 출력한다. white는 대문자로 표시하고, black은 소문자로 표시한다.

23번째 줄부터 게임 결과를 출력한 것이다. Score의 첫번째 숫자는 white의 점수를 나타내고, 두 번째 숫자는 black의 점수를 나타낸다.
(특별한 경우가 아니면 플랫폼의 나머지 부분에서도 white를 black보다 먼저 출력함)
게임에 승리하면 1.0, 패배하면 0.0, 그리고 비기면 0.5점을 얻는다.

일반 Chess (Microchess 포함)와 달리 이 플랫폼에서는 승패가 결정되지 않으면,
게임이 종료되었을 때 남아있는 기물의 점수를 계산하여 최종 승패를 판단한다.
따라서, 기존 무승부 조건을 만족한 상태에서 기물의 점수도 동일한 경우만 무승부가 가능하다.

turns는 전체 게임의 턴 수를 보여주고, 그 다음 줄의 game end는 게임이 정상적으로 종료되었는지 여부를 알려준다.
그 다음부터는 게임이 종료된 이유(checkmate, stalemate, 등)를 알려준다.

AI 끼리의 성능을 평가할 때는 benchmark 옵션을 사용한다. benchmark 옵션을 사용하면, 지정된 white와 black 옵션과 상관없이
총 20게임 (지정된 white와 black으로 10게임, white와 black을 뒤집어서 10게임)을 플레이하고 전체 승률을 출력해준다.

\sphinxstylestrong{Random AI vs. One Step Search AI}

Random AI와 One Step Search AI의 성능을 비교하려면 다음과 같이 실행한다.:

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{python} \PYG{n}{scripts}\PYG{o}{/}\PYG{n}{run\PYGZus{}game}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{white}\PYG{o}{=}\PYG{n}{random} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{black}\PYG{o}{=}\PYG{n}{one\PYGZus{}step\PYGZus{}search} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{benchmark}
\end{sphinxVerbatim}

실행 결과는 다음과 같이 나타난다.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
[2018\PYGZhy{}04\PYGZhy{}23 15:52:19,247 INFO]          random          \textbar{}    one\PYGZus{}step\PYGZus{}search
[2018\PYGZhy{}04\PYGZhy{}23 15:52:19,247 INFO] W                  0.000 \textbar{} B                  1.000
[2018\PYGZhy{}04\PYGZhy{}23 15:52:19,247 INFO] W                  0.500 \textbar{} B                  0.500
... (생략)
[2018\PYGZhy{}04\PYGZhy{}23 15:52:19,248 INFO] B                  1.000 \textbar{} W                  0.000
[2018\PYGZhy{}04\PYGZhy{}23 15:52:19,248 INFO] B                  0.000 \textbar{} W                  1.000
[2018\PYGZhy{}04\PYGZhy{}23 15:52:19,248 INFO] random (White): 0.300 vs. one\PYGZus{}step\PYGZus{}search (Black): 0.700
[2018\PYGZhy{}04\PYGZhy{}23 15:52:19,249 INFO] random (Black): 0.350 vs. one\PYGZus{}step\PYGZus{}search (White): 0.650
[2018\PYGZhy{}04\PYGZhy{}23 15:52:19,250 INFO] random: 0.325 vs. one\PYGZus{}step\PYGZus{}search: 0.675
\end{sphinxVerbatim}

7번째 줄 까지는 두 AI가 얻은 점수를 보여주고, 마지막 세 줄은 그 평균을 보여준다.
두 AI가 각각 White/Black 그리고 Black/White인 경우의 평균을 따로 보여주고, 마지막으로 둘을 통합한 평균을 보여준다.
이 결과에서는 One Step Search AI가 평균 0.675점을 얻어 더 좋은 성능을 보여주고 있다.


\subsection{인간 플레이어 vs. AI 인터페이스}
\label{\detokenize{02-microchess_ai_platform:vs-ai}}
AI 성능 테스트 용으로 게임 플레이 인터페이스 제공한다. AI의 경로를 입력하는 부분에 human이라고 쓰면, AI와 동일한 방식으로 실행할 수 있다.
인간 플레이어로 게임을 실행하면, {\hyperref[\detokenize{02-microchess_ai_platform:human-play-interface}]{\sphinxcrossref{\DUrole{std,std-ref}{인간 플레이어 인터페이스}}}} 가 나타나고, 플레이어의 순서에 마우스로 조작이 가능해진다.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{p}{(}\PYG{n}{mchess}\PYG{p}{)} \PYG{n}{C}\PYG{p}{:}\PYGZbs{}\PYG{n}{Users}\PYGZbs{}\PYG{n}{user}\PYGZbs{}\PYG{n}{MicrochessAICompetition}\PYG{o}{\PYGZgt{}} \PYG{n}{python} \PYG{n}{scripts}\PYG{o}{/}\PYG{n}{run\PYGZus{}game}\PYG{o}{.}\PYG{n}{py} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{white}\PYG{o}{=}\PYG{n}{agents}\PYG{o}{.}\PYG{n}{basic}\PYG{o}{.}\PYG{n}{random\PYGZus{}agent}\PYG{o}{.}\PYG{n}{RandomAgent} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZhy{}}\PYG{n}{black}\PYG{o}{=}\PYG{n}{agents}\PYG{o}{.}\PYG{n}{basic}\PYG{o}{.}\PYG{n}{human}\PYG{o}{.}\PYG{n}{Player}
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[height=300\sphinxpxdimen]{{microchess-human}.png}
\caption{인간 플레이어 인터페이스}\label{\detokenize{02-microchess_ai_platform:human-play-interface}}\label{\detokenize{02-microchess_ai_platform:id12}}\end{figure}

\newpage


\section{기본 AI 예제}
\label{\detokenize{03-basic_ai_examples::doc}}\label{\detokenize{03-basic_ai_examples:ai}}

\subsection{Random AI}
\label{\detokenize{03-basic_ai_examples:random-ai}}
\def\sphinxLiteralBlockLabel{\label{\detokenize{03-basic_ai_examples:random-ai-code}}\label{\detokenize{03-basic_ai_examples:id28}}}
\sphinxSetupCaptionForVerbatim{Random AI}
\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{k+kn}{import} \PYG{n+nn}{random}
\PYG{k+kn}{from} \PYG{n+nn}{agents} \PYG{k}{import} \PYG{n}{BaseAgent}
    

\PYG{k}{class} \PYG{n+nc}{RandomAgent}\PYG{p}{(}\PYG{n}{BaseAgent}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Random AI}

\PYG{l+s+sd}{    \PYGZhy{} 무작위로 행동을 결정하는 예제}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{k}{def} \PYG{n+nf}{reset}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{act}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{moves} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{state}\PYG{o}{.}\PYG{n}{legal\PYGZus{}moves}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{moves}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{close}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
\end{sphinxVerbatim}


\subsubsection{가장 간단한 AI 구현}
\label{\detokenize{03-basic_ai_examples:id1}}
{\hyperref[\detokenize{03-basic_ai_examples:random-ai-code}]{\sphinxcrossref{\DUrole{std,std-ref}{Random AI}}}} 는 이 플랫폼을 이용해 구현된 가장 간단한 AI 중 하나이다.
이 AI의 목적은 플랫폼 사용자들에게 AI를 구현하는 가장 기초적인 방법에 대해 설명하고,
AI 성능의 최저 기준선을 제시하는 것이다.
새로 구현한 AI가 Random AI에 대비하여 명확하게 좋은 성능을 보이지 않는다면,
새로 구현한 AI 제대로 작동하고 있지 않을 가능성이 매우 높다.

AI를 구현할 때는 Random AI처럼 BaseAgent를 상속받아 reset, act, close 메소드를 구현하면 된다.
반드시 필수는 아니지만, 모든 예제 AI는 "agents/\{package\}/\{agent\_name\}.py" 에 저장하고 있다. package와 agent\_name은
AI의 특징을 나타낼 수 있도록 임의로 정하여 사용한다.
게임 시작할 때 AI 클래스의 위치를 "agents.\{package\}.\{agent\_name\}.\{agent\_class\_name\}" 형식으로 전달하면,
플랫폼이 AI 클래스를 import 하여 실행한다.

reset() 메소드는 새로운 게임이 시작하기 전에 한번 호출된다. 보통 이 메소드에서는 AI 초기화 작업을 진행한다.
예를 들면 학습한 모델이나 데이터를 메모리로 읽어들이는 작업을 이때 할 수 있다.

close() 메소드는 반대로 게임이 종료되었을 때, 한번 호출된다. 여기서는 아직 열려있는 파일을 닫거나,
메모리에 아직 남아있는 데이터를 저장하는 등의 작업을 할 수 있다.

reset()과 close() 메소드는 필요하지 않다면, 반드시 구현할 필요는 없다.
구현하지 않으면 BaseAgent에 있는 아무것도 하지 않는 reest()과 close()가 사용된다.

하지만, act(state) 메소드는 반드시 구현해야 한다. 이 메소드는 해당 AI 턴마다 실행된다.
인자로 전달되는 state는 현재 게임 상태정보를 가지고 있다. AI는 이 정보를 이용해서 다음에 둘 수를 반환해야한다.
AI를 구현한다는 것은 기본적으로 이 부분에 특정 게임상태(state)와 최선의 수(move)를 매핑해주는 함수를 정의하는 일이다.

state 인스턴스는 {\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{State}}}}} 객체이다. State객체는 python-chess의 \sphinxcode{\sphinxupquote{Board}} 객체를 상속받아
시뮬레이션 기능을 지원하는 forward(move) 메소드를 추가한 것이기 때문에, 기본적으로 chess.Board객체에서 지원하는 기능을
모두 가지고 있다. 향후 부정행위에 이용할 만한 기능을 발견되면 일부 기능이 제한될 수 있다.

Board 객체에서 지원하는 기능 중에 하나는 주어진 게임상태에서 둘 수 있는 수를 알려주는 것이다. 15번째 줄의
state.legal\_moves 는 현재 둘 수 있는 수들을 반환하는 python generator이다. Random AI는 이것을 list (moves)에 담은 뒤,
그 중에서 무작위로 하나를 선택(random.choice)하여 반환한다.


\subsubsection{act 메소드의 입력과 출력 요약}
\label{\detokenize{03-basic_ai_examples:act}}
act(state) 메소드의 입력과 출력을 핵심을 정리하면 다음과 같다.
\begin{itemize}
\item {} \begin{description}
\item[{입력: state}] \leavevmode\begin{itemize}
\item {} 
python-chess의 Board 객체 + 다음 상태 시뮬레이션(forward)

\item {} 
현재 보드 상태에 대한 정보

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{출력: move}] \leavevmode\begin{itemize}
\item {} 
python-chess의 Move 객체

\item {} 
기물의 현재 위치, 다음 위치 저장

\item {} 
{[}0, 20{]} 정수 x {[}0, 20{]} 정수 = 400 가지 조합 가능함

\item {} 
특정 상태에서 선택가능한 수는 평균 10개 정도

\end{itemize}

\end{description}

\end{itemize}


\subsection{Search 기반 AI}
\label{\detokenize{03-basic_ai_examples:search-ai}}
AI가 하는 가장 기본적인 작업은 특정 상태(\(s\) ; state)가 주어졌을 때, 최선의 행동(\(a\); action)을 결정하는 일이다.
즉 우리는 AI를 상태와 행동을 매핑하는 일종의 함수라고 볼 수 있다(\(f(s) \rightarrow a\)).
일반적으로는 행동(\(a\); action)으로 표현하지만, 체스같은 보드게임에서는 수(\(m\); move)라고 표현하므로, 이 문서에는 필요에따라
행동(\(a\))과 수(\(m\))를 혼용해서 한다.

현재 적합한 행동을 결정하는 가장 간단한 방법은 실제 그 행동을 했을 때, 어떤 결과가 나타날지 시뮬레이션을 해보고,
가장 좋은 결과가 나타나는 행동으로 다음 행동을 결정하는 것이다.
시뮬레이션은 현재 상태(\(s_t\))와 행동(\(a\))이 주어졌을 때, 다음 상태(\(s_{t+1}\))를 반환해주는 함수
(\(f(s_t, a) \rightarrow s_{t+1}\))로 볼 수 있는데, 이 플랫폼에서는 State.forward(move) 함수가 그 역할을 한다.
foward 함수는 게임로직과 동일하게 작동하고, 상대방의 미래 행동을 제외하면 게임에는 불확실성이 없기 때문에
상대방의 미래 행동을 정확하게 예측할 수 있다면 정확한 미래 상태를 예측할 수 있다.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{state-space}.png}
\caption{상태공간}\label{\detokenize{03-basic_ai_examples:state-space-example}}\label{\detokenize{03-basic_ai_examples:id29}}\end{figure}

Simulation 함수가 정의되고 나면, 그래프 구조로 만들어진 {\hyperref[\detokenize{03-basic_ai_examples:state-space-example}]{\sphinxcrossref{\DUrole{std,std-ref}{상태공간}}}} 을 만들 수 있다.
각각의 노드는 특정 상태(예. 특정한 체스기물의 배치상태)를 나타내고,
노드와 노드를 연결하는 링크는 행동을 의미한다. AI의 궁극적인 목적은 상태공간의 시작 상태(\(s_0\))에서 시작하여,
목표상태(\(g\), 게임에 승리한 상태)까지 도달하기 위해 현재 어떤 행동을 해야하는지 알아내는 것이다.
이것은 그래프 구조에서 경로를 탐색하는 문제로 취급할 수 있기 때문에 순회(traverse), 탐색(search) 알고리즘을 사용해 문제를 해결한다.
따라서 이런 방식의 AI를 탐색(Search)기반 AI라고 한다.

실제 상태공간은 순환(cycle)이 존재하는 그래프지만 문제를 단순화하기 위해 일반적으로는
순환을 없에고(실제 동일한 상태를 새로운 상태로 취급) 상태공간을 트리 구조로 모델링한다.


\subsubsection{One Step Search AI}
\label{\detokenize{03-basic_ai_examples:one-step-search-ai}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{one-step-search-ai}.png}
\caption{One Step Search AI}\label{\detokenize{03-basic_ai_examples:id2}}\label{\detokenize{03-basic_ai_examples:id30}}\end{figure}

{\hyperref[\detokenize{03-basic_ai_examples:id2}]{\sphinxcrossref{\DUrole{std,std-ref}{One Step Search AI}}}} 는 바로 다음 수의 결과만을 예측하여 현재 행동을 결정하는 가장 단순한 Search 기반 AI이다.
현재 상태 \(S_t\) 에서 가능한 수를 모두 시뮬레이션 하여, 다음 상태 \(S_{t+1}\) 들을 알아낸 뒤,
목표상태와 가장 가까운 상태를 찾아, 그와 연관된 수를 다음 수로 결정한다.

여기서 목표상태와 얼마나 가까운지 판단하기 위해 평가함수를 사용한다.
평가함수는 현재 상태(\(s\))를 입력받아, 이것이 목표상태(\(g\))에 얼마나 가까운지 정량적으로
측정하는 함수이다(\(eval(s) \rightarrow v\)).
\begin{quote}
\begin{equation}\label{equation:03-basic_ai_examples:eval_func}
\begin{split}v = \frac{{score}_{my}}{{score}_{my} + {score}_{opponent}}\end{split}
\end{equation}\end{quote}

One Step Search AI가 사용하는 평가함수 \eqref{equation:03-basic_ai_examples:eval_func} 는 얼마나 승리에 가까운지 평가하여 {[}0, 1{]} 실수로 반환한다.
자신과 상대방의 점수(\({score}_{my}, {score}_{opponent}\))는 {\hyperref[\detokenize{01-microchess_ai_competition:score-table}]{\sphinxcrossref{\DUrole{std,std-ref}{점수 표}}}} 의 기물점수를 사용한다.
패배에 가까울 수록 0에 가깜고, 승리에 가까울 수록 1에 가까운 값을 반환한다.

가장 간단한 탐색 방법을 사용하는 One Step Search AI는 Random AI 보다는 좋은 성적을 보이지만,
한 수 이상을 고려하지 않기 때문에, 여러가지 문제를 가지고 있다.
상대방의 대응을 예상하지 못하기 때문에 쉽게 반격당할 수 있고,
다음 상태에서, 다른 상태에 비해 더 좋은 상태가 없는 경우 의사결정이 어렵다.
이 문제의 원인은 One Step Search AI가 매우 단기적인 이익 만을 추구하기 때문에,
장기적인 계획(연속된 행동)이 필요한 목표를 추구할 수 없기 때문이다.

따라서 이 문제의 근본적인 해결 방법은 더 먼 미래까지 탐색하는 것이 된다.


\subsubsection{Two Step Search AI}
\label{\detokenize{03-basic_ai_examples:two-step-search-ai}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{two-step-search-ai}.png}
\caption{Two Step Search AI}\label{\detokenize{03-basic_ai_examples:id3}}\label{\detokenize{03-basic_ai_examples:id31}}\end{figure}

{\hyperref[\detokenize{03-basic_ai_examples:id3}]{\sphinxcrossref{\DUrole{std,std-ref}{Two Step Search AI}}}} 는 이 방향에서 One Step Search AI를 개선한 것이다. Two Step Search AI는 이름 그대로
두 단계 미래 상태 (\(S_{t+2}\))까지 탐색하고 가장 좋은 상태로 연결되는 수를 찾아내는 AI 이다.

Two Step Search 에서 두 번째 수는 내가 아닌 상대방의 수 이기 때문에, 미래 상태를 예측 하려면
다음에 상대방이 어떤 수를 둘 것인지를 알아야 한다. 실제로 상대방이 다음에 어떤 수를 둘 지는 알 수 없지만
상대방도 나와 마찬가지로 현재 상태를 목표상태에 가깝게 바꿔나가고자 한다고 가정하면, 상대방의 수를 예측할 수 있다.

Chess 같은 zero-sum 게임 %
\begin{footnote}[1]\sphinxAtStartFootnote
한쪽의 승리가 다른쪽에서는 패배가 되는 게임
%
\end{footnote} 에서는 상태의 평가 점수가 낮을 수록 상대방의 입장에서는 유리한 것이기 때문에,
내가 더 좋은 상태로 연결되는 수를 선택해야하는 것과는 반대로 상대방은 (나에게) 더 나쁜 상태로 연결되는 수를 선택할 것이다.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{two-step-search-ai-backup}.png}
\caption{Two Step Search AI의 평가 과정}\label{\detokenize{03-basic_ai_examples:two-step-search-ai-eval}}\label{\detokenize{03-basic_ai_examples:id32}}\end{figure}

{\hyperref[\detokenize{03-basic_ai_examples:two-step-search-ai-eval}]{\sphinxcrossref{\DUrole{std,std-ref}{Two Step Search AI의 평가 과정}}}} 은 그림 처럼 마지막 상태들을 평가한 뒤에, 꺼꾸로 가장 먼 미래서부터 현재 상태까지 어떤 행동들을 선택해야
가장 높은 평가를 받은 마지막 상태에 도달할 수 있는지 알아낸다. 차례대로  가장 낮은 평가값를 받은 상태로 연결되는 수(상대순서),
가장 높은 평가를 받은 상태로 연결되는 수(내 순서)를 선택하여 현재 상태에서 두어야 할 수를 결정한다.


\subsubsection{Two Step Search AI 구현}
\label{\detokenize{03-basic_ai_examples:id5}}
\def\sphinxLiteralBlockLabel{\label{\detokenize{03-basic_ai_examples:two-step-search-ai-code}}\label{\detokenize{03-basic_ai_examples:id33}}}
\sphinxSetupCaptionForVerbatim{Two Step Search AI (Greedy AI)}
\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
    \PYG{k}{def} \PYG{n+nf}{act}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{my\PYGZus{}action\PYGZus{}evals} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{my\PYGZus{}action} \PYG{o+ow}{in} \PYG{n}{state}\PYG{o}{.}\PYG{n}{legal\PYGZus{}moves}\PYG{p}{:}
            \PYG{n}{next\PYGZus{}state} \PYG{o}{=} \PYG{n}{state}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{my\PYGZus{}action}\PYG{p}{)}
            \PYG{k}{if} \PYG{n}{next\PYGZus{}state}\PYG{o}{.}\PYG{n}{is\PYGZus{}game\PYGZus{}over}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                \PYG{c+c1}{\PYGZsh{} 한 수 뒤에 게임이 종료될 경우 상대방의 수를 보지 않고 바로 평가}
                \PYG{c+c1}{\PYGZsh{} add\PYGZus{}nodise: 평가값 이 동일한 경우 그 중에 하나를}
                \PYG{c+c1}{\PYGZsh{} 무작위로 선택하도록 하기 위해 작은 노이즈 추가}
                \PYG{n}{score} \PYG{o}{=} \PYG{n}{Evaluator}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{turn}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}6} \PYG{o}{*} \PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
            \PYG{k}{else}\PYG{p}{:}
                \PYG{n}{score} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{opponent\PYGZus{}act}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{)}
            \PYG{n}{my\PYGZus{}action\PYGZus{}evals}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{(}\PYG{n}{my\PYGZus{}action}\PYG{p}{,} \PYG{n}{score}\PYG{p}{)}\PYG{p}{)}
            
        \PYG{c+c1}{\PYGZsh{} 가장 평가값이 높은 수을 반환함}
        \PYG{n}{best\PYGZus{}action}\PYG{p}{,} \PYG{n}{score} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n}{my\PYGZus{}action\PYGZus{}evals}\PYG{p}{,} \PYG{n}{key}\PYG{o}{=}\PYG{n}{itemgetter}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{best\PYGZus{}action}

    \PYG{k}{def} \PYG{n+nf}{opponent\PYGZus{}act}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        상대방의 행동 시뮬레이션}
\PYG{l+s+sd}{        이 상태의 평가는 상대방의 그 다음 행동에 따라 달라짐}
\PYG{l+s+sd}{        }
\PYG{l+s+sd}{        :param State state: 현재 상태}
\PYG{l+s+sd}{        :return: (float) \PYGZhy{}\PYGZhy{} 평가값 [0, 1] 범위}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n}{opp\PYGZus{}action\PYGZus{}evals} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{opp\PYGZus{}action} \PYG{o+ow}{in} \PYG{n}{state}\PYG{o}{.}\PYG{n}{legal\PYGZus{}moves}\PYG{p}{:}
            \PYG{n}{next\PYGZus{}state} \PYG{o}{=} \PYG{n}{state}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{opp\PYGZus{}action}\PYG{p}{)}
            \PYG{n}{score} \PYG{o}{=} \PYG{n}{Evaluator}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{turn}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}6} \PYG{o}{*} \PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
            \PYG{n}{opp\PYGZus{}action\PYGZus{}evals}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{(}\PYG{n}{opp\PYGZus{}action}\PYG{p}{,} \PYG{n}{score}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{score} \PYG{o}{=} \PYG{n+nb}{min}\PYG{p}{(}\PYG{n}{opp\PYGZus{}action\PYGZus{}evals}\PYG{p}{,} \PYG{n}{key}\PYG{o}{=}\PYG{n}{itemgetter}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{score}
\end{sphinxVerbatim}

{\hyperref[\detokenize{03-basic_ai_examples:two-step-search-ai-code}]{\sphinxcrossref{\DUrole{std,std-ref}{Two Step Search AI (Greedy AI)}}}} 은 Two Step AI의 핵심인 act 와 opponent\_act 메소드이다.
act 메소드는 state(\(s_t\))를 입력받아, 다음 상태(\(s_{t+1}\))를 평가할 때, 상대방이 대응하고
난 뒤의 상태(\(s_{t+2}\))를 먼저 평가한 뒤에 그 결과를 이용한다.

매우 많은 약점을 노출하는 One Step Search AI와 달리, Two Step Search AI는 쉽게 반격당하는 무모한 공격을 하지
않기 때문에 상당히 그럴듯하게 작동한다. 하지만, 실수하지 않고 주의깊게 공략하면 쉽게 이길 수 있다.

기본적으로 Two Step Search AI는 One Step Search AI와 동일한 한계를 가진다. Two Step Search AI도 두 수 이상의 미래는
고려하지 않기 때문에, 두 수 이내에서 현재보다 좋은 상황이 없을 경우 다음 수를 결정하기 어렵고,
상대방이 두 수 이상을 고려한 전술을 사용할 경우 효과적으로 대처가 불가능하다.

이것을 해결하는 직접적인 방법은 보다 깊게(먼 미래) 상태공간을 탐색하는 것이다.
두 수만이 아니라, 더 많은 미래의 경우의 수를 계산하면 쉽게 성능을 높일 수 있다.
Microchess는 모든 정보가 공개된, 불확실성이 없는 게임이기 때문에, 상대방의 행동만 예상할 수 있다면
게임 시작부터 끝날 때 까지 모든 상태를 탐색할 수는 있다.

Two Step Search AI과 같은 방식의 AI를 보다 일반적으로 구현한 것이 Negamax Search이다.
Negamax Search는 Two Step Search AI와 달리 임의의 미래까지 탐색이 가능하다.


\subsubsection{Negamax Search AI}
\label{\detokenize{03-basic_ai_examples:negamax-search-ai}}
\def\sphinxLiteralBlockLabel{\label{\detokenize{03-basic_ai_examples:negamax-search-code}}\label{\detokenize{03-basic_ai_examples:id34}}}
\sphinxSetupCaptionForVerbatim{Negamax Search 함수}
\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
    \PYG{k}{def} \PYG{n+nf}{negamax}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{depth}\PYG{p}{,} \PYG{n}{color}\PYG{p}{)}\PYG{p}{:}
        \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Negamax 탐색}
\PYG{l+s+sd}{        }
\PYG{l+s+sd}{        :param State state: 현재 상태}
\PYG{l+s+sd}{        :param int depth: 남은 탐색 깊이, self.max\PYGZus{}depth \PYGZhy{} 현재 깊이}
\PYG{l+s+sd}{        :param float color: white = 1., black = \PYGZhy{}1}
\PYG{l+s+sd}{        :return: (chess.Move, float) \PYGZhy{}\PYGZhy{} best\PYGZus{}move, 평가값}

\PYG{l+s+sd}{        \PYGZhy{} chess.Move: 가장 좋은 행동}
\PYG{l+s+sd}{        \PYGZhy{} float: 가장 좋은 행동을 했을 때 얻을 수 있는 미래 보상}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{if} \PYG{n}{depth} \PYG{o}{==} \PYG{l+m+mi}{0} \PYG{o+ow}{or} \PYG{n}{state}\PYG{o}{.}\PYG{n}{is\PYGZus{}game\PYGZus{}over}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} 입력 받은 상태가 최대 탐색 깊이거나, 게임이 종료된 상태라면, 상태를 평가함}
            \PYG{n}{heuristic\PYGZus{}value} \PYG{o}{=} \PYG{n}{Evaluator}\PYG{o}{.}\PYG{n}{eval\PYGZus{}2}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{k+kc}{True}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}6} \PYG{o}{*} \PYG{n}{random}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} white는 1.0, black은 \PYGZhy{}1.0이 가장 좋은 점수}
            \PYG{k}{return} \PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{color} \PYG{o}{*} \PYG{n}{heuristic\PYGZus{}value}
        
        \PYG{c+c1}{\PYGZsh{} 최저값으로 기본값 결정}
        \PYG{n}{best\PYGZus{}move}\PYG{p}{,} \PYG{n}{best\PYGZus{}value} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1000}
        \PYG{k}{for} \PYG{n}{move} \PYG{o+ow}{in} \PYG{n}{state}\PYG{o}{.}\PYG{n}{legal\PYGZus{}moves}\PYG{p}{:}
            \PYG{n}{next\PYGZus{}state} \PYG{o}{=} \PYG{n}{state}\PYG{o}{.}\PYG{n}{forward}\PYG{p}{(}\PYG{n}{move}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} 재귀적으로 미래상태를 시뮬레이션함}
            \PYG{c+c1}{\PYGZsh{} 다음 상태는 상대방 순서이므로, color를 바꾸고, }
            \PYG{c+c1}{\PYGZsh{} 반환받은 평가값(value)도 부호를 변경함}
            \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{negamax}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{depth}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{n}{color}\PYG{p}{)}
            \PYG{n}{value} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{value}
            \PYG{k}{if} \PYG{n}{value} \PYG{o}{\PYGZgt{}} \PYG{n}{best\PYGZus{}value}\PYG{p}{:}
                \PYG{n}{best\PYGZus{}move}\PYG{p}{,} \PYG{n}{best\PYGZus{}value} \PYG{o}{=} \PYG{n}{move}\PYG{p}{,} \PYG{n}{value}
                
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{n\PYGZus{}nodes} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
        \PYG{k}{return} \PYG{n}{best\PYGZus{}move}\PYG{p}{,} \PYG{n}{best\PYGZus{}value}

\end{sphinxVerbatim}

Negamax Search %
\begin{footnote}[2]\sphinxAtStartFootnote
\sphinxurl{https://en.wikipedia.org/wiki/Negamax}
%
\end{footnote} 는 Minimax Search의 일종으로, 구현을 단순하게 하기 위해, 평가값 구간을 {[}0, 1{]} 사이 실수로 하는 대신,
{[}-1, 1{]} 사이 실수를 사용한다. 한 단계 미래상태를 탐색할 때마다, 보상값에 -1을 곱하여
상대방의 입장에서도 최대값(-를 곱했으므로 나에게는 최소값)을 찾도록 구현하였다.
재귀적으로 {\hyperref[\detokenize{03-basic_ai_examples:negamax-search-code}]{\sphinxcrossref{\DUrole{std,std-ref}{Negamax Search 함수}}}} 를 호출하면서, 깊이 우선탐색(depth-first search)로 정해진 최대 깊이(미래 상태)까지 탐색하고,
그 중에 가장 좋은상태로 연결되는 현재 수와 평가값을 반환한다.

여기서 가장 중요한 파라미터는 최대 탐색깊이(depth 인자)이다.
Microchess를 포함한 대부분의 문제에서 하나의 행동으로 목표를 달성하는 경우는 드물고,
보통 연속된 행동의 결과로 목표 상태에 도달할 수 있다.
따라서 시작상태부터 목표상태까지 경로를 찾아내기 위해서는 가능한 먼 미래 상태까지 탐색할 필요가 있다.
만약 탐색깊이가 부족하면, 모든 상태를 탐색했음에도 불구하고, 목표상태를 발견하지 못할 수 있다.
그러나, 탐색 깊이를 증가시키면 탐색공간이 지수적으로 증가하기 때문에 매우 많은
계산비용이 필요할 수 있다. 따라서, 목표까지의 거리와 탐색 비용은 상충관계(trade-off)에 해당하고,
이것을 고려해서 최대 탐색 깊이를 신중히 결정해야한다. 탐색깊이가 부족해서 목표상태를 발견하지 못하는 현상을
수평선 효과(horizon-effect %
\begin{footnote}[3]\sphinxAtStartFootnote
\sphinxurl{https://en.wikipedia.org/wiki/Horizon\_effect}
%
\end{footnote} )라고 하며, AI 분야에서 대표적인 문제 중의 하나이다.

이 플랫폼에서는 기물점수를 계산해서 승패에 중요한 요소로 삼기 때문에, 수평선 효과가 상대적으로 덜 발생할 수 있다.
기존 체스(Microchess 포함)에서는 게임 종료상태까지 탐색하지 못하면(약 80수), 현재 수의 좋고 나쁨을 판단할 수 없기 때문에
충분히 먼 미래를 탐색하지 않으면 수평선효과가 강하게 나타날 수 밖에 없다. 하지만 이 플랫폼의 기본 규칙에서는
내 기물을 지키고 상대방의 기물을 제거하기만 하면(5수 이하) 더 좋은 상태로 판단할 수 있기 때문에 얕은(가까운 미래) 탐색만으로도
상태의 좋고 나쁨을 쉽게 판단할 수 있다.


\subsubsection{Negamax Search AI + \protect\(\alpha-\beta\protect\) pruning}
\label{\detokenize{03-basic_ai_examples:negamax-search-ai-pruning}}
\(\alpha-\beta\) pruning %
\begin{footnote}[4]\sphinxAtStartFootnote
\sphinxurl{https://en.wikipedia.org/wiki/Alpha}\textendash{}beta\_pruning
%
\end{footnote} 은 Negamax 알고리즘의 탐색공간을 줄여주는 대표적인 기법이다.
기본 아이디어는 탐색하는 도중에 상대방 순서에서 찾아낸 최소 평가값(\(\beta\))이 그 이전의 내 순서에서
찾아낸 최대 평가값(\(\alpha\)) 보다 작거나 같으면(\(\beta \le \alpha\)), 나머지 수를 탐색하지 않고,
현재 평가값을 최종 평가값으로 반환하는 것이다.
이 경우 나머지 수에서 어떤 작은 평가값이 나와도, 그 위 단계에서 나는 그것을 선택하지 않고 \(\alpha\) 을 선택할
것이기 때문에 더 이상 탐색할 필요가 없다.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{alpha-beta-pruning}.png}
\caption{\(\alpha-\beta\) pruning 예}\label{\detokenize{03-basic_ai_examples:alpha-beta-pruning-example}}\label{\detokenize{03-basic_ai_examples:id35}}\end{figure}

간단한 \(\alpha-\beta\) {\hyperref[\detokenize{03-basic_ai_examples:alpha-beta-pruning-example}]{\sphinxcrossref{\DUrole{std,std-ref}{ pruning 예}}}} 를 보면 \(S_1\) 에서 평가값 1을 얻었을 때,
다음 평가 대상인 \(S_3\) 의 평가값이 1보다 낮거나 높거나 두 가지 가능성이 있다.
만약 이 값 이 1보다 작은 0.5가 나온다면 \(S_4\) 는 탐색이 필요없다. \(S_4\) 에서 0.5보다 큰 값이 나온다면
상대방이 이것을 선택하지 않을 것이고, 0.5보다 작은 값이 나온다면 상대방은 선택하겠지만 내가 그것을 선택하지 않을 것이기 때문이다.

\(\alpha-\beta\) pruning 을 사용한 기본적인 Negamax Search는 {\hyperref[\detokenize{agents.search:agents.search.abp_negamax_search_agent.ABPNegamaxSearchAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{ABPNegamaxSearchAgent}}}}} 에 구현되어 있다.
기존 Negamax Search AI는 탐색 깊이 4이상부터 급격히 탐색시간이 증가하여 경진대회에서 가정한 10초를 거의 다 사용한다.
반면 ABP Negamax Search AI는 탐색깊이 6까지는 어렵지 않게 탐색할 수 있다.
계산비용을 절약한 만큼 더 많은 공간을 탐색할 수 있기 때문에, 결과적으로 성능을 향상시킬 수 있다.
이 알고리즘은 일반적인 체스 AI를 구현하는 가장 기본적인 접근방법이다 %
\begin{footnote}[5]\sphinxAtStartFootnote
\sphinxurl{https://medium.freecodecamp.org/simple-chess-ai-step-by-step-1d55a9266977}
%
\end{footnote}.
많은 유명한 AI 들도 기본적으로 이런 접근방법을 사용한다.


\subsubsection{Microchess 상태공간 규모}
\label{\detokenize{03-basic_ai_examples:microchess}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{tree}.png}
\caption{상태공간 트리}\label{\detokenize{03-basic_ai_examples:state-state-tree}}\label{\detokenize{03-basic_ai_examples:id36}}\end{figure}

Microchess 처럼 간단한 문제에서도 모든 탐색공간을 탐색하는 것은 현실적으로 매우 어렵다고 볼 수 있다.
한 상태에서 선택가능한 수가 평균적으로 10개이고, 게임 길이가 평균 20턴이라고 가정하면, \(b\) (branch) 가 10이고,
\(d\) (depth)가 20인 {\hyperref[\detokenize{03-basic_ai_examples:state-state-tree}]{\sphinxcrossref{\DUrole{std,std-ref}{상태공간 트리}}}} 를 순회해야 하고 가장 좋은 상태를 찾아야 하는데, 총 순회해야할 노드의 개수는
약 \(b^d = 10^{20} = 100,000,000,000,000,000,000\) 개 %
\begin{footnote}[6]\sphinxAtStartFootnote
\sphinxurl{https://en.wikipedia.org/wiki/Endgame\_tablebase}
%
\end{footnote} 정도 된다. 매 초마다, 백만개씩 순회가능하다고 하더라도,
한 수를 두는데 100,000,000,000,000초가 소모된다.
비록 \(\alpha-\beta\) pruning 같은 기법을 사용하여 불필요한 탐색공간을 줄이더라도, 문제는 크게 완화되지 않는다.
만약 전체 탐색공간의 99\%를 제외할 수 있다고 하더라도, 전체 탐색공간에서 0이 두 개 줄어들 뿐이다.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{n_nodes}.png}
\caption{탐색공간의 규모}\label{\detokenize{03-basic_ai_examples:search-space-scale}}\label{\detokenize{03-basic_ai_examples:id37}}\end{figure}

게임 시작상태부터 실제로 너비우선 탐색으로(breadth-first search)로 가능한 모든 상태를 순회하면
전체 {\hyperref[\detokenize{03-basic_ai_examples:search-space-scale}]{\sphinxcrossref{\DUrole{std,std-ref}{탐색공간의 규모}}}} 를 예상해 볼 수 있다.
깊이 8까지 순회한 결과 상태공간의 개수는 예측한 결과와 유사하게 지수적으로 증가하는 것을 볼 수 있다.
깊이가 깊어질 수록 점차 예상보다는 줄어들기는 하지만, 깊이 8에서 증가폭은 여전히 매우 크다.
또한 순회한 상태 중에서 게임이 종료된 상태(종단노드)에 도달하는 수는 전체 공간의 0.01\textasciitilde{}0.001에 불과하다는 점도 알 수 있다.
때문에 아무런 정보도 없이 탐색 기법만으로 게임이 종료될 상태를 찾아낼 확률은 매우 낮다.

탐색공간이 크기 때문에 게임 도중 탐색하는 대신에, 사전에 미리 탐색하여 데이터를 테이블 형태로 정리해놓고,
탐색을 대체할 수 도 있는데, Microchess 모든 경우의 수를 모두 조사하여 디스크에 저장하려 한다면,
한 상태에 1 Byte씩 소모된다고 해도(실제로는 최소 24bit 필요) 100,000,000TB의 용량이 필요하다.

따라서, 특별한 기술이나 지식없이 게임 도중에 실시간으로 탐색하거나,
사전에 모두 탐색하여 테이블 형태로 정리해 놓거나, 모든 경우의 수를 탐색하는 것은
상당한 시간과 메모리/디스크 용량을 필요로 한다.

일반 체스에서는 기물이 7\textasciitilde{}5개 남은 상태에서 모든 경우의 수를 계산해 놓은 Endgame tablebase %
\begin{footnote}[7]\sphinxAtStartFootnote
일반 Chess 상태의 수 \(b^d = 35^{80}\)
%
\end{footnote} 이 있는데,
이 테이블을 참조하면 게임의 종반부에서 더 이상게임을 진행할 필요 없이 언제 승패가 결정될 지를 알 수 있다.
일반 체스에서 7개 기물을 대상으로한 Syzygy endgame tablebase의 용량은 약 140TB에 달한다고 알려져 있다.
그러나 Microchess는 기물의 개수가 더 많은 데다가, 승리규칙도 조금 다르기 때문에 이 테이블을 직접 사용할 수 도 없다.


\subsubsection{평가함수}
\label{\detokenize{03-basic_ai_examples:id12}}
One Step Search 부터 ABP Negamax Search 까지 목표상태로 가는 방향을 알아내기 위해 더 깊이 탐색하는 방법을 사용했다.
더 깊이 탐색할 수록 수평선 효과를 완화할 수 있고, 결과적으로 더 높은 성능을 기대할 수 있기 때문이다.
그러나, 깊이 탐색할 수록 지수적으로 늘어나는 상태공간을 탐색해야하고 일정시점을 넘어가면 더 이상 감당할 수 없는 수준이 된다.

때문에 많은 AI들은 여러가지 다른 접근방법을 동시에 사용한다. 그 중 하나가 정교한 {\hyperref[\detokenize{03-basic_ai_examples:eval-func-fig}]{\sphinxcrossref{\DUrole{std,std-ref}{평가함수}}}} 를 사용하는 것이다.
지금까지 예제 AI들은 단순히 기물 점수를 이용해서 현재 상태가 얼마나 좋은지 나쁜지를 판단한다.
그러나, 좋은 성능을 보이는 유명한 AI들은 그보다 훨씬 복잡한 평가함수를 사용한다.
기물의 절대적인 위치, 다른 기물과의 상대적인 위치등 다양한 정보를 이용하면, 간단한 평가함수로 잡아낼 수 없는
좋고 나쁨에 대한 신호를 잡아낼 수 있다. 비록 탐색깊이를 계산가능한 수준까지 줄이더라도 평가함수를 이용하면 수평선 효과를 완화할 수 있다.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{eval_func}.png}
\caption{평가함수}\label{\detokenize{03-basic_ai_examples:eval-func-fig}}\label{\detokenize{03-basic_ai_examples:id38}}\end{figure}

{\hyperref[\detokenize{03-basic_ai_examples:eval-func-fig}]{\sphinxcrossref{\DUrole{std,std-ref}{평가함수}}}} 예 에서 왼쪽은 단순한 탐색기법으로 상태공간을 탐색했는데, 총 7개 노드를 탐색했다.
그러나, 평가함수를 사용한 오른쪽은 단 한 개의 노드만을 탐색하고 평가함수로 그것이 얼마나 좋은지 예측하였다.
만약 평가함수가 충분히 정확하다면, 1개 노드 탐색 + 평가함수 실행으로 7개 노드 탐색을 대체하여 계산비용을 대폭 줄일 수 있다.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{path_finding}.png}
\caption{경로탐색 문제}\label{\detokenize{03-basic_ai_examples:path-finding}}\label{\detokenize{03-basic_ai_examples:id39}}\end{figure}

대부분 평가함수는 문제(게임)에 대한 전문지식을 가진 전문가에 의해 설계되며, 매우 많은 노력이 필요하다.
잘못된 평가함수는 탐색공간을 왜곡하여 탐색알고리즘이 최종 목표상태를 찾는 것을 방해할 위험성이 있다.
출발지점 S 부터 목표지점 G까지 {\hyperref[\detokenize{03-basic_ai_examples:path-finding}]{\sphinxcrossref{\DUrole{std,std-ref}{경로탐색 문제}}}} (a) 에서, 각 상태에서 선택할 수 있는 행동이 4가지(상, 하, 좌, 우 이동)라고하면,
S 부터 G까지 최소거리는 5이기 때문에, 첫 번째 이동방향을 결정하기 위해 기본적인 트리 탐색으로는 최소한 깊이 5를 탐색할 필요가 있다.
따라서, 최소 \(4^5 = 1,024\) 개 노드를 탐색하지 않으면 수평선 효과 때문에, 시작 지점에서 어느 위치로 움직여야 하는지 판단할 수 없다.

경로탐색 문제에서 가장 쉽게 사용할 수 있는 평가함수는 현재 위치와 목표지점사이의 거리가 얼마나 가까운지를 이용하는 것이다.
현재 위치와 목표지점이 가까울 수록 큰 평가를 받는 함수(예. 1/거리; \(1 / (Gx - Sx) + (Gy - Sy)\))를 설계하고 탐색에 사용하면,
트리 탐색으로 목표지점에 도달하지 못하더라도, 어느 방향으로 이동하는것이 좀 더 가까운지를 판단할 수 있기 때문에,
수평선 효과를 완화할 수 있다.

그러나, {\hyperref[\detokenize{03-basic_ai_examples:path-finding}]{\sphinxcrossref{\DUrole{std,std-ref}{경로탐색 문제}}}} (b) 같은 환경에서는 똑같은 평가함수가 경로탐색에 오히려 방해가 된다.
목표지점까지 절대적인 거리가 가까운 최단경로에는 큰 장애물이 있기 때문에, AI는 절대 목표지점에 도착하지 못한다.
이 경우에는 오히려 일시적으로 목표지점으로부터 멀어지는 장애물을 우회하는 경로가 더 가까운 경로지만,
평가함수는 그것을 반영하지 않았다. 이런 문제를 기만적 문제(deceptive problem) %
\begin{footnote}[8]\sphinxAtStartFootnote
Chen, Yang, et al. "Solving deceptive problems using a genetic algorithm with reserve selection." Evolutionary Computation, 2008. CEC 2008.(IEEE World Congress on Computational Intelligence). IEEE Congress on. IEEE, 2008.
%
\end{footnote} 이라고 한다.

{\hyperref[\detokenize{03-basic_ai_examples:path-finding}]{\sphinxcrossref{\DUrole{std,std-ref}{경로탐색 문제}}}} (b)예 에서처럼 문제의 규모가 매우 작고 장애물이 명확하게 보인다면
평가함수를 개선할 수 있지만, 대부분의 문제는 이렇게 쉽게 해결되지 않기 때문에
좋은 평가함수를 설계하는 것은 매우 어려운 작업이다.

이론적으로는, 매우 정교한 평가 함수를 정의할 수 있어서 모든 상태에서 각 상태가 얼마나 상대적으로 좋은지 알아낼 수 있다면,
언제나 현재 상태보다 더 좋은 상태로 연결되는 행동을 선택함으로써, 목표상태에 도달할 수 있다.
이 경우 One Step Search 만으로도 목표상태를 찾아갈 수 있다. 그러나 실질적으로 우리는 각 상태가 목표지점에 얼마나 정확한지
정확하게 알 수 없기 때문에, 우리가 신뢰할만한 평가값을 알 수 있는 상태까지 깊이 탐색할 수 있는 효율적인 탐색알고리즘이 도움이 된다.


\subsubsection{Stockfish}
\label{\detokenize{03-basic_ai_examples:stockfish}}
실제로 강력한 체스 AI들은 빠르고 효율적인 탐색알고리즘(Minimax Search + \(\alpha-\beta\) pruning)과
정교한 평가함수를 결합한 형태로 구현되었다고 알려져 있다. 계산 자원이 한정된 상황에서 탐색능력이 비슷하다면
그들 사이에 성능을 결정짓는 것은 얼마나 정교하고 정확한 평가 함수를 사용하느냐에 달려있다.

Stockfish %
\begin{footnote}[9]\sphinxAtStartFootnote
\sphinxurl{https://stockfishchess.org/}
%
\end{footnote} 는 그 중에서도 가장 성능이 좋은 체스 AI로 알려져 있다.
개인용 데스크톱이나 스마트폰에서 작동할 수 있을 정도로 가볍지만, 체스 AI 중에서 가장 높은 성능을 보여준다.
이 플랫폼에서도 {\hyperref[\detokenize{agents.stockfish:agents.stockfish.agent.Stockfish}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Stockfish}}}}} 래퍼로 Stockfish 8을 사용할 수 있다.

경진대회 플랫폼에서 직접 지원하지는 않지만,
원래 체스 AI 분야에서는 여러가지 방식으로 구현된 체스 AI(일반적으로는 체스 엔진 이라고 표현)들이
서로 게임을 플레이할 수 있도록 UCI(Universal Chess Interface) %
\begin{footnote}[10]\sphinxAtStartFootnote
\sphinxurl{https://it.wikipedia.org/wiki/Universal\_Chess\_Interface}
%
\end{footnote} 라는 공통 인터페이스를 정의했다.
이 플랫폼에서는 python-chess의 UCI 지원기능 %
\begin{footnote}[11]\sphinxAtStartFootnote
\sphinxurl{http://python-chess.readthedocs.io/en/latest/uci.html}
%
\end{footnote} 을 이용해 다른 언어(C++)로 구현된 Stockfish를 간접적으로 지원한다.
UCI는 PIPE를 이용해 서로 인터페이스 하므로, 이것을 구현하기만 하면 python 이외에 다른 언어로도 AI를 구현할 수 있다.

Stockfish는 매우 효율적인 탐색알고리즘과 정교한 평가함수를 가지고 있기 때문에 Microchess에서도 뛰어난 성능을 보여준다.
다만, 경진대회의 평가기준과 달리 일반 체스의 규칙을 따르기 때문에
일반 체스에서 Stockfish의 실력보다 이 플랫폼의 Stockfish는 낮은 성능을 보이고 있을 가능성이 높다.
예를들어, 경진대회 규칙으로는 패배지만, 일반 체스의 규칙으로는 무승부라면 Stockfish는 무승부 상태라고 잘못 판단하기 때문에, 실제 게임에서는 패배할 것이다.
이런 제약에도 불구하고, Stockfish는 상당히 강력해서, 지금까지 소개한 예제 AI들은 Stockfish를 이기기 매우 어렵다.

하지만, 경진대회에서 규칙에서 기존 Stockfish의 성능을 더 높이는 작업은 매우 어려울 가능성이 높다.
빠르고 효율적인 탐색 알고리즘 구현은 체스와 관련된 전문지식과 거의 상관없이 때문에 쉽게 재활용 가능하지만,
일반 체스에 최적화된 평가함수를 Microchess에 맞게 수정하는 것은, 매우 많은 시행작오가 필요할 것이다.

\newpage


\section{Monte Carlo Tree Search}
\label{\detokenize{04-mcts::doc}}\label{\detokenize{04-mcts:monte-carlo-tree-search}}
지금까지는 상태공간 탐색에 대한 기본적인 사항을 알아보았고, 탐색기반 예제 AI들과 일반 체스에서 가장 강력한 AI인 Stockfish를 살펴보았다.
Microchess가 간단함 게임에도 불구하고, 탐색공간이 매우 커질 수 있기 때문에, 간단한 예제 AI들의 성능은 충분하지 않았다.
심지어 기존에 가장 강력한 체스 AI인 Stockfish라고 하더라도
평가기준이 달라진 경진대회에서는 상당히 정밀한 수정이 없이는 기존 성능을 보여주지 못하기 때문에,
이 이상의 성능을 위해서는 새로운 방식으로 AI를 구현해야만 한다.
지금부터는 MCTS (Monte-Carlo Tree Search)와 Self Learning으로 Stockfish 이상의 성능을 달성하는 방법을 보인다.


\subsection{MCTS 개요}
\label{\detokenize{04-mcts:mcts}}
MCTS (Monte-Carlo Tree Search) %
\begin{footnote}[1]\sphinxAtStartFootnote
Browne, Cameron B., et al. "A survey of monte carlo tree search methods." IEEE Transactions on Computational Intelligence and AI in games 4.1 (2012): 1-43.
%
\end{footnote} 의 기본 아이디어는 탐색 공간이 거대하여 모두 탐색이 불가능하다면,
무작위로 상태공간을 표본추출(sampling)하여 통계적인 근사치를 구하고, 그 근사치에 근거한 의사결정을 하겠다는 것이다.
만약 충분히 많은 표본추출을 할 수 있다면 어떤 행동을 하는 것이 가장 승률이 높은지를 추정할 수 있다.
하나의 표본을 추출하는 과정은 현재 상태부터 마지막(예. 게임종료)까지 무작위로 행동을 결정하여 첫 번째 행동의 보상(평가값)을 알아내는 것이며,
이것을 시뮬레이션이라고 한다.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{simple_mcts}.png}
\caption{간단한 MCTS}\label{\detokenize{04-mcts:simple-mcts}}\label{\detokenize{04-mcts:id6}}\end{figure}

{\hyperref[\detokenize{04-mcts:simple-mcts}]{\sphinxcrossref{\DUrole{std,std-ref}{간단한 MCTS}}}} 는 현재 상태( \(S_0\) )에서 여러 번 무작위로 시뮬레이션(점선)을 하고,
현재 선택 할 수 있는 행동들의 기대 보상을 추정하는 것이다.
무작위 시뮬레이션을 충분히 많이 해볼 수 있다면, 특정 상태에서 행동마다 기대보상을 추정할 수 있기 때문에,
전체 공간을 탐색하지 않더라도, 현재 상태에서 의사결정을 할 수 있다.
이 방법의 성능은 충분한 시뮬레이션에 달려있기 때문에, 기본 계산비용이 큰 경우가 많다.

단순 MCTS를 개선한 것이 UCT (Upper Confidence bound Tree search) \sphinxfootnotemark[1] 알고리즘이다. 이 알고리즘은 시뮬레이션을 할 때
완전히 무작위로 행동을 결정하는 대신, 이전에 시뮬레이션한 정보를 이용하여 시뮬레이션의 효율을 개선한 것이다.
요즘은 보통 UCT를 그냥 MCTS라고 부른다.


\subsection{Multi-Armed Bandit}
\label{\detokenize{04-mcts:multi-armed-bandit}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{mcts_multi_armed_bandit}.png}
\caption{MCTS \(\rightarrow\) Multi-Armed Bandit}\label{\detokenize{04-mcts:mcts-mab-fig}}\label{\detokenize{04-mcts:id7}}\end{figure}

MCTS에서 특정 한 상태를 떼어놓고 보면( {\hyperref[\detokenize{04-mcts:mcts-mab-fig}]{\sphinxcrossref{\DUrole{std,std-ref}{MCTS \(\rightarrow\) Multi-Armed Bandit}}}} ), 이것은 비결정적인 보상을 반환하는 환경에서 특정 상태에서 어떤 행동을 하는것이
더 좋은지를 알아내는 문제이다. 지금까지 살펴본 탐색 기반 AI에서 취하는 기본 접근방법은 결정적인 환경을 가정하고,
특정 상태에서 가능한 행동들을 모두 시뮬레이션 해본다음, 그 결과를 바탕으로 어떤 행동이 더 좋은지 알아내는 것이다.
그러나, 만약 시뮬레이션 결과가 (MCTS의 무작위 탐색 때문에) 시뮬레이션을 수행 할 때마다 달라진다면,
가능한 행동들 중에서 더 좋은 행동을 찾기위해서는 모든 행동들을 여러 번 반복하여 시뮬레이션 해보고,
통계적인 추정을 할 필요가 있다. 시뮬레이션을 하면 할 수록 추정이 정확해 지겠지만, 계산비용을 절약하기 위해서는
최소한의 시뮬레이션 만으로 정확한 추정을 할 필요가 있다.

이런 문제를 탐색과 활용 딜레마(exploration-exploitation dilemma)라고 한다.
탐색과 활용 딜레마가 발생하는 가장 간단한 문제는 MAB (Multi-Armed Bandit) 문제이다.
여기서 Bandit란 슬롯머신을 의미한다. 만약 상금이 나올 확률이 서로 다른 여러 대의 슬롯머신이 있다면,
가장 높은 상금을 타는 방법은 어떤 슬롯머신이 더 확률이 높은지를 알아내고, 그 슬롯머신에서 게임을 계속 시도하는 것이다.
그러나 문제는 슬롯머신에서 상금을 탈 확률을 알아내기 위해서는 충분한 게임(실험)을 해서,
통계적인 추정치를 알아내야만 한다는 것이다. 투자할 수 있는 총 자금에는 한계가 있고,
실험에는 비용이 들기 때문에 지나친 게임은 최종 기대이익을 낮출 수 있다.

이 문제에 대한 가장 간단한 해법 중 하나는 \(\epsilon-greedy\) 알고리즘이다.
이 알고리즘은 전체 자원 중 일정 비율(\(\epsilon\))을 실험을 위해 사용하는 알고리즘이다.
예를 들어 현재 가진 돈으로 1,000게임을 할 수 있다면, 그 중 500게임을 실험을 위해 사용하는 식이다.
2대의 슬롯머신이 있다면, 500게임을 250게임씩 균등하게 나눠서 실험해보고, 그 중에서 상금이 더 높은 슬롯머신에서
나머지 500게임을 하는 것이다. 흔히 처음에 실험을 위해 하는 500게임을 탐색(exploration)이라고 하고,
나중에 상금을 얻기 위해 하는 행동을 활용(exploitation)이라고 한다.

이 방법은 직관적으로 알 수 있다시피, 탐색과 활용의 비율을 잘 조절하는 것이 기대이익을 높이는 핵심이다.
탐색에 너무 많은 게임을 하면, 이익을 극대화 시키기 위한 활동인 활용의 비율이 낮아져 최종 상금이 낮아진다.
반면, 탐색에 너무 적은 게임을 할당하면, 부정확한 추정에 근거해 활용을 하기 때문에, 최종 기대보상이 낮아진다.
따라서, 탐색과 활용의 비율을 적절히 조정하는 것이 이 문제를 잘 해결하는 방법이다.
이 문제, 탐색과 활용 딜레마(dilemma)는 강화학습에서 중요한 문제 중 하나이다.

UCB (Upper Confidence Bound)는 \(\epsilon\) 보다 좀 더 정교한 방식으로 탐색과 활용을 조정한다.
반복해서 시뮬레이션을 하면서, 얻은 예측의 신뢰도(upper confidence)를 이용해 현재 추정치가 얼마나 신뢰할 만한지를 판단한다.
\(\epsilon\) 처럼 명확하게 탐색과 활용을 분리하지 않고, 언제나 각 행동에 대해 현재 보상과 신뢰도에 의해 결정된
UCB 값이 가장 높은 행동을 결정한다. 기본적으로 기대 보상이 높으며, 신뢰도가 낮은 행동일 수록 선택될 가능성이 높다.
\eqref{equation:04-mcts:ucb1} 은 가장 대표적인 UCB 공식이다. 첫 번째 항은 활용에 대응되는 기대보상(평균)이며,
두 번째 항은 탐색에 대응되는 값이다. 두 번째 항의 N은 전체 시뮬레이션 횟수이고, n\_a는 그 중에서 특정 행동 a를 시뮬레이션한
횟수이다. 따라서, 전체 시뮬레이션 횟수에 비해서 a를 시뮬레이션 한 횟수가 클 수록 두번째 항은 자연스럽게 줄어든다.
\begin{equation}\label{equation:04-mcts:ucb1}
\begin{split}UCB_a = \frac{v_a}{n_a} + \sqrt{\frac{2log{N}}{n_a}}\end{split}
\end{equation}
\(\epsilon\) -greedy 는 적절한 \(\epsilon\) 를 알고 있다면, 좋은 결과를 얻을 수 있지만,
좋은 \(\epsilon\) 값을 모를 경우 UCB가 보다 좋은 성능을 보이는 경우가 많다. 다만, 끝까지 탐색을 멈추지 않기 때문에
충분히 탐색을 한 뒤에도 예상과 달리 전혀 엉뚱한 행동을 선택하기도 한다.


\subsection{Upper Confidence Tree}
\label{\detokenize{04-mcts:upper-confidence-tree}}
MCTS의 시뮬레이션은 결국 각 단계에서 MAB 문제를 연속적으로 풀고 있는 것으로 볼 수 있다.
각 깊이에서 아직 충분히 시도해 보지 않은 행동을 실험을(탐색) 할 것인지, 아니면 지금까지 시뮬레이션 한 결과 가장 좋았던
행동에서 연결되는 미래상태를 더 시뮬레이션하여 추정치를 정교하게 갱신(활용)할 것인지를 끊임없이 결정하는 것이다.
UCT (Upper Confidence Tree)는 이 과정에 UCB를 사용하여 MCTS의 성능을 높인 것이다.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{uct}.png}
\caption{UCT 알고리즘}\label{\detokenize{04-mcts:uct-algorithm}}\label{\detokenize{04-mcts:id8}}\end{figure}

무작위 탐색(Monte-Carlo simulation)만을 하는 기존 MCTS와 달리 UCT는 UCB를 사용하는 Tree Policy 와
Default Policy (무작위 탐색) 단계로 구분된다. {\hyperref[\detokenize{04-mcts:uct-algorithm}]{\sphinxcrossref{\DUrole{std,std-ref}{UCT 알고리즘}}}} 에 UCT의 네 단계를 설명하였다.
첫 번째 선택 단계에서는 \eqref{equation:04-mcts:ucb1} 을 이용하여 현재 상태에서 UCB값이 가장 큰 행동을 선택한다.
제일 처음에 시뮬레이션을 시작할 때나 현재까지 탐색을 해본 상태를 따라 깊게 내려가다보면,
아직 한번도 시도해보지 않은 행동이 나타난다.
이때, 두 번째 확장 단계로 넘어간다. 이 단계에서는 아직 시도해보지 않은 행동들 중에서 무작위로 하나를 골라
현재까지 탐색해본 트리의 끝부분에 상태 노드를 하나 추가한다. 이 두 단계를 Tree Policy라고 한다.
세 번째 단계에서는 기존 MCTS 처럼 게임이 종료되는 상태까지 무작위로 트리를 탐색한다. 이것을 Default Policy라고 한다.
마지막 네 번째 단계에서는 게임이 종료되었을 때의 결과를 역전파하여 트리 노드의 기대 보상값을 갱신한다.
갱신된 보상값은 다음 반복(iteration)때, Tree Policy에서 사용된다.

\def\sphinxLiteralBlockLabel{\label{\detokenize{04-mcts:mcts-code}}\label{\detokenize{04-mcts:id9}}}
\sphinxSetupCaptionForVerbatim{MCTS 기본코드}
\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{v0} \PYG{o}{=} \PYG{n}{Node}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{turn}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}
\PYG{n}{start\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{perf\PYGZus{}counter}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{simulation\PYGZus{}count} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{k}{while} \PYG{n+nb+bp}{True}\PYG{p}{:}
   \PYG{k}{if} \PYG{n}{simulation\PYGZus{}count} \PYG{o}{\PYGZgt{}} \PYG{n}{n\PYGZus{}simulations} \PYG{o+ow}{or} \PYG{n}{time}\PYG{o}{.}\PYG{n}{perf\PYGZus{}counter}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{start\PYGZus{}time} \PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{0.99} \PYG{o}{*} \PYG{n}{timeout}\PYG{p}{:}
      \PYG{c+c1}{\PYGZsh{} 제약조건(시뮬레이션 횟수와 시간제한)을 초과하면 바로 시뮬레이션 종료}
      \PYG{k}{break}

   \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}depth} \PYG{o}{=} \PYG{l+m+mi}{0}
   \PYG{c+c1}{\PYGZsh{} Tree Policy: 선택 및 확장}
   \PYG{n}{vl} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree\PYGZus{}policy}\PYG{p}{(}\PYG{n}{v0}\PYG{p}{)}
   \PYG{c+c1}{\PYGZsh{} Default Policy: Monte\PYGZhy{}Carlo 시뮬레이션}
   \PYG{n}{delta} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{default\PYGZus{}policy}\PYG{p}{(}\PYG{n}{vl}\PYG{o}{.}\PYG{n}{state}\PYG{p}{)}
   \PYG{c+c1}{\PYGZsh{} Backup: 보상 역전파}
   \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{backup}\PYG{p}{(}\PYG{n}{vl}\PYG{p}{,} \PYG{n}{delta}\PYG{p}{)}
   \PYG{n}{simulation\PYGZus{}count} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
\end{sphinxVerbatim}

MCTS의 전체 흐름은 {\hyperref[\detokenize{04-mcts:mcts-code}]{\sphinxcrossref{\DUrole{std,std-ref}{MCTS 기본코드}}}} 에서 볼 수 있다. 주어진 자원(시뮬레이션 횟수 또는 시간제한)동안
계속 시뮬레이션을 반복하면서, Tree를 확장해 간다.

이 네 단계를 한번 수행하는 것을 시뮬레이션이라고 하고, 이 과정을 충분히 반복하면 현재 상태(root node)에서 각 행동들의
기대 보상값을 추정할 수 있다. 최종 의사결정을 할 때는 행동의 기대보상값(보상 / 실험 횟수)가 가장 높은 행동을 고르기도 하지만,
시도 횟수가 가장많은 것을 고르는 경우가 많다.

Minimax 같은 기존 탐색 알고리즘은 기본적으로 수평선 효과를 완화하기 위해 깊은 탐색이 필요하다.
탐색의 깊이가 깊어질 수록 탐색해야 할 상태가 지수적으로 증가하기 때문에,
계산복잡도를 대강 \(O(b^d)\) 로 생각할 수 있다 (\sphinxstyleemphasis{b}: 평군 선택가능한 행동, \sphinxstyleemphasis{d}: 탐색깊이).
반면에 MCTS는 시뮬레이션 횟수(Monte-Carlo 샘플링 횟수)에 따라 계산비용이 증가한다.
대강의 계산복잡도는 \(O(nd)\) 로 볼 수 있다(\sphinxstyleemphasis{n}: 시뮬레이션 횟수, \sphinxstyleemphasis{d}: 탐색깊이).
따라서 상태공간의 규모가 커질 수록 상대적으로 적은 계산 비용만을 필요로 한다.

주의할 점은, MCTS로 충분한 성능을 얻으려면, 충분힌 시뮬레이션이 필요하다는 것이다..
매우 오랜시간이 걸리지만 최적해를 찾는 것이 모장된 기존 탐색알고리즘과 달리,
MCTS는 근사해를 구하는 알고리즘이기 때문에 최적해를 찾는다고 보장할 수 없다.
이것을 해결하는 직접적인 방법은 충분한 시뮬레이션 횟수를 확보하는 것 뿐이다.
그러나, 너무 큰 시뮬레이션 횟수도 불필요하다.
비록 이론적으로는 MCTS의 시간복잡도가 선형으로 증가하기는 하지만,
실제구현에서는 그 외 요소도 있기 때문에 너무 큰 시뮬레이션 횟수는 큰 부담이 되는 경우가 많다.
때문에 대부분의 구현에서는 시뮬레이션을 몇 번할 것인지 미리 지정해두기 보다는,
주어진 시간에 맞춰 최대한 많이 시뮬레이션을 하는 방식을 사용한다.
이 경우 만약 주어진 시간이 변경되더라도 그에 맞춰 언제나 최선의 성능(근사해)을 보일 수 있다.


\subsection{MCTS 예제}
\label{\detokenize{04-mcts:id3}}
MCTS 예제는 {\hyperref[\detokenize{agents.search:agents.search.mcts_agent.MCTSAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{MCTSAgent}}}}} 에 구현되어 있다.
기본 MCTS 구현은 Microchess 정도의 문제이서도 충분한 성능을 보이지 못하기 때문에, 두 가지를 수정 했다.

\sphinxstylestrong{탐색깊이 제한}

원래 MCTS는 게임의 종료상태까지 탐색을 시도하고, 종료상태에서 승리했는지 패배했는지에 따라 보상을 받고,
그 정보를 이용해 의사결정을 하는 것이지만, 실질적으로는 너무 깊은 공간을 탐색하는 것은 큰 계산비용이 소모된다.
뿐만아니라, 탐색할 공간이 커질 수록 충분히 정확한 결과를 얻기 위해 시뮬레이션의 횟수가 많이 필요하기 때문에
탐색깊이를 최대 6으로 제한하고, 마지막에서 평가함수를 사용하여 계산비용을 절감하였다.
비록 정교하지 못한 평가함수로 인해 보상의 추정이 왜곡될 위험이 있기는 하지만,
주어진 시뮬레이션 횟수에서 탐색깊이를 제한하지 않으면, 부정확한 추정으로 인해 명백히 나쁜 수를 둘 확률이 높았다.
체스 같은 턴제 게임에서는 한번의 나쁜 수가 승패를 가를 정도로 중요하기 때문에, 이것을 막기위해 탐색깊이를 제한했다.

\sphinxstylestrong{보상신호 강화}

탐색깊이를 제한했지만, 깊이가 깊어질 수록 노드의 개수가 10배씩 증가하고, 보상신호는 1/10씩 감소하기 때문에,
깊이 6정도에서도 보상신호는 매우 약해졌다. 즉 현재 상태에서 좋은 행동과 나쁜 행동의 차이를 구분하기 어려워졌다.
게다가, 탐색깊이를 제한하고, 평가함수를 사용하면서 가장 좋은 상태와 가장 나쁜 상태의 차이는 더 작아졌다.
이 문제를 완화하기 위해 보상을 평가함수의 출력을 그대로 사용하지 않고,
마지막 상태가 현재 상태보다 좋은을 때는 보상 1, 나쁠 때는 보상 0으로 사용하였다.
이 변경으로 인해 상대적인 보상의 크기가 훨씬 커지게 되었고, AI는 보다 그럴듯하게 작동하게 되었다.
다만, 마지막 상태가 현재 상태보다 좋기만하다면 모두 동일하게 보상 1을 받기 때문에 더 좋은 수와 덜 좋은 수를 구분하지는
못하게 되었다.

탐색깊이 제한과 보상신호를 강화하기 전에 MCTS는 너무 낮은 보상신호 때문에, 종종 매우 나쁜 수를 두고는 했다.
한 게임 동안에 반드시 몇 번을 그런 수를 두기 때문에 승률을 높이기 어려웠다.
하지만, 개선한 MCTS는 훨씬 안정적이고 그럴듯하게 작동했다.

구현한 MCTS 예제의 성능을 평가하기 위해 Stockfish와 비교하였다.
Stockfish는 비록 Microchess 용으로 개발된 AI도 아니고, 승리/패배 규칙도 경진대회 규칙과 다르지만,
상당히 강력한 성능을 보인다. 다른 예제 AI는 Stockfish를 상대로 0.3 이상의 승률을 보이기 어렵다.
게임을 잘 플레이한 경우에도 Stockfish를 상대로 승리하기는 어렵고, 겨우 비기는 경우였다.

하지만, MCTS는 Stockfish를 상대로 평균(40게임) 0.562의 승률을 보였다. White로 20게임 Black으로 20게임을 했는데,
White일 때는 0.85, Black일 때는 0.275를 기록했다. 일정 수준의 실력을 보이는 AI끼리는 누가봐도 명백한 실수를 하지 않기
때문에 먼저시작하는 White로 할 때 이기고, 나중에 두는 Black일 때 진다.
따라서 평균 승률을 높이기 위해서는 White일때 확실히 이겨야 하고, Black일 때 비기거나 져야 한다.
MCTS도 Black일 때 이견 경우는 20게임 중 불과 2게임에 불과하다.

Stockfish는 체스에 대한 전문적인 지식을 가지고 설계된 매우 정교한 평가함수와
매우 빠른 탐색 알고리즘을 사용하기 때문에 매우 가벼우면서 높은 성능을 가지고 있다.
하지만, 이 경진대회 처럼 게임의 규칙이 바뀐 경우, 여기에 대처하도록 개선하는 것은 쉽지 않다.
Microchess는 일반 체스와 유사하기 때문에, 어느 정도 성능을 보일 수 있었지만, 만약 전혀 다른 보드 게임이라면
Stockfish가 사용하는 정교한 평가함수는 전혀 사용할 수 없을 것이기 때문에, 일정 규모 이상의 문제에서는 빠르고 효율적인
탐색 알고리즘으로도 해결하기 어려울 것이다.

반면 MCTS는 전문적인 지식에 거의 의존하지 않는다. 비록 예제 MCTS에서는 문제의 규모를 축소하기 위해
간단한 평가함수를 도입했지만, 이것은 매우 간단하기 때문에 다른 문제에서도 쉽게 이 수준의 평가함수를 구현할 수 있다.
그 대신에 MCTS는 수 많은 시뮬레이션으로 근사해를 찾아낸다.
실제로 예제 MCTS가 높은 성능을 내기 위해서는 사용할 수 있는 모든 시간동안 계속 시뮬레이션을 해야만 했다.
경진대회에서 한 턴에 약 10초를 가정했기 때문에, MCTS는 10초동안 약 7,000\textasciitilde{}12,000번의 시뮬레이션을 수행했다(Intel i7-7700).
만약 시간이 반 이하로 주어지거나, 실행하는 PC의 사양이 낮다면, MCTS가 Stockfish를 상대로 높은 승률을 보이기는 어려울 것이다.


\subsection{MCTS 시각화}
\label{\detokenize{04-mcts:id4}}
MCTS AI는 10초 시간제한으로 {\hyperref[\detokenize{agents.search:agents.search.mcts_agent.MCTSAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.search.mcts\_agent.MCTSAgent}}}}} 와 5000회 시뮬레이션 횟수 제한으로 실행되는
{\hyperref[\detokenize{agents.search:agents.search.mcts_agent.MCTSAgentDev}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.search.mcts\_agent.MCTSAgentDev}}}}} 가 있다.
MCTSAgentDev가 작동하기 전에 visdom server를 실행시켜두면 {\hyperref[\detokenize{04-mcts:mcts-visualization}]{\sphinxcrossref{\DUrole{std,std-ref}{MCTS 의사결정과정}}}} 을 볼 수 있다.
시각화 기능에도 무시하기 어려운 계산비용이 필요하기 때문에, 기본적으로 개발용 버전인 MCTSAgentDev만 기능이 활성화 되어있다.

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} visdom 서버 실행
(mchess) \PYGZti{}/ python \PYGZhy{}m visdom.server

\PYGZsh{} 다른 콘솔 창에서
(mchess) \PYGZti{}/ python scripts/run\PYGZus{}game.py \PYGZhy{}\PYGZhy{}white=mcts\PYGZus{}dev \PYGZhy{}\PYGZhy{}black=mcts
\end{sphinxVerbatim}

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{mcts_visualization}.png}
\caption{MCTS 의사결정과정}\label{\detokenize{04-mcts:mcts-visualization}}\label{\detokenize{04-mcts:id10}}\end{figure}

\newpage


\section{Self Learning AI}
\label{\detokenize{05-self_learning:self-learning-ai}}\label{\detokenize{05-self_learning::doc}}
MCTS를 포함해 트리 탐색 알고리즘의 성능을 개선하는 방법은 크게 두 가지가 있다.
첫째, 탐색할 가치가 있는 상태를 선별적으로 탐색한다. 둘째, 정교한 평가함수를 사용한다.
첫 번째 방법은 흔히 가지치기(pruning)라고 불리는 방법으로, 여러 행동들 중에서 실제로 탐색해보지 않더라도,
결과가 나쁠 것으로 예측 되는 것을 무시하는 기법이다. \(\alpha-\beta\) pruning 도 이런 기법의 일종이다.
이런 기법들은 \(O(b^d)\) 에서 \sphinxstyleemphasis{b} 를 줄이는 효과가 있다.
두 번째 방법은 높은 성능을 보이는 많은 AI에서 사용하는 방법이다. 전문가에 의해 혹은 데이터에 기반해 설계된 정교한
평가함수를 이용해 실제 시뮬레이션을 해보지 않고 미래를 예측하는 것이다. 실제 결과와 차이가 있을 가능성이 있지만,
잘 설계한다면 \(O(b^d)\) 에서 \sphinxstyleemphasis{d} 를 크게 줄일 수 있다.
\(\alpha-\beta\) pruning 은 특정 게임에 대한 전문지식에 의존하지 않지만, 이런 기법들의 대부분은 특정 게임에 강하게 종속되어 있다.
때문에 Stockfish가 아무리 강력한 체스 AI라고 해도, 장기나 바둑같은 다른 보드게임을 전해 플레이 할 수 없는 것이다.
반면 MCTS는 전문지식에 거의 의존하지 않기 때문에 최소한의 변경만으로도 다른 게임에 쉽게 적용가능하다.
하지만, 기본 계산비용이 크기 때문에 계산비용을 줄이기 위한 방법이 필요하고, 결국 가지치기나 평가함수를 도입할 필요가 있다.
단, 가지치기나 평가함수가 전문지식에 의존하지 않아야 MCTS의 장점을 유지할 수 있다.

Alpha Go Zero %
\begin{footnote}[1]\sphinxAtStartFootnote
Silver, David, et al. "Mastering the game of go without human knowledge." Nature 550.7676 (2017): 354.
%
\end{footnote} 는 이 문제를 해결한 가장 유명한 예이다. 기본 의사결정은 Monte-Carlo Tree Search (MCTS)를 사용하면서,
인공신경망을 이용해 가지치기와 평가를 수행한다.
기존 Alpha Go %
\begin{footnote}[2]\sphinxAtStartFootnote
Silver, David, et al. "Mastering the game of Go with deep neural networks and tree search." nature 529.7587 (2016): 484-489
%
\end{footnote} 도 이것과 동일한 접근방법이지만, 인공신경망을 학습할 때, 사람들이 플래이한 기보를 이용해 기본학습을 진행했다.
아무것도 모르는 처음(scratch)부터 학습하는 것은 어렵기 때문에 기보 데이터를 이용해 기본적인 플레이방법을 학습하고,
그 이후 강화학습으로 성능을 향상시키는 방법을 사용했다.
반면 Alpha Go Zero 는 처음부터 기보 데이터를 전혀 사용하지 않고, AI 가 스스로 게임을 플레이(Self Play) 하면서
게임 데이터를 수집하고 학습 하였다.
따라서, Alpha Go Zero는 전혀 탐색알고리즘, 가지치기, 그리고 평가함수까지 알고리즘의 거의 모든 부분이 전문지식(도메인 지식)에 독립적이다.
그 덕분에 Alpha Go Zero가 발표된지 불과 몇 달 뒤에 바둑 뿐 아니라 다른 보드 게임을 모두 플레이 할 수 있는 Alpha Zero %
\begin{footnote}[3]\sphinxAtStartFootnote
Silver, David, et al. "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm." arXiv preprint arXiv:1712.01815 (2017).
%
\end{footnote} 가 공개되었다.
Alpha Zero는 학습을 시작한지 불과 4시간만에 기존에 가장 강력한 AI였던 Stockfish를 뛰어넘는 성능에 도달할 수 있었다.

Self Learning AI 예제는 Alpha Go Zero와 유사한 방식으로 구현되었다. 실제 Alpha Zero는 매우 큰 계산비용을 요구하기 때문에,
최종 학습성능은 부족하더라도, 보다 빠르게 학습 결과를 확인할 수 있도록, 보다 간단하게 구현했다.
평가방식으로는 MCTS AI와 마찬가지로 Stockfish를 대상으로 승률을 측정했다.
MCTS AI는 약 7,000\textasciitilde{}12,000번의 시뮬레이션(Intel i7-7700에서 10초 제한)에서 근소한 차이로 Stockfish를 이기는 수준이었지만,
약 4일을 학습한 인공신경망으로 성능을 강화한 Self Learning AI는 동일한 실험에서 승률 0.7을 기록했다.
특히 Self Learning AI가 White 일 때는 거의 모든 게임을 이겨서 승률 0.925를 기록했으며,
Black일 때도 대부분 게임을 무승부로 끝내서 0.475를 기록했다.


\subsection{Self Learning 알고리즘}
\label{\detokenize{05-self_learning:self-learning}}
\def\sphinxLiteralBlockLabel{\label{\detokenize{05-self_learning:self-learning-code}}\label{\detokenize{05-self_learning:id9}}}
\sphinxSetupCaptionForVerbatim{Self Learning 알고리즘}
\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{memory} \PYG{o}{=} \PYG{n}{ReplayMemory}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{best\PYGZus{}model} \PYG{o}{=} \PYG{n}{create\PYGZus{}mode}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{current\PYGZus{}model} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{best\PYGZus{}model}\PYG{p}{)}

\PYG{k}{while} \PYG{n+nb+bp}{True}\PYG{p}{:}
     \PYG{c+c1}{\PYGZsh{} self play: best\PYGZus{}model 끼리 게임 플레이하고, 학습 데이터 생성}
     \PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}self\PYGZus{}play}\PYG{p}{)}\PYG{p}{:}
         \PYG{n}{state}\PYG{p}{,} \PYG{n}{pi}\PYG{p}{,} \PYG{n}{legal\PYGZus{}moves}\PYG{p}{,} \PYG{n}{win} \PYG{o}{=} \PYG{n}{play\PYGZus{}game}\PYG{p}{(}\PYG{n}{best\PYGZus{}model}\PYG{p}{,} \PYG{n}{best\PYGZus{}model}\PYG{p}{,} \PYG{n}{exploration}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}
         \PYG{n}{reward} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{win} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}
         \PYG{n}{memory}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{pi}\PYG{p}{,} \PYG{n}{legal\PYGZus{}moves}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{)}

     \PYG{c+c1}{\PYGZsh{} training: 학습 데이터로 current\PYGZus{}model 학습}
     \PYG{n}{train\PYGZus{}date} \PYG{o}{=} \PYG{n}{memory}\PYG{o}{.}\PYG{n}{get\PYGZus{}minibatch}\PYG{p}{(}\PYG{p}{)}
     \PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}train}\PYG{p}{)}\PYG{p}{:}
         \PYG{n}{current\PYGZus{}model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{train\PYGZus{}data}\PYG{p}{)}

     \PYG{c+c1}{\PYGZsh{} evaluation: current\PYGZus{}model이 best\PYGZus{}model보다 좋아지면}
     \PYG{c+c1}{\PYGZsh{}    current\PYGZus{}model을 best\PYGZus{}model로 교체}
     \PYG{n}{total\PYGZus{}wins} \PYG{o}{=} \PYG{l+m+mi}{0}
     \PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}evaluations}\PYG{p}{)}\PYG{p}{:}
         \PYG{n}{state}\PYG{p}{,} \PYG{n}{pi}\PYG{p}{,} \PYG{n}{legal\PYGZus{}moves}\PYG{p}{,} \PYG{n}{win} \PYG{o}{=} \PYG{n}{play\PYGZus{}game}\PYG{p}{(}\PYG{n}{current\PYGZus{}model}\PYG{p}{,} \PYG{n}{best\PYGZus{}model}\PYG{p}{,} \PYG{n}{exploration}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{)}\PYG{p}{:}
         \PYG{n}{total\PYGZus{}wins} \PYG{o}{+}\PYG{o}{=} \PYG{n}{win}

     \PYG{k}{if} \PYG{n}{total\PYGZus{}wins} \PYG{o}{/} \PYG{n}{n\PYGZus{}evaluations} \PYG{o}{\PYGZgt{}} \PYG{l+m+mf}{0.6}\PYG{p}{:}
         \PYG{n}{best\PYGZus{}model} \PYG{o}{=} \PYG{n}{copy}\PYG{p}{(}\PYG{n}{current\PYGZus{}model}\PYG{p}{)}
\end{sphinxVerbatim}

{\hyperref[\detokenize{05-self_learning:self-learning-code}]{\sphinxcrossref{\DUrole{std,std-ref}{Self Learning 알고리즘}}}} 는 전체 알고리즘의 의사코드이다. 이 코드는 실제 예제 코드가 아니라, 설명하기 위해 단순화 시킨 코드이다.
이 알고리즘은 기본적으로 현재 학습한 가장 좋은 모델(인공신경망)끼리 게임을 플레이한 데이터를 이용해서 현재 모델을 학습하는 것이다.
한번 반복할 때마다, 현재 모델이 기존의 가장 좋은 모델보다 좋아졌는지 평가하고, 더 좋다고 판단될 경우 교체하기 때문에,
점진적으로 성능을 향상시켜 나갈 수 있다.


\subsection{인공신경망}
\label{\detokenize{05-self_learning:id4}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{neural_network}.png}
\caption{인공신경망 구조}\label{\detokenize{05-self_learning:neural-network-architecture}}\label{\detokenize{05-self_learning:id10}}\end{figure}

Best model과 Current model은 현재 게임상태를 입력받아, 다음에 둘 수의 확률과 평가값을 출력하는 인공신경망이다.
예제 AI가 기본으로 사용하는 {\hyperref[\detokenize{05-self_learning:neural-network-architecture}]{\sphinxcrossref{\DUrole{std,std-ref}{인공신경망 구조}}}} 는 Convolution layer 6개 층에 Linear layer를 결합한
간단한 구조를 가지고 있다. Observation과 State 노드가 입력 노드이고, Policy와 Value 노드가 출력노드이다.
시각적으로 보이는 정보는 Observation 노드로 입력되어 64개의 필터(필터 크기: 3 \(\times\) 3 )를 가지고있는 6개의 Convolution layer를 지난다.
그 뒤에 비시각적인 정보가 저장된 2차원 State노드와 결합하여 Linear layer를 거치고 최종적으로는 Policy와 Value 노드로 예측 값이 출력된다.

Observation 는 \(12 \times 5 \times 4\) 차원의 배열로 되어있다.
Observation에 입력할 정보는 현재 보드 상태 수치연산에 적합하도록 배열 형태로 변형해야한다.
첫 번째 차원은 12종류의 기물마다 할당되어 있고, 두 번째와 세 번째 차원은 기물의 위치를 의미한다. 특정 기물이 특정 위치에 있으면
배열에서 해당하는 부분의 값은 1로 할당하고, 나머지 부분은 0으로 한다.
그외 시각적으로 보이는 게임 상태이외의 정보를 입력하기 위한 것이 State이다. 여기에는 캐슬링 가능여부가 기록되어 있다.
캐슬링이 가능하면 1, 불가능하면 0으로 설정한다.

학습옵션 mirror에 따라 차원이 할당되는 부분이 달라진다. 기본적으로는 mirror 옵션이 켜져있는데, 이 상태에서는
내 기물이 Observation의 1\textasciitilde{}6 채널을 사용하고 상대방의 기물이 6\textasciitilde{}12채널을 사용한다. State에서도 내가 0차원을 상대방이 1차원을 사용한다.
하지만, mirror 옵션을 끄면 White가 Observation의 1\textasciitilde{}6차원, State의 0차원을 사용한다.

출력부에는 평가값 출력노드(Value) 한 개와 정책 출력노드(Policy) 400개가 있다.
Microchess 보드에 기물을 놓을 수 있는 위치는 총 20개( \(5 \times 4\) )이기 때문에,
현재 기물의 위치와 기물의 다음 위치의 모든 경우의 수는 총 400개가 된다.
하지만, 그 중에 대부분은 현재 기물이 없거나 기물이 움직일 수 없는 곳이기 때문에 현재 게임 상태에서는 유효하지 않다.
유효하지 않은 수와 관련된 출력을 mask를 사용해 제거한다. 유효하지 않은 수를 -100으로 덮어쓰면,
마지막 softmax 출력에서 0에 가까운 값이 된다.
학습의 결과 인공신경망의 Policy 출력은 현재 게임 상태에서 다음에 둘 수의 확률(\(p\); probability)이 되어야 하며,
Value 출력은 게임 상태가 얼마나 승리상태에 가까운지 {[}-1, 1{]}사이 실수(\(v\); value)로 평가할 수 있어야 한다.

학습 데이터 샘플에는 게임상태(\(s\) ; state), MCTS의 탐색 빈도(\(\pi\) , 전체 합이 1.이 되도록 변환),
유효한 수 목록(legal moves), 그리고 게임의 승패(\(r\) ; reward)가 있어야 한다.
여기서 \(\pi\) 는 다음에 둘 수의 확률 \(p\) 로, 게임의 승패 \(r\) 은 평가값 \(v\) 로 볼 수 있다.
그래서, loss 함수는 \eqref{equation:05-self_learning:loss_function} 같이 정의된다.
첫 번째 항은 Policy 출력의 loss 이고, 두 번째 항은 Value 출력의 loss 이다. 마지막 항은 과적합을 막기위한 \(l2\) 값이다.
\begin{equation}\label{equation:05-self_learning:loss_function}
\begin{split}L =  - \pi log p + c_1 (r - v)^2  + c_2 ||\theta||^2\end{split}
\end{equation}
\def\sphinxLiteralBlockLabel{\label{\detokenize{05-self_learning:training-code}}\label{\detokenize{05-self_learning:id11}}}
\sphinxSetupCaptionForVerbatim{신경망 학습}
\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\},numbers=left,firstnumber=1,stepnumber=1]
\PYG{c+c1}{\PYGZsh{} optimizer 초기화}
\PYG{n}{c1} \PYG{o}{=} \PYG{l+m+mf}{0.02}
\PYG{n}{c2} \PYG{o}{=} \PYG{l+m+mf}{0.0001}
\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n}{current\PYGZus{}model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{momentum}\PYG{o}{=}\PYG{l+m+mf}{0.9}\PYG{p}{,} \PYG{n}{weight\PYGZus{}decay}\PYG{o}{=}\PYG{n}{c2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} 중간 생략 ...}

\PYG{k}{while} \PYG{n+nb+bp}{True}\PYG{p}{:}

   \PYG{c+c1}{\PYGZsh{} 중간 생략 ...}

   \PYG{c+c1}{\PYGZsh{} 입력 상태: ob, s}
   \PYG{n}{prob}\PYG{p}{,} \PYG{n}{value} \PYG{o}{=} \PYG{n}{current\PYGZus{}model}\PYG{p}{(}\PYG{n}{ob}\PYG{p}{,} \PYG{n}{s}\PYG{p}{)}

   \PYG{c+c1}{\PYGZsh{} 실제 가능한 수에 마스킹을 함}
   \PYG{n}{prob} \PYG{o}{=} \PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{legal\PYGZus{}moves}\PYG{p}{)} \PYG{o}{*} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{)} \PYG{o}{+} \PYG{p}{(}\PYG{n}{legal\PYGZus{}moves} \PYG{o}{*} \PYG{n}{prob}\PYG{p}{)}

   \PYG{c+c1}{\PYGZsh{} 신경망 가중치 갱신}
   \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}
   \PYG{n}{log\PYGZus{}prob} \PYG{o}{=} \PYG{n}{F}\PYG{o}{.}\PYG{n}{log\PYGZus{}softmax}\PYG{p}{(}\PYG{n}{prob}\PYG{p}{,} \PYG{n}{dim}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
   \PYG{n}{policy\PYGZus{}loss} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{pi} \PYG{o}{*} \PYG{n}{log\PYGZus{}prob}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
   \PYG{n}{value\PYGZus{}loss} \PYG{o}{=} \PYG{p}{(}\PYG{n}{z} \PYG{o}{\PYGZhy{}} \PYG{n}{value\PYGZus{}pred}\PYG{p}{)}\PYG{o}{.}\PYG{n}{pow}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
   \PYG{n}{loss} \PYG{o}{=} \PYG{n}{policy\PYGZus{}loss} \PYG{o}{+} \PYG{n}{value\PYGZus{}loss\PYGZus{}coef} \PYG{o}{*} \PYG{n}{value\PYGZus{}loss}
   \PYG{n}{loss}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

이것을 실제 코드로 작성하면 {\hyperref[\detokenize{05-self_learning:training-code}]{\sphinxcrossref{\DUrole{std,std-ref}{신경망 학습}}}} 처럼된다(예제 AI와 다를 수 있음).
많은 인공신경망 라이브러리는 \(l2\) 항을 따로 사용하지 않고, optimizer에 인자를 추가하는 식으로 처리한다(4번째 줄 weight\_decay).
Value loss 값은 {[}-1, 1{]} 사이 실수이고, 실제 더 커질 가능성도 충분히 있지만, Policy loss값은 {[}0, 1{]}범위인데다가 대부분이 0이므로,
평균적으로 Value loss값이 Policy loss값에 비해 약 50배 정도 크다. 이것을 보정하기위해 \(c_2\) 값을 0.02를 사용했다.


\subsection{MCTS}
\label{\detokenize{05-self_learning:mcts}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{self_learning_fig}.png}
\caption{Self Learning 구조}\label{\detokenize{05-self_learning:self-learning-fig}}\label{\detokenize{05-self_learning:id12}}\end{figure}

{\hyperref[\detokenize{05-self_learning:self-learning-fig}]{\sphinxcrossref{\DUrole{std,std-ref}{Self Learning 구조}}}} 를 다시 살펴보면 MCTS와 인공신경망이 결합되어 있다.
게임을 플레이할 때, MCTS는 현재 게임상태에서 어떤 수를 탐색(search)해야할 지 결정할 때,
선택과 Default Policy에서 인공신경망의 예측을 사용한다.
기존 MCTS에서는 선택 단계에서 UCB 알고리즘을 사용했는데, Self Learning에서는 그 변종인 PUCT 알고리즘을 사용한다.
기본적인 사항은 기존 UCB 알고리즘과 같지만, 두 번째 항(탐색)에서 신경망의 Policy 출력(\(P(s, a)\) )를 사용한다.
신경망의 출력은 과거 MCTS에서 탐색했던 결과를 반영하고 있으므로,
과거에 유망했던 것으로 판단했던 수는 현재에도 더 탐색을 하게된다.
\begin{equation}\label{equation:05-self_learning:PUCT}
\begin{split}a_t = argmax(Q(s_t, a) + c_{puct} P(s_t, a) \frac{\sqrt{N}}{1 + N_a})\end{split}
\end{equation}
Self Learning에서는 Default policy는 무작위 샘플링으로 상태의 좋고 나쁨을 판단하기 위한 것이었기 때문에,
이것 대신하여 인공 신경망의 Value 출력을 사용한다.
Default policy는 시뮬레이션 횟수와 탐색 깊이에 비례하여 시간복잡도가 증가할 뿐만 아니라,
실제로는 상태노드를 생성하기위해 메모리 할당과 해제가 빈번하게 발생하므로, 많은 경우 꽤 큰 계산비용을 요구한다.
반면, 인공신경망은 계산비용이 크기는 하지만, 병렬처리가 더 쉽고 고정되어 있기 때문에 훨씬 빠르다.

MCTS를 사용할 때 시뮬레이션 횟수를 주의해서 정해야 한다.
MCTS 시뮬레이션 횟수가 크면 학습할 때도 더 높은 성능을 보일 가능성이 있지만, 학습시간이 오래 걸리기 때문에 제한된
시간동안 성능을 충분히 높이기 어렵다.
반면 시뮬레이션 횟수가 너무 부족하면 MCTS가 충분히 일관성있는 탐색이 어렵기 때문에 실제 성능이 향상되기 어렵다.
예제 AI에서는 학습할 때 시뮬레이션 횟수를 500번으로 하였다.


\subsection{결론 및 한계}
\label{\detokenize{05-self_learning:id5}}
Self Learning으로 학습한 AI는 4일 정도 학습으로두 Stockfish 보다 좋은 성능을 보일 수 있었다.
단순히 더 오랜시간 학습을 하면 성능을 높일 수 있을 가능성은 있지만, 쉽지는 않다.
최종 학습결과가 초기 조건(신경망 초기값, Replay Memory 초기 데이터, 등)에 민감하기 때문에 일정수준 이상의 성능향상을 위해서는 더욱 개선이 필요하다.
현재 예제 AI는 Stockfish Level 5\textasciitilde{}6 정도까지는 하루 이틀 사이에 안정적으로 학습 가능하고, 8\textasciitilde{}10정도는 학습을 더 진행하면(약 4일) 학습이 가능하지만,
그 이상의 성능은 안정적으로 학습하지 못한다.


\chapter{API}
\label{\detokenize{index:api}}

\section{scripts package}
\label{\detokenize{scripts::doc}}\label{\detokenize{scripts:scripts-package}}

\subsection{Submodules}
\label{\detokenize{scripts:submodules}}

\subsection{scripts.chess\_board module}
\label{\detokenize{scripts:module-scripts.chess_board}}\label{\detokenize{scripts:scripts-chess-board-module}}\index{scripts.chess\_board (모듈)}
인간 플레이어 인터페이스와 관련된 클래스 모음
\index{ChessBoard (scripts.chess\_board 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.chess_board.ChessBoard}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{scripts.chess\_board.}}\sphinxbfcode{\sphinxupquote{ChessBoard}}}
Bases: \sphinxcode{\sphinxupquote{object}}

인간 플레이어 인터페이스 시각화 클래스
\begin{quote}\begin{description}
\item[{속성 block\_size}] \leavevmode
int, 체스 한 칸의 픽셀 크기

\item[{속성 width}] \leavevmode
int, 마이크로 체스 가로 칸 수

\item[{속성 height}] \leavevmode
int, 마이크로 체스 세로 칸 수

\end{description}\end{quote}
\index{block\_size (scripts.chess\_board.ChessBoard attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.chess_board.ChessBoard.block_size}}\pysigline{\sphinxbfcode{\sphinxupquote{block\_size}}\sphinxbfcode{\sphinxupquote{ = 62}}}
\end{fulllineitems}

\index{height (scripts.chess\_board.ChessBoard attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.chess_board.ChessBoard.height}}\pysigline{\sphinxbfcode{\sphinxupquote{height}}\sphinxbfcode{\sphinxupquote{ = 310}}}
\end{fulllineitems}

\index{step() (scripts.chess\_board.ChessBoard method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.chess_board.ChessBoard.step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{step}}}{\emph{board}, \emph{move=None}}{}
board 객체와 move 객체를 입력받아,
pygame 모듈을 사용해서 상태를 시각화 함
\begin{itemize}
\item {} 
AI 플레이어가 Move 를 입력하면, 단순히 현재 상태를 시각화 시킴고 그대로 Move를 반환

\item {} 
인간플레이어는 Move 객체대신 None을 입력하고, 이 함수에서 마우스 입력을 받아, 
게임 상태를 변경할 Move 객체를 생성해서 반환함

\item {} 
폰을 마지막 칸으로 보내면 무조건 퀸으로 승급하도록 했음 (AI는 어떤 기물로든 승급가능)

\end{itemize}
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{board}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{move}} (\sphinxstyleliteralemphasis{\sphinxupquote{chess.Move}}) -- 

\end{itemize}

\item[{반환}] \leavevmode
(chess.Move) --

\end{description}\end{quote}

\end{fulllineitems}

\index{width (scripts.chess\_board.ChessBoard attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.chess_board.ChessBoard.width}}\pysigline{\sphinxbfcode{\sphinxupquote{width}}\sphinxbfcode{\sphinxupquote{ = 248}}}
\end{fulllineitems}


\end{fulllineitems}

\index{micro\_to\_std (scripts.chess\_board 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.chess_board.micro_to_std}}\pysigline{\sphinxcode{\sphinxupquote{scripts.chess\_board.}}\sphinxbfcode{\sphinxupquote{micro\_to\_std}}\sphinxbfcode{\sphinxupquote{ = \{0: 0, 1: 1, 2: 2, 3: 3, 4: 8, 5: 9, 6: 10, 7: 11, 8: 16, 9: 17, 10: 18, 11: 19, 12: 24, 13: 25, 14: 26, 15: 27, 16: 32, 17: 33, 18: 34, 19: 35\}}}}
체스 기물 이미지 파일 경로

\end{fulllineitems}

\index{std\_to\_micro (scripts.chess\_board 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.chess_board.std_to_micro}}\pysigline{\sphinxcode{\sphinxupquote{scripts.chess\_board.}}\sphinxbfcode{\sphinxupquote{std\_to\_micro}}\sphinxbfcode{\sphinxupquote{ = \{0: 0, 1: 1, 2: 2, 3: 3, 8: 4, 9: 5, 10: 6, 11: 7, 16: 8, 17: 9, 18: 10, 19: 11, 24: 12, 25: 13, 26: 14, 27: 15, 32: 16, 33: 17, 34: 18, 35: 19\}}}}
마이크로 체스 좌표와 일반 체스 좌표를 매핑

\end{fulllineitems}



\subsection{scripts.run\_game module}
\label{\detokenize{scripts:scripts-run-game-module}}\label{\detokenize{scripts:module-scripts.run_game}}\index{scripts.run\_game (모듈)}
scripts/run\_game.py

Microchess AI 플랫폼에서 사용하는 체스와 관련된 기본 구성 요소 모음
\index{Environment (scripts.run\_game 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.Environment}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{scripts.run\_game.}}\sphinxbfcode{\sphinxupquote{Environment}}}
Bases: \sphinxcode{\sphinxupquote{object}}

마이크로 체스 게임 환경의 기본 객체
\begin{itemize}
\item {} 
게임의 초기화, 진행, 종료에 관한 기능은 담당

\item {} 
문자를 이용한 기초적인 시각화 가능

\item {} 
그 이상의 시각화 기능은 chess\_board.py (pygame)와 visdom에서 담당

\end{itemize}
\index{close() (scripts.run\_game.Environment method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.Environment.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
\end{fulllineitems}

\index{render() (scripts.run\_game.Environment method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.Environment.render}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{render}}}{}{}
시각화
\begin{itemize}
\item {} 
언제나 jupyter console에 출력 가능한 svg객체를 출력함 (jupyter console에서 시각화 됨)

\item {} 
문자열을 이용해 체스 상태를 출력함 (대문자: white, 소문자: black)

\item {} 
인간 플레이어가 한명이라도 게임을 할 때는 pygame을 이용함

\end{itemize}

\end{fulllineitems}

\index{reset() (scripts.run\_game.Environment method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.Environment.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{fen=None}, \emph{max\_turns=80}}{}
환경 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{fen}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) -- fen 표기법으로 체스 보드 상태를 초기화 함, None 으로 전달할 경우 기본 초기화 상태로 세팅함

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{max\_turns}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 최대 게임 턴, 기봅값 80

\end{itemize}

\item[{반환}] \leavevmode
(State) -- 현재 게임 상태 반환

\end{description}\end{quote}

\end{fulllineitems}

\index{step() (scripts.run\_game.Environment method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.Environment.step}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{step}}}{\emph{move}}{}
현재 상태에 move를 적용하여 다음 턴으로 게임을 진행
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{move}} -- chess.Move, 현재 플레이어의 다음 수

\item[{반환}] \leavevmode

({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{State}}}}}, float, bool, dict) --
\begin{itemize}
\item {} 
State, move가 적용된 다음 게임 상태

\item {} 
float, 보상, 이번 수로 게임이 승리하면 1., 패배하면 0., 그 외에는 0.5

\item {} 
bool, 이번 수로 게임이 종료되었는지 여부

\item {} 
dict, 그 외 기타 정보 전달 용

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Evaluator (scripts.run\_game 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.Evaluator}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{scripts.run\_game.}}\sphinxbfcode{\sphinxupquote{Evaluator}}}
Bases: \sphinxcode{\sphinxupquote{object}}

\sphinxstylestrong{기본 예제에서 사용하는 평가함수 모음}

현재 상태(board.Chess)와 현재 턴(bool, white=True, black=False)를 입력으로 받아,
평가 값을 {[}0, 1{]} 실수로 반환함
\index{eval() (scripts.run\_game.Evaluator static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.Evaluator.eval}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{eval}}}{\emph{turn}}{}
기물 점수를 계산하는 평가함수
\begin{itemize}
\item {} 
현재 판위에 남아있는 기물의 종류와 개수에 따라 {[}0, 1{]} 실수값으로 평가함

\item {} 
게임의 승패가 결정되어 있다면, Evaluator.win\_or\_lose로 평가함

\item {} 
게임이 종료되지 않았다면 기물의 종류와 개수에 따라 점수를 계산한 뒤 {[}0, 1{]}값으로 정규화 시킴

\item {} 
절대적인, 상대적인 기물의 위치, 캐슬링 여부등 다른 요소는 고려하지 않음

\end{itemize}

기물의 점수
\begin{itemize}
\item {} 
Pawn: 1점

\item {} 
Knight, Bissop: 3점

\item {} 
Rook: 5점

\item {} 
Queen: 10점

\item {} 
King: 4점

\end{itemize}
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{board}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 평가할 상태

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{turn}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- chess.WHITE 또는 chess.BLACK

\end{itemize}

\item[{반환}] \leavevmode
(float) -- {[}0, 1{]} 범위 실수

\end{description}\end{quote}

\end{fulllineitems}

\index{eval\_2() (scripts.run\_game.Evaluator static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.Evaluator.eval_2}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{eval\_2}}}{\emph{turn}}{}
Evaluator.eval 결과({[}0, 1{]} 실수)를 {[}-1, 1{]} 실수로 변환
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{board}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 평가할 상태

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{turn}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- chess.WHITE 또는 chess.BLACK

\end{itemize}

\item[{반환}] \leavevmode
(float) -- {[}-1, 1{]} 실수

\end{description}\end{quote}

\end{fulllineitems}

\index{win\_or\_lose() (scripts.run\_game.Evaluator static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.Evaluator.win_or_lose}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{win\_or\_lose}}}{\emph{turn}}{}
승/패만 판단하는 간단한 평가함수
\begin{itemize}
\item {} 
간단한 평가함수, {[}0., 0.5, 1.{]} 중 하나의 값을 반환

\item {} 
승리하면 1., 패배하면 0.을 반환함

\item {} 
나머지 경우에는 0.5 반환 (무승부, 게임이 진행 중)

\end{itemize}
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{board}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 평가할 게임 상태

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{turn}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- chess.WHITE 또는 chess.BLACK

\end{itemize}

\item[{반환}] \leavevmode
(float) -- {[}0, 1{]} 범위 실수

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Move (scripts.run\_game 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.Move}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{scripts.run\_game.}}\sphinxbfcode{\sphinxupquote{Move}}}{\emph{from\_square}, \emph{to\_square}, \emph{promotion=None}, \emph{drop=None}}{}
Bases: \sphinxcode{\sphinxupquote{chess.Move}}

\end{fulllineitems}

\index{State (scripts.run\_game 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.State}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{scripts.run\_game.}}\sphinxbfcode{\sphinxupquote{State}}}{\emph{fen='8/8/8/knbr4/p7/8/3P4/RBNK4 w Kk - 0 1'}, \emph{chess960=False}}{}
Bases: \sphinxcode{\sphinxupquote{chess.Board}}

마이크로 체스 AI 플랫폼에서 게임 상태를 전달하기 위해 사용
\begin{itemize}
\item {} 
chess.Board + forward 기능

\item {} 
chess.Board 에서 제공하는 대부분의 기능은 그대로 사용가능

\item {} 
향후 부정행위에 사용할 수 있는 기능이 발견되는 경우 일부 기능이 제한될 수 있음

\end{itemize}
\index{black\_remain\_sec (scripts.run\_game.State attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.State.black_remain_sec}}\pysigline{\sphinxbfcode{\sphinxupquote{black\_remain\_sec}}\sphinxbfcode{\sphinxupquote{ = 400}}}
(int) -- black 플레이어의 남은 시간

\end{fulllineitems}

\index{color (scripts.run\_game.State attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.State.color}}\pysigline{\sphinxbfcode{\sphinxupquote{color}}}
\end{fulllineitems}

\index{forward() (scripts.run\_game.State method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.State.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{move}}{}
시뮬레이션
- 현재 State에 move를 적용한 다음 상태를 반환
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{move}} ({\hyperref[\detokenize{scripts:scripts.run_game.Move}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{Move}}}}}) -- 

\item[{반환}] \leavevmode
({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{State}}}}}) -- move가 적용된 다음 상태

\end{description}\end{quote}

\end{fulllineitems}

\index{white\_remain\_sec (scripts.run\_game.State attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.State.white_remain_sec}}\pysigline{\sphinxbfcode{\sphinxupquote{white\_remain\_sec}}\sphinxbfcode{\sphinxupquote{ = 400}}}
(int) -- white 플레이어의 남은 시간

\end{fulllineitems}


\end{fulllineitems}

\index{TIME\_LIMIT (scripts.run\_game 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.TIME_LIMIT}}\pysigline{\sphinxcode{\sphinxupquote{scripts.run\_game.}}\sphinxbfcode{\sphinxupquote{TIME\_LIMIT}}\sphinxbfcode{\sphinxupquote{ = 400}}}
한 플레이어에게 주어진 시간제한,
한턴에 10초씩 80턴으로 계산해서 한 플레이어에 400초 할당

\end{fulllineitems}

\index{game() (scripts.run\_game 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.run_game.game}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{scripts.run\_game.}}\sphinxbfcode{\sphinxupquote{game}}}{\emph{white}, \emph{black}, \emph{human\_play}, \emph{initial\_fen}, \emph{max\_turns}, \emph{timelimit}}{}
게임을 1회 실행하는 함수
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{white}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) -- white 플레이어의 객체(BaseAgent 상속한 클래스) 경로

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{black}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) -- black 플레이어의 객체(BaseAgent 상속한 클래스) 경로

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{human\_play}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- 인간 플레이어용 인터페이스를 사용여부

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{initial\_fen}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) -- 게임의 초기상태를 fen으로 전달

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{max\_turns}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 최대 게임 턴, 경진대회 기본은 80

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{timelimit}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 최대 게임 시간, 경진대회 기본은 400

\end{itemize}

\item[{반환}] \leavevmode
({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{State}}}}}, int, float) -- state, turn, reward

\end{description}\end{quote}

\end{fulllineitems}



\subsection{scripts.utils module}
\label{\detokenize{scripts:module-scripts.utils}}\label{\detokenize{scripts:scripts-utils-module}}\index{scripts.utils (모듈)}
마이크로 체스와 관계없는 기타 코드 모음
\index{ascii\_plot() (scripts.utils 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{scripts:scripts.utils.ascii_plot}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{scripts.utils.}}\sphinxbfcode{\sphinxupquote{ascii\_plot}}}{\emph{xs}, \emph{ys}, \emph{title=None}, \emph{print\_out=False}}{}
gnuplot을 이용해서 ascii 문자로 꺽은선 그래프를 그려줌
GNU plot을 별도로 설치해야 함
- Windows (C:/Program Files/gnuplot/bin/gnuplot.exe)
- Linux (/usr/bin/gnuplot)
설치가 되어있지 않으면 경고 메시지만 출력함

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{scripts}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k}{import} \PYG{n}{ascii\PYGZus{}plot}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{fig} \PYG{o}{=} \PYG{n}{ascii\PYGZus{}plot}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.4}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{fig}\PYG{p}{)}
\end{sphinxVerbatim}

\fvset{hllines={, ,}}%
\begin{sphinxVerbatim}[commandchars=\\\{\}]
 0.4 +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
     \textbar{}                   +                   +                  +               *** \textbar{}
     \textbar{}                                                                        **    \textbar{}
     \textbar{}                                                                     ***      \textbar{}
0.35 \textbar{}\PYGZhy{}+                                                                ***       +\PYGZhy{}\textbar{}
     \textbar{}                                                               ***            \textbar{}
     \textbar{}                                                            ***               \textbar{}
 0.3 \textbar{}\PYGZhy{}+                                                        **                +\PYGZhy{}\textbar{}
     \textbar{}                                                       ***                    \textbar{}
     \textbar{}                                                    ***                       \textbar{}
     \textbar{}                                                 ***                          \textbar{}
0.25 \textbar{}\PYGZhy{}+                                            ***                           +\PYGZhy{}\textbar{}
     \textbar{}                                            **                                \textbar{}
     \textbar{}                                         ***                                  \textbar{}
 0.2 \textbar{}\PYGZhy{}+                                   **A*                                   +\PYGZhy{}\textbar{}
     \textbar{}                               ******                                         \textbar{}
     \textbar{}                         ******                                               \textbar{}
0.15 \textbar{}\PYGZhy{}+                 ******                                                   +\PYGZhy{}\textbar{}
     \textbar{}              *****                                                           \textbar{}
     \textbar{}        ******                                                                \textbar{}
     \textbar{}  ******           +                   +                  +                   \textbar{}
 0.1 +\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}+
     0                  0.5                  1                 1.5                  2
\end{sphinxVerbatim}
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{xs}} -- list of \{int, float\}, 숫자 리스트

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{ys}} -- list of \{int, float\}, 숫자 리스트

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{title}} -- str, 그래프 위에 표시할 문자열

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{print\_out}} -- bool, True일 때는 반환하는 것과 별개로 그래프를 print 함

\end{itemize}

\item[{반환}] \leavevmode
str, ascii 문자로 만든 그래프

\end{description}\end{quote}

\end{fulllineitems}



\subsection{Module contents}
\label{\detokenize{scripts:module-contents}}\label{\detokenize{scripts:module-scripts}}\index{scripts (모듈)}

\section{agents package}
\label{\detokenize{agents:agents-package}}\label{\detokenize{agents::doc}}

\subsection{Module contents}
\label{\detokenize{agents:module-contents}}\label{\detokenize{agents:module-agents}}\index{agents (모듈)}\index{BaseAgent (agents 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents:agents.BaseAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.}}\sphinxbfcode{\sphinxupquote{BaseAgent}}}{\emph{name}, \emph{color}}{}
Bases: \sphinxcode{\sphinxupquote{object}}

AI의 기반 클래스
AI를 구현할 때는 이 클래스를 상속받아 reset, act, close를 구현해야 함
\index{act() (agents.BaseAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents:agents.BaseAgent.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.BaseAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents:agents.BaseAgent.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{opponent\_color (agents.BaseAgent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents:agents.BaseAgent.opponent_color}}\pysigline{\sphinxbfcode{\sphinxupquote{opponent\_color}}}
상대의 턴

\end{fulllineitems}

\index{reset() (agents.BaseAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents:agents.BaseAgent.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}


\end{fulllineitems}



\section{agents.basic package}
\label{\detokenize{agents.basic:agents-basic-package}}\label{\detokenize{agents.basic::doc}}

\subsection{Submodules}
\label{\detokenize{agents.basic:submodules}}

\subsection{agents.basic.debug\_agent module}
\label{\detokenize{agents.basic:agents-basic-debug-agent-module}}\label{\detokenize{agents.basic:module-agents.basic.debug_agent}}\index{agents.basic.debug\_agent (모듈)}
플랫폼 디버깅 및 테스트 용으로 만들어진 AI 모음
\index{FirstMoveAgent (agents.basic.debug\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.debug_agent.FirstMoveAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.basic.debug\_agent.}}\sphinxbfcode{\sphinxupquote{FirstMoveAgent}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

First Move Agent

가능한 수 중에서 가장 첫번째 수를 선택함
\begin{itemize}
\item {} 
플랫폼 테스트용

\end{itemize}
\index{act() (agents.basic.debug\_agent.FirstMoveAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.debug_agent.FirstMoveAgent.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.basic.debug\_agent.FirstMoveAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.debug_agent.FirstMoveAgent.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{reset() (agents.basic.debug\_agent.FirstMoveAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.debug_agent.FirstMoveAgent.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}


\end{fulllineitems}

\index{MalfunctionAgent (agents.basic.debug\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.debug_agent.MalfunctionAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.basic.debug\_agent.}}\sphinxbfcode{\sphinxupquote{MalfunctionAgent}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

Malfunction Agent

한 수를 둘 때마다 50\%의 확률로 예외를 발생함
\begin{itemize}
\item {} 
플랫폼 테스트용

\item {} 
예외를 발생시키면 반드시 패배해야함

\end{itemize}
\index{act() (agents.basic.debug\_agent.MalfunctionAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.debug_agent.MalfunctionAgent.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.basic.debug\_agent.MalfunctionAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.debug_agent.MalfunctionAgent.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{reset() (agents.basic.debug\_agent.MalfunctionAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.debug_agent.MalfunctionAgent.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}


\end{fulllineitems}



\subsection{agents.basic.human module}
\label{\detokenize{agents.basic:agents-basic-human-module}}\label{\detokenize{agents.basic:module-agents.basic.human}}\index{agents.basic.human (모듈)}
인간 플레이어 인터페이스를 작동시키기 위한 더미 AI
\index{Player (agents.basic.human 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.human.Player}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.basic.human.}}\sphinxbfcode{\sphinxupquote{Player}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

인간 플레이어를 위한 더미 AI
\index{act() (agents.basic.human.Player method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.human.Player.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.basic.human.Player method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.human.Player.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{reset() (agents.basic.human.Player method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.human.Player.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}


\end{fulllineitems}



\subsection{agents.basic.random\_agent module}
\label{\detokenize{agents.basic:agents-basic-random-agent-module}}\label{\detokenize{agents.basic:module-agents.basic.random_agent}}\index{agents.basic.random\_agent (모듈)}
Random AI 구현
\index{RandomAgent (agents.basic.random\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.random_agent.RandomAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.basic.random\_agent.}}\sphinxbfcode{\sphinxupquote{RandomAgent}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

Random AI
\begin{itemize}
\item {} 
무작위로 행동을 결정하는 예제

\end{itemize}
\index{act() (agents.basic.random\_agent.RandomAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.random_agent.RandomAgent.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.basic.random\_agent.RandomAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.random_agent.RandomAgent.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{reset() (agents.basic.random\_agent.RandomAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.basic:agents.basic.random_agent.RandomAgent.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}


\end{fulllineitems}



\subsection{Module contents}
\label{\detokenize{agents.basic:module-agents.basic}}\label{\detokenize{agents.basic:module-contents}}\index{agents.basic (모듈)}

\section{agents.search package}
\label{\detokenize{agents.search:agents-search-package}}\label{\detokenize{agents.search::doc}}

\subsection{Submodules}
\label{\detokenize{agents.search:submodules}}

\subsection{Module contents}
\label{\detokenize{agents.search:module-contents}}\label{\detokenize{agents.search:module-agents.search}}\index{agents.search (모듈)}

\subsection{agents.search.one\_step\_search\_agent module}
\label{\detokenize{agents.search:module-agents.search.one_step_search_agent}}\label{\detokenize{agents.search:agents-search-one-step-search-agent-module}}\index{agents.search.one\_step\_search\_agent (모듈)}
One Step Search AI 구현
\index{OneStepSearchAgent (agents.search.one\_step\_search\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.one_step_search_agent.OneStepSearchAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.search.one\_step\_search\_agent.}}\sphinxbfcode{\sphinxupquote{OneStepSearchAgent}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

One Step Search Agent
\begin{itemize}
\item {} 
현재 가능한 모든 수를 시뮬레이션 하여 다음 결과를 예상하고, 가장 좋은 수를 결정하는 AI

\item {} 
바로 다음 단계만을 고려하는 가장 단순한 탐색 AI

\item {} 
예제에 포함된 가장 약한 탐색 AI

\end{itemize}
\index{act() (agents.search.one\_step\_search\_agent.OneStepSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.one_step_search_agent.OneStepSearchAgent.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.search.one\_step\_search\_agent.OneStepSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.one_step_search_agent.OneStepSearchAgent.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{reset() (agents.search.one\_step\_search\_agent.OneStepSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.one_step_search_agent.OneStepSearchAgent.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}


\end{fulllineitems}



\subsection{agents.search.two\_step\_search\_agent module}
\label{\detokenize{agents.search:module-agents.search.two_step_search_agent}}\label{\detokenize{agents.search:agents-search-two-step-search-agent-module}}\index{agents.search.two\_step\_search\_agent (모듈)}
Two Step Search AI 구현
\index{TwoStepSearchAgent (agents.search.two\_step\_search\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.two_step_search_agent.TwoStepSearchAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.search.two\_step\_search\_agent.}}\sphinxbfcode{\sphinxupquote{TwoStepSearchAgent}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

Two Step Search Agent
\begin{itemize}
\item {} 
나의 다음 수와 상대방의 그 다음 수, 두 수를 시뮬레이션 하고 의사결정을 하는 AI

\item {} 
게임 승패를 판단하는 것과 동일 한 평가함수 사용

\item {} 
플랫폼의 다른 부분에서는 greedy AI로 부르기도 함

\item {} 
예제에 포함된 두 번째로 약한 탐색기반 AI

\end{itemize}
\index{act() (agents.search.two\_step\_search\_agent.TwoStepSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.two_step_search_agent.TwoStepSearchAgent.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.search.two\_step\_search\_agent.TwoStepSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.two_step_search_agent.TwoStepSearchAgent.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{opponent\_act() (agents.search.two\_step\_search\_agent.TwoStepSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.two_step_search_agent.TwoStepSearchAgent.opponent_act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{opponent\_act}}}{\emph{state}}{}
상대방의 행동 시뮬레이션
이 상태의 평가는 상대방의 그 다음 행동에 따라 달라짐
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 상태

\item[{반환}] \leavevmode
(float) -- 평가값 {[}0, 1{]} 범위

\end{description}\end{quote}

\end{fulllineitems}

\index{reset() (agents.search.two\_step\_search\_agent.TwoStepSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.two_step_search_agent.TwoStepSearchAgent.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}


\end{fulllineitems}



\subsection{agents.search.negamax\_search\_agent module}
\label{\detokenize{agents.search:module-agents.search.negamax_search_agent}}\label{\detokenize{agents.search:agents-search-negamax-search-agent-module}}\index{agents.search.negamax\_search\_agent (모듈)}
Negamax Search AI 구현
\index{NegamaxSearchAgent (agents.search.negamax\_search\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.negamax_search_agent.NegamaxSearchAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.search.negamax\_search\_agent.}}\sphinxbfcode{\sphinxupquote{NegamaxSearchAgent}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

Negamax Search AI
\begin{itemize}
\item {} 
Minimax AI의 일종인 Negamax AI 기본 구현

\item {} 
\sphinxurl{https://en.wikipedia.org/wiki/Negamax} 참조

\item {} 
Negamax가 Minimax에 비해 코드가 간결함

\item {} 
Two Search AI의 일반적인 형태, Two Search AI와 달리 임의의 깊이를 탐색 가능함

\item {} 
탐색 깊이가 깊어질 수록 탐색 공간이 10배씩 증가하기 때문에 최대 탐색 깊이를 적절히 선택해야함

\end{itemize}
\index{act() (agents.search.negamax\_search\_agent.NegamaxSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.negamax_search_agent.NegamaxSearchAgent.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.search.negamax\_search\_agent.NegamaxSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.negamax_search_agent.NegamaxSearchAgent.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{negamax() (agents.search.negamax\_search\_agent.NegamaxSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.negamax_search_agent.NegamaxSearchAgent.negamax}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{negamax}}}{\emph{state}, \emph{depth}, \emph{color}}{}
Negamax 탐색
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 상태

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{depth}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 남은 탐색 깊이, self.max\_depth - 현재 깊이

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{color}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) -- white = 1., black = -1

\end{itemize}

\item[{반환}] \leavevmode
(chess.Move, float) -- best\_move, 평가값

\end{description}\end{quote}
\begin{itemize}
\item {} 
chess.Move: 가장 좋은 행동

\item {} 
float: 가장 좋은 행동을 했을 때 얻을 수 있는 미래 보상

\end{itemize}

\end{fulllineitems}

\index{reset() (agents.search.negamax\_search\_agent.NegamaxSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.negamax_search_agent.NegamaxSearchAgent.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}


\end{fulllineitems}



\subsection{agents.search.abp\_negamax\_search\_agent module}
\label{\detokenize{agents.search:agents-search-abp-negamax-search-agent-module}}\label{\detokenize{agents.search:module-agents.search.abp_negamax_search_agent}}\index{agents.search.abp\_negamax\_search\_agent (모듈)}
Negamax Search  AI + alpha-beta pruning 구현
\index{ABPNegamaxSearchAgent (agents.search.abp\_negamax\_search\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.abp_negamax_search_agent.ABPNegamaxSearchAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.search.abp\_negamax\_search\_agent.}}\sphinxbfcode{\sphinxupquote{ABPNegamaxSearchAgent}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

Negamax Search AI + alpha-beta pruning
\begin{itemize}
\item {} 
기본 Negamax 알고리즘의 탐색 성능 향상을 위해 alpha-beta pruning 사용

\item {} 
alpha-beta pruning 은 불필요한 상태공간을 탐색하지 않기 때문에 같은 시간동안 더 깊은 탐색이 가능함

\end{itemize}
\index{act() (agents.search.abp\_negamax\_search\_agent.ABPNegamaxSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.abp_negamax_search_agent.ABPNegamaxSearchAgent.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.search.abp\_negamax\_search\_agent.ABPNegamaxSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.abp_negamax_search_agent.ABPNegamaxSearchAgent.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{negamax() (agents.search.abp\_negamax\_search\_agent.ABPNegamaxSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.abp_negamax_search_agent.ABPNegamaxSearchAgent.negamax}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{negamax}}}{\emph{state}, \emph{depth}, \emph{alpha}, \emph{beta}, \emph{color}}{}
\end{fulllineitems}

\index{reset() (agents.search.abp\_negamax\_search\_agent.ABPNegamaxSearchAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.abp_negamax_search_agent.ABPNegamaxSearchAgent.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}


\end{fulllineitems}



\subsection{agents.search.mcts\_agent module}
\label{\detokenize{agents.search:module-agents.search.mcts_agent}}\label{\detokenize{agents.search:agents-search-mcts-agent-module}}\index{agents.search.mcts\_agent (모듈)}
Monte-Carlo Tree Search 구현
\index{Arrow (agents.search.mcts\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Arrow}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.search.mcts\_agent.}}\sphinxbfcode{\sphinxupquote{Arrow}}}
Bases: {\hyperref[\detokenize{agents.search:agents.search.mcts_agent.Arrow}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.search.mcts\_agent.Arrow}}}}}

체스 보드 시각화에 사용

\end{fulllineitems}

\index{MCTSAgent (agents.search.mcts\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.search.mcts\_agent.}}\sphinxbfcode{\sphinxupquote{MCTSAgent}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

MCTS AI

평가용 MCTS AI,
시뮬레이션 횟수를 최대한으로 하고, 탐색시간을 제한함,
주어진 시간을 최대한 활용하여 최선의 성능을 보임,
\index{act() (agents.search.mcts\_agent.MCTSAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgent.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.search.mcts\_agent.MCTSAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgent.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{max\_depth (agents.search.mcts\_agent.MCTSAgent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgent.max_depth}}\pysigline{\sphinxbfcode{\sphinxupquote{max\_depth}}\sphinxbfcode{\sphinxupquote{ = 6}}}
\end{fulllineitems}

\index{n\_simulations (agents.search.mcts\_agent.MCTSAgent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgent.n_simulations}}\pysigline{\sphinxbfcode{\sphinxupquote{n\_simulations}}\sphinxbfcode{\sphinxupquote{ = 1000000}}}
\end{fulllineitems}

\index{planner (agents.search.mcts\_agent.MCTSAgent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgent.planner}}\pysigline{\sphinxbfcode{\sphinxupquote{planner}}\sphinxbfcode{\sphinxupquote{ = None}}}
\end{fulllineitems}

\index{reset() (agents.search.mcts\_agent.MCTSAgent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgent.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}

\index{search\_time (agents.search.mcts\_agent.MCTSAgent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgent.search_time}}\pysigline{\sphinxbfcode{\sphinxupquote{search\_time}}\sphinxbfcode{\sphinxupquote{ = 10}}}
\end{fulllineitems}


\end{fulllineitems}

\index{MCTSAgentDev (agents.search.mcts\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgentDev}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.search.mcts\_agent.}}\sphinxbfcode{\sphinxupquote{MCTSAgentDev}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

개발용 MCTS AI

시뮬레이션 횟수를 고정하고, 탐색 시간은 매우 크게함,
PC 사양에관계없이 균일한 성능을 보임
\index{act() (agents.search.mcts\_agent.MCTSAgentDev method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgentDev.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{close() (agents.search.mcts\_agent.MCTSAgentDev method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgentDev.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{max\_depth (agents.search.mcts\_agent.MCTSAgentDev attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgentDev.max_depth}}\pysigline{\sphinxbfcode{\sphinxupquote{max\_depth}}\sphinxbfcode{\sphinxupquote{ = 6}}}
\end{fulllineitems}

\index{n\_simulations (agents.search.mcts\_agent.MCTSAgentDev attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgentDev.n_simulations}}\pysigline{\sphinxbfcode{\sphinxupquote{n\_simulations}}\sphinxbfcode{\sphinxupquote{ = 5000}}}
\end{fulllineitems}

\index{planner (agents.search.mcts\_agent.MCTSAgentDev attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgentDev.planner}}\pysigline{\sphinxbfcode{\sphinxupquote{planner}}\sphinxbfcode{\sphinxupquote{ = None}}}
\end{fulllineitems}

\index{reset() (agents.search.mcts\_agent.MCTSAgentDev method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgentDev.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}

\index{search\_time (agents.search.mcts\_agent.MCTSAgentDev attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSAgentDev.search_time}}\pysigline{\sphinxbfcode{\sphinxupquote{search\_time}}\sphinxbfcode{\sphinxupquote{ = 10000}}}
\end{fulllineitems}


\end{fulllineitems}

\index{MCTSPlanner (agents.search.mcts\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSPlanner}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.search.mcts\_agent.}}\sphinxbfcode{\sphinxupquote{MCTSPlanner}}}{\emph{color}, \emph{max\_depth=9223372036854775807}, \emph{action\_score='visits'}, \emph{eval\_func=\textless{}function Evaluator.eval\_2\textgreater{}}, \emph{exploration\_coef=1.0}, \emph{reward\_amplify=True}, \emph{debug=False}}{}
Bases: \sphinxcode{\sphinxupquote{object}}

MCTS 알고리즘 구현
\index{backup() (agents.search.mcts\_agent.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSPlanner.backup}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{backup}}}{\emph{node}, \emph{delta}}{}
시뮬레이션 결과 업데이트
턴마다 Negamax 스타일로 보상의 부호를 바꿈, 
보상구간 {[}-1, 1{]}을 가정함
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{node}} (\sphinxstyleliteralemphasis{\sphinxupquote{agents.search.macts\_agent.Node}}) -- 탐색한 트리의 종단노드

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{delta}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) -- 시뮬레이션으로 알아낸 보상

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{best\_child() (agents.search.mcts\_agent.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSPlanner.best_child}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{best\_child}}}{\emph{node}}{}
현재 노드에서 가장 좋은 행동을 결정하여 반환함
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{node}} (\sphinxstyleliteralemphasis{\sphinxupquote{agents.search.macts\_agent.Node}}) -- 현재 상태 노드 v0

\item[{반환}] \leavevmode
(chess.Move) -- 가장 좋은 행동

\end{description}\end{quote}

\end{fulllineitems}

\index{default\_policy() (agents.search.mcts\_agent.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSPlanner.default_policy}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{default\_policy}}}{\emph{state}}{}
Monte Carlo 시뮬레이션
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 탐색한 트리의 종단노드에 저장되어 있는 state

\item[{반환}] \leavevmode
(float) -- 시뮬레이션 결과 얻은 최종 보상

\end{description}\end{quote}

\end{fulllineitems}

\index{search() (agents.search.mcts\_agent.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSPlanner.search}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{search}}}{\emph{state}, \emph{n\_simulations=1000}, \emph{timeout=10}}{}
주어진 상태와 제약조건(시뮬레이션 횟수와 시간제한)동안 시뮬레이션을 하고
찾아낸 가장 좋은 수을 반환함
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_simulations}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 최대 시뮬레이션 횟수

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{timeout}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 시간제한 (sec.)

\end{itemize}

\item[{반환}] \leavevmode
(chess.Move) -- 가장 좋은 수

\end{description}\end{quote}

\end{fulllineitems}

\index{tree\_policy() (agents.search.mcts\_agent.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.MCTSPlanner.tree_policy}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{tree\_policy}}}{\emph{node}}{}
선택 및 확장 단계
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{node}} (\sphinxstyleliteralemphasis{\sphinxupquote{agents.search.macts\_agent.Node}}) -- 현재 노드 v0

\item[{반환}] \leavevmode
(Node) -- 생성한 트리의 종단노드 vl

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Node (agents.search.mcts\_agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Node}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.search.mcts\_agent.}}\sphinxbfcode{\sphinxupquote{Node}}}{\emph{turn}, \emph{state}}{}
Bases: \sphinxcode{\sphinxupquote{object}}

상태공간 노드

Attrs:
\begin{itemize}
\item {} 
(bool) -- color, white=True, black=False

\item {} 
({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{scripts.run\_game.State}}}}}) -- state

\item {} 
(\sphinxcode{\sphinxupquote{agents.search.macts\_agent.Node}}) -- parent 이전 상태를 저장하고 있는 노드

\item {} 
(dict) -- children, dict(key-\textgreater{} chess.Move, value-\textgreater{} Node), 특정 수를 두고 난 뒤의 상태

\item {} 
(int) -- visits, 이 노드를 방문한 횟수

\item {} 
(float) -- wins, 이 노드와 자식 노드에서 받은 보상의 총합

\item {} 
(float) -- ucb 이 노드의 ucb 값

\end{itemize}
\index{children (agents.search.mcts\_agent.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Node.children}}\pysigline{\sphinxbfcode{\sphinxupquote{children}}}
\end{fulllineitems}

\index{key (agents.search.mcts\_agent.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Node.key}}\pysigline{\sphinxbfcode{\sphinxupquote{key}}}
\end{fulllineitems}

\index{next\_turn (agents.search.mcts\_agent.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Node.next_turn}}\pysigline{\sphinxbfcode{\sphinxupquote{next\_turn}}}
\end{fulllineitems}

\index{parent (agents.search.mcts\_agent.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Node.parent}}\pysigline{\sphinxbfcode{\sphinxupquote{parent}}}
\end{fulllineitems}

\index{state (agents.search.mcts\_agent.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Node.state}}\pysigline{\sphinxbfcode{\sphinxupquote{state}}}
\end{fulllineitems}

\index{turn (agents.search.mcts\_agent.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Node.turn}}\pysigline{\sphinxbfcode{\sphinxupquote{turn}}}
\end{fulllineitems}

\index{ucb (agents.search.mcts\_agent.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Node.ucb}}\pysigline{\sphinxbfcode{\sphinxupquote{ucb}}}
\end{fulllineitems}

\index{visits (agents.search.mcts\_agent.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Node.visits}}\pysigline{\sphinxbfcode{\sphinxupquote{visits}}}
\end{fulllineitems}

\index{wins (agents.search.mcts\_agent.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.search:agents.search.mcts_agent.Node.wins}}\pysigline{\sphinxbfcode{\sphinxupquote{wins}}}
\end{fulllineitems}


\end{fulllineitems}



\section{agents.self\_learning package}
\label{\detokenize{agents.self_learning::doc}}\label{\detokenize{agents.self_learning:agents-self-learning-package}}

\subsection{Submodules}
\label{\detokenize{agents.self_learning:submodules}}

\subsection{Module contents}
\label{\detokenize{agents.self_learning:module-agents.self_learning}}\label{\detokenize{agents.self_learning:module-contents}}\index{agents.self\_learning (모듈)}\begin{description}
\item[{OBSERVATION\_SHAPE:}] \leavevmode\begin{description}
\item[{12 channels: }] \leavevmode
말 종류마다 하나의 채널을 사용 함(P, R, N, B, Q, K, p, r, n, b, q, k)
백은 대문자, 흑은 소문자로 표시함

\item[{5 rows:}] \leavevmode
세로

\item[{4 columns:}] \leavevmode
가로

\end{description}

특정 위치에 기물이 위치하면 1., 기물이 없으면 0.으로 표시

\item[{LEN\_STATE}] \leavevmode
지금이 백의 차례인가?, 흑의 차례인가? , 
현재 플레이어에게 castling 권리가 있는가?, 다음 플레이어에게 castling 권리가 있는가?
정보를 1., 과 0.로 표시

\item[{N\_ACTIONS}] \leavevmode
가능한 모든 행동의 개수, 실제 기물의 존재 여부와 관계없이 
가능한 모든 출발 지점 20개와 목표 지점 20개를 곱한 400개를 가능한 모든 행동으로 함

\end{description}


\subsection{agents.self\_learning.agent module}
\label{\detokenize{agents.self_learning:agents-self-learning-agent-module}}\label{\detokenize{agents.self_learning:module-agents.self_learning.agent}}\index{agents.self\_learning.agent (모듈)}
Self Learning AI
\index{Agent (agents.self\_learning.agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.self\_learning.agent.}}\sphinxbfcode{\sphinxupquote{Agent}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

Self Learning AI

초기 속성은 eval 모드에서 사용할 값으로 설정되어 있음
\index{act() (agents.self\_learning.agent.Agent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}, \emph{include\_pi=False}, \emph{tau=0}}{}~\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{scripts.run\_game.State}}}}}) -- 현재 게임 상태

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{include\_pi}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- 다음 수와 pi를 같이 반환할지 여부
True일 때는 pi를 같이 반환함

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{tau}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 몇 번째 수 까지 탐색을 할 것인지 {[}0, n{]}
0 일 때는 탐색을 하지 않음 (평가, 테스트)
0 이상일 때는 정해진 확률에 따라 무작위로 다음 수를 선택함 (학습)

\end{itemize}

\item[{반환}] \leavevmode

(int, np.array{[}1, 400{]}) -- move, pi
\begin{itemize}
\item {} 
move: {[}0, 399{]} 사이의 정수, 탐색을 하지 않는다면, 가장 pi값이 높은 행동을 선택함

\item {} 
pi: 1 x 400 실수 행렬, 모든 행동에 대한 MCTS의 탐색 빈도를 정규화

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{alpha (agents.self\_learning.agent.Agent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.alpha}}\pysigline{\sphinxbfcode{\sphinxupquote{alpha}}\sphinxbfcode{\sphinxupquote{ = 1.0}}}
\end{fulllineitems}

\index{close() (agents.self\_learning.agent.Agent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{cputc (agents.self\_learning.agent.Agent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.cputc}}\pysigline{\sphinxbfcode{\sphinxupquote{cputc}}\sphinxbfcode{\sphinxupquote{ = 1.0}}}
\end{fulllineitems}

\index{epsilon (agents.self\_learning.agent.Agent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.epsilon}}\pysigline{\sphinxbfcode{\sphinxupquote{epsilon}}\sphinxbfcode{\sphinxupquote{ = -1.0}}}
\end{fulllineitems}

\index{max\_depth (agents.self\_learning.agent.Agent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.max_depth}}\pysigline{\sphinxbfcode{\sphinxupquote{max\_depth}}\sphinxbfcode{\sphinxupquote{ = 32}}}
\end{fulllineitems}

\index{mirror (agents.self\_learning.agent.Agent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.mirror}}\pysigline{\sphinxbfcode{\sphinxupquote{mirror}}\sphinxbfcode{\sphinxupquote{ = True}}}
\end{fulllineitems}

\index{model (agents.self\_learning.agent.Agent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.model}}\pysigline{\sphinxbfcode{\sphinxupquote{model}}\sphinxbfcode{\sphinxupquote{ = None}}}
\end{fulllineitems}

\index{model\_cls (agents.self\_learning.agent.Agent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.model_cls}}\pysigline{\sphinxbfcode{\sphinxupquote{model\_cls}}}
alias of {\hyperref[\detokenize{agents.self_learning:agents.self_learning.models.C64r6L1024r}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.self\_learning.models.C64r6L1024r}}}}}

\end{fulllineitems}

\index{model\_file (agents.self\_learning.agent.Agent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.model_file}}\pysigline{\sphinxbfcode{\sphinxupquote{model\_file}}\sphinxbfcode{\sphinxupquote{ = 'models/C64r6L1024r/v1/best.pt'}}}
\end{fulllineitems}

\index{n\_simulations (agents.self\_learning.agent.Agent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.n_simulations}}\pysigline{\sphinxbfcode{\sphinxupquote{n\_simulations}}\sphinxbfcode{\sphinxupquote{ = 100000}}}
\end{fulllineitems}

\index{planner (agents.self\_learning.agent.Agent attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.planner}}\pysigline{\sphinxbfcode{\sphinxupquote{planner}}\sphinxbfcode{\sphinxupquote{ = None}}}
\end{fulllineitems}

\index{reset() (agents.self\_learning.agent.Agent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{\emph{load\_model=True}, \emph{visualize\_tree=False}}{}~\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{load\_model}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- 학습한 모델을 로드할 지 여부, 학습할 때는 False 로 하고,
평가할 때는 기본 값 True 사용함

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{visualize\_tree}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- MCTS 시각화 기능을 사용하는지 여부, 학습할 때는 False, 평가할 때는 True

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{set\_params() (agents.self\_learning.agent.Agent method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Agent.set_params}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{set\_params}}}{\emph{model}, \emph{max\_depth}, \emph{n\_simulations}, \emph{cputc}, \emph{epsilon}, \emph{alpha}, \emph{mirror}}{}
AI의 초기화 작업은 reset 에서 실행하지만, 편의 목적으로 일부 값은 set\_params 에서 설정함
set\_params 는 reset 이전에 호출되어야 함
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{model}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.nn}}) -- 인공신경망(policy + value net)

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{max\_depth}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- MCTS 최대 탐색 깊이

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_simulations}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- MCTS 시뮬레이션 횟수

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{cputc}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) -- MCTS, UCB 공식에서 U의 가중치 c (cU+Q)S

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{epsilon}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) -- MCTS, UCB 공식에서 탐색 가중치

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{alpha}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) -- MCTS, UCB 공식에서 활용 가중치

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mirror}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- True일 때는 학습 AI가 흑과 백의 데이터를 모두 학습에 사용함
현재 자신과 다른 색깔일때는 판을 180도 회전하여 학습 데이터로 사용

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Sample (agents.self\_learning.agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Sample}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.self\_learning.agent.}}\sphinxbfcode{\sphinxupquote{Sample}}}{\emph{observation}, \emph{state}, \emph{move}, \emph{pi}, \emph{reward}, \emph{done}, \emph{legal\_moves}}{}
Bases: \sphinxcode{\sphinxupquote{object}}

데이터 샘플 저장 용
\index{done (agents.self\_learning.agent.Sample attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Sample.done}}\pysigline{\sphinxbfcode{\sphinxupquote{done}}}
\end{fulllineitems}

\index{legal\_moves (agents.self\_learning.agent.Sample attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Sample.legal_moves}}\pysigline{\sphinxbfcode{\sphinxupquote{legal\_moves}}}
\end{fulllineitems}

\index{move (agents.self\_learning.agent.Sample attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Sample.move}}\pysigline{\sphinxbfcode{\sphinxupquote{move}}}
\end{fulllineitems}

\index{observation (agents.self\_learning.agent.Sample attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Sample.observation}}\pysigline{\sphinxbfcode{\sphinxupquote{observation}}}
\end{fulllineitems}

\index{pi (agents.self\_learning.agent.Sample attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Sample.pi}}\pysigline{\sphinxbfcode{\sphinxupquote{pi}}}
\end{fulllineitems}

\index{reward (agents.self\_learning.agent.Sample attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Sample.reward}}\pysigline{\sphinxbfcode{\sphinxupquote{reward}}}
\end{fulllineitems}

\index{state (agents.self\_learning.agent.Sample attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.Sample.state}}\pysigline{\sphinxbfcode{\sphinxupquote{state}}}
\end{fulllineitems}


\end{fulllineitems}

\index{play\_game() (agents.self\_learning.agent 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.play_game}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.self\_learning.agent.}}\sphinxbfcode{\sphinxupquote{play\_game}}}{\emph{inqueue}, \emph{outqueue}, \emph{seed}}{}
종료되지 않고 무한 루프로 실행,
inqueue 에서 다음 인자를 받아서, outqueue로 반환
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{inqueue}} -- \begin{itemize}
\item {} 
(Agent) -- white, white AI

\item {} 
(Agent) -- black, black AI

\item {} 
(float) -- turns\_until\_tau0, 탐색 횟수 (tau)

\item {} 
(int) -- max\_turn, 최대 턴 (default: 80)

\item {} 
(bool) -- mirror

\item {} 
(bool) -- reversed\_reward, 학습 AI가 black 일 때 True 설정

\end{itemize}


\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{outqueue}} -- \begin{itemize}
\item {} 
(float) -- reward

\item {} 
(bool) -- turn

\item {} 
(list) -- short\_term\_memory

\end{itemize}


\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{seed}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- worker 를 이 seed 값으로 초기화, worker 는 모두 다른 seed 값으로 초기화 해야 함

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{train() (agents.self\_learning.agent 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.agent.train}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.self\_learning.agent.}}\sphinxbfcode{\sphinxupquote{train}}}{}{}
모델 학습 함수

\end{fulllineitems}



\subsection{agents.self\_learning.models module}
\label{\detokenize{agents.self_learning:agents-self-learning-models-module}}\label{\detokenize{agents.self_learning:module-agents.self_learning.models}}\index{agents.self\_learning.models (모듈)}
Self Learning AI 가 사용할 수 있는 인공신경망
\index{C64r6L1024r (agents.self\_learning.models 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.models.C64r6L1024r}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.self\_learning.models.}}\sphinxbfcode{\sphinxupquote{C64r6L1024r}}}{\emph{observation\_shape}, \emph{len\_state}, \emph{n\_actions}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

CNN 6층 + MLP 1층 구조
\index{forward() (agents.self\_learning.models.C64r6L1024r method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.models.C64r6L1024r.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{observations}, \emph{states}}{}
Defines the computation performed at every call.

Should be overriden by all subclasses.

\begin{sphinxadmonition}{note}{주석:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{L1024r (agents.self\_learning.models 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.models.L1024r}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.self\_learning.models.}}\sphinxbfcode{\sphinxupquote{L1024r}}}{\emph{observation\_shape}, \emph{len\_state}, \emph{n\_actions}}{}
Bases: \sphinxcode{\sphinxupquote{torch.nn.modules.module.Module}}

1024개의 은닉노드를 가진 간단한 MLP
\index{forward() (agents.self\_learning.models.L1024r method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.models.L1024r.forward}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\emph{observations}, \emph{states}}{}
Defines the computation performed at every call.

Should be overriden by all subclasses.

\begin{sphinxadmonition}{note}{주석:}
Although the recipe for forward pass needs to be defined within
this function, one should call the \sphinxcode{\sphinxupquote{Module}} instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{weights\_init() (agents.self\_learning.models 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.models.weights_init}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.self\_learning.models.}}\sphinxbfcode{\sphinxupquote{weights\_init}}}{\emph{m}}{}
\end{fulllineitems}



\subsection{agents.self\_learning.memory module}
\label{\detokenize{agents.self_learning:agents-self-learning-memory-module}}\label{\detokenize{agents.self_learning:module-agents.self_learning.memory}}\index{agents.self\_learning.memory (모듈)}
Replay Memory
\index{ReplayMemory (agents.self\_learning.memory 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.memory.ReplayMemory}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.self\_learning.memory.}}\sphinxbfcode{\sphinxupquote{ReplayMemory}}}{\emph{capacity}, \emph{memory\_path}}{}
Bases: \sphinxcode{\sphinxupquote{object}}

게임 플레이 데이터를 저장하는 용도
\index{add\_samples() (agents.self\_learning.memory.ReplayMemory method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.memory.ReplayMemory.add_samples}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{add\_samples}}}{\emph{samples}}{}
\end{fulllineitems}

\index{capacity (agents.self\_learning.memory.ReplayMemory attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.memory.ReplayMemory.capacity}}\pysigline{\sphinxbfcode{\sphinxupquote{capacity}}}
\end{fulllineitems}

\index{get\_dataset() (agents.self\_learning.memory.ReplayMemory method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.memory.ReplayMemory.get_dataset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{get\_dataset}}}{\emph{train\_set\_size}, \emph{validation\_set\_size}}{}
학습 데이터와, 검증 데이터의 개수를 입력함
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{train\_set\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 학습 데이터 개수

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{validation\_set\_size}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 검증 데이터 개수

\end{itemize}

\item[{반환}] \leavevmode
(torch.Tensor, torch.Tensor) -- train\_set, validation\_set
- train\_set: 학습 데이터 세트
- validation\_set: 검증 데이터 세트

\end{description}\end{quote}

\end{fulllineitems}

\index{pos (agents.self\_learning.memory.ReplayMemory attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.memory.ReplayMemory.pos}}\pysigline{\sphinxbfcode{\sphinxupquote{pos}}}
\end{fulllineitems}

\index{size (agents.self\_learning.memory.ReplayMemory attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.memory.ReplayMemory.size}}\pysigline{\sphinxbfcode{\sphinxupquote{size}}}
\end{fulllineitems}


\end{fulllineitems}



\subsection{agents.self\_learning.mcts module}
\label{\detokenize{agents.self_learning:agents-self-learning-mcts-module}}\label{\detokenize{agents.self_learning:module-agents.self_learning.mcts}}\index{agents.self\_learning.mcts (모듈)}
Self Learning용 Monte Carlo Tree Search 구현
\index{MCTSPlanner (agents.self\_learning.mcts 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.MCTSPlanner}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.self\_learning.mcts.}}\sphinxbfcode{\sphinxupquote{MCTSPlanner}}}{\emph{turn}, \emph{max\_depth=9223372036854775807}, \emph{action\_score='visits'}, \emph{cputc=1.0}, \emph{epsilon=0.2}, \emph{alpha=0.8}, \emph{model=None}, \emph{mirror=True}}{}
Bases: \sphinxcode{\sphinxupquote{object}}

MCTS 알고리즘 구현
\index{backup() (agents.self\_learning.mcts.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.MCTSPlanner.backup}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{backup}}}{\emph{node}, \emph{delta}}{}
시뮬레이션 결과 업데이트
턴마다 Negamax 스타일로 보상의 부호를 바꿈,
보상구간 {[}-1, 1{]}을 가정함
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{node}} ({\hyperref[\detokenize{agents.self_learning:agents.self_learning.mcts.Node}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{agents.self\_learning.mcts.Node}}}}}) -- 탐색한 트리의 종단노드

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{delta}} (\sphinxstyleliteralemphasis{\sphinxupquote{float}}) -- 시뮬레이션으로 알아낸 보상

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{best\_action() (agents.self\_learning.mcts.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.MCTSPlanner.best_action}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{best\_action}}}{\emph{node}, \emph{deterministic}}{}
현재 노드에서 가장 좋은 행동을 결정하여 반환함
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{node}} ({\hyperref[\detokenize{agents.self_learning:agents.self_learning.mcts.Node}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{agents.self\_learning.mcts.Node}}}}}) -- 현재 상태 노드 v0

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{deterministic}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- 결정적으로 행동을 선택(True)할 것인지 확률적으로 선택(False)할 것인지 여부

\end{itemize}

\item[{반환}] \leavevmode
(chess.Move) -- 가장 좋은 행동

\end{description}\end{quote}

\end{fulllineitems}

\index{pi() (agents.self\_learning.mcts.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.MCTSPlanner.pi}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{pi}}}{\emph{node}, \emph{tau}}{}
현재 상태 노드에서 pi 값을 계산함
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{node}} ({\hyperref[\detokenize{agents.self_learning:agents.self_learning.mcts.Node}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{agents.self\_learning.mcts.Node}}}}}) -- 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{tau}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 

\end{itemize}

\item[{반환}] \leavevmode
(torch.Tensor{[}400{]}) -- 현재 노드에서 시뮬레이션 했던 모든 행동들을 확률 값으로 변환함

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (agents.self\_learning.mcts.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.MCTSPlanner.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{board\_state}}{}
인공신경망을 이용해 현재 상태에서 가능한 수들의 선택 확률과 가치를 예측함
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{board\_state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 

\item[{반환}] \leavevmode
(torch.Tensor, torch.Tensor) -- move\_prob, value
- move\_prob: torch.Tensor, 가능한 수들의 선택 확률
- value: torch.Tensor, 현재 상태의 가치

\end{description}\end{quote}

\end{fulllineitems}

\index{search() (agents.self\_learning.mcts.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.MCTSPlanner.search}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{search}}}{\emph{state}, \emph{n\_simulations=1000}, \emph{tau=0.0}, \emph{timeout=10}}{}
주어진 상태와 제약조건(시뮬레이션 횟수와 시간제한)동안 시뮬레이션을 하고
찾아낸 가장 좋은 수을 반환함
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{n\_simulations}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 최대 시뮬레이션 횟수

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{tau}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{timeout}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) -- 시간제한 (sec.)

\end{itemize}

\item[{반환}] \leavevmode
(chess.Move) -- 가장 좋은 수

\end{description}\end{quote}

\end{fulllineitems}

\index{tree\_policy() (agents.self\_learning.mcts.MCTSPlanner method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.MCTSPlanner.tree_policy}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{tree\_policy}}}{\emph{node}}{}
선택 및 확장 단계
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{node}} ({\hyperref[\detokenize{agents.self_learning:agents.self_learning.mcts.Node}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{agents.self\_learning.mcts.Node}}}}}) -- 현재 노드 v0

\item[{반환}] \leavevmode
(Node) -- 생성한 트리의 종단노드 vl

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Node (agents.self\_learning.mcts 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.self\_learning.mcts.}}\sphinxbfcode{\sphinxupquote{Node}}}{\emph{turn}, \emph{state}}{}
Bases: \sphinxcode{\sphinxupquote{object}}

상태공간 노드

Attrs:
\begin{itemize}
\item {} 
(bool) -- turn, white=True, black=False

\item {} 
({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{scripts.run\_game.State}}}}}) -- state

\item {} 
({\hyperref[\detokenize{agents.self_learning:agents.self_learning.mcts.Node}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.self\_learning.mcts.Node}}}}}) -- parent, 이전 상태를 저장하고 있는 노드

\item {} 
(dict) -- children, dict(key-\textgreater{} chess.Move, value-\textgreater{} Node), 특정 수를 두고 난 뒤의 상태

\item {} 
(int) -- visits, 이 노드를 방문한 횟수

\item {} 
(float) -- wins, 이 노드와 자식 노드에서 받은 보상의 총합

\item {} 
(float) -- ucb, 이 노드의 ucb 값

\end{itemize}
\index{children (agents.self\_learning.mcts.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.children}}\pysigline{\sphinxbfcode{\sphinxupquote{children}}}
\end{fulllineitems}

\index{key (agents.self\_learning.mcts.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.key}}\pysigline{\sphinxbfcode{\sphinxupquote{key}}}
\end{fulllineitems}

\index{next\_turn (agents.self\_learning.mcts.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.next_turn}}\pysigline{\sphinxbfcode{\sphinxupquote{next\_turn}}}
\end{fulllineitems}

\index{parent (agents.self\_learning.mcts.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.parent}}\pysigline{\sphinxbfcode{\sphinxupquote{parent}}}
\end{fulllineitems}

\index{print\_tree() (agents.self\_learning.mcts.Node method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.print_tree}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{print\_tree}}}{\emph{move=None}, \emph{depth=0}}{}
콘솔에 트리 출력

\end{fulllineitems}

\index{prob (agents.self\_learning.mcts.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.prob}}\pysigline{\sphinxbfcode{\sphinxupquote{prob}}}
\end{fulllineitems}

\index{state (agents.self\_learning.mcts.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.state}}\pysigline{\sphinxbfcode{\sphinxupquote{state}}}
\end{fulllineitems}

\index{turn (agents.self\_learning.mcts.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.turn}}\pysigline{\sphinxbfcode{\sphinxupquote{turn}}}
\end{fulllineitems}

\index{ucb (agents.self\_learning.mcts.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.ucb}}\pysigline{\sphinxbfcode{\sphinxupquote{ucb}}}
\end{fulllineitems}

\index{visits (agents.self\_learning.mcts.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.visits}}\pysigline{\sphinxbfcode{\sphinxupquote{visits}}}
\end{fulllineitems}

\index{wins (agents.self\_learning.mcts.Node attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.mcts.Node.wins}}\pysigline{\sphinxbfcode{\sphinxupquote{wins}}}
\end{fulllineitems}


\end{fulllineitems}



\subsection{agents.self\_learning.utils module}
\label{\detokenize{agents.self_learning:module-agents.self_learning.utils}}\label{\detokenize{agents.self_learning:agents-self-learning-utils-module}}\index{agents.self\_learning.utils (모듈)}
Self Learning AI에 필요한 기타 파일
\index{Buffer (agents.self\_learning.utils 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.Buffer}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.self\_learning.utils.}}\sphinxbfcode{\sphinxupquote{Buffer}}}{\emph{args}}{}
Bases: \sphinxcode{\sphinxupquote{object}}

학습에 필요한 메모리 공간을 미리 할당해두고 재사용하기 위해 사용

\end{fulllineitems}

\index{Operators (agents.self\_learning.utils 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.Operators}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.self\_learning.utils.}}\sphinxbfcode{\sphinxupquote{Operators}}}
Bases: \sphinxcode{\sphinxupquote{object}}

학습 도중 실험 조건을 변경하기 위해 사용
\index{lr() (agents.self\_learning.utils.Operators static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.Operators.lr}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{lr}}}{\emph{log}, \emph{current\_model}, \emph{best\_model}, \emph{arguments={[}{]}}}{}
\end{fulllineitems}

\index{n\_simulations() (agents.self\_learning.utils.Operators static method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.Operators.n_simulations}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static }}\sphinxbfcode{\sphinxupquote{n\_simulations}}}{\emph{log}, \emph{current\_model}, \emph{best\_model}, \emph{arguments={[}{]}}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{Record (agents.self\_learning.utils 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.Record}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.self\_learning.utils.}}\sphinxbfcode{\sphinxupquote{Record}}}{\emph{args}}{}
Bases: \sphinxcode{\sphinxupquote{object}}

학습 동안에 저장되는 데이터를 모아두는 객체

\end{fulllineitems}

\index{decode\_move() (agents.self\_learning.utils 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.decode_move}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.self\_learning.utils.}}\sphinxbfcode{\sphinxupquote{decode\_move}}}{\emph{board}, \emph{move\_code}, \emph{turn}, \emph{mirror}}{}
{[}0, 399{]} 정수로 된 수를 chess.Move로 변환, encode\_move의 반대
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{board}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{move\_code}} (\sphinxstyleliteralemphasis{\sphinxupquote{chess.Move}}) -- 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{turn}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mirror}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- 

\end{itemize}

\item[{반환}] \leavevmode
(chess.Move) --

\end{description}\end{quote}

\end{fulllineitems}

\index{encode\_move() (agents.self\_learning.utils 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.encode_move}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.self\_learning.utils.}}\sphinxbfcode{\sphinxupquote{encode\_move}}}{\emph{move}, \emph{turn}, \emph{mirror}}{}
chess.Move를 {[}0, 399{]} 정수로 변환, decode\_move의 반대
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{move}} (\sphinxstyleliteralemphasis{\sphinxupquote{chess.Move}}) -- 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{turn}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- white=True, black=False

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mirror}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- 

\end{itemize}

\item[{반환}] \leavevmode
(int) -- 정수 {[}0, 399{]} 범위

\end{description}\end{quote}

\end{fulllineitems}

\index{encode\_observation() (agents.self\_learning.utils 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.encode_observation}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.self\_learning.utils.}}\sphinxbfcode{\sphinxupquote{encode\_observation}}}{\emph{state}, \emph{turn}, \emph{mirror}}{}
fen 표기법으로 된 보드 상태를 입력받아서, numpy array 형태로 변환
각 채널은 특정 말에 대응되며, 해당 말이 존재하는 위치에 1.로 표시
나머지 공간은 0.
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{state}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) -- fen 표기법

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{turn}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- white=True, black=False

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mirror}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- 미러 옵션 사용여부

\end{itemize}

\item[{반환}] \leavevmode
(torch.Tensor{[}6x8x8{]}) -- 게임 상태를 tensor로 변환

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_logger() (agents.self\_learning.utils 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.get_logger}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.self\_learning.utils.}}\sphinxbfcode{\sphinxupquote{get\_logger}}}{}{}
로거 반환
\begin{quote}\begin{description}
\item[{반환}] \leavevmode
(logging.logger) --

\end{description}\end{quote}

\end{fulllineitems}

\index{is\_castling() (agents.self\_learning.utils 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.is_castling}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.self\_learning.utils.}}\sphinxbfcode{\sphinxupquote{is\_castling}}}{\emph{state}, \emph{turn}, \emph{mirror=True}}{}
castling 가능 여부 검사하여 실수 list로 반환, 게임 상태로 사용
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{turn}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- 

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{mirror}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) -- 

\end{itemize}

\item[{반환}] \leavevmode
({[}float, float{]}) -- 길이 2

\end{description}\end{quote}

\end{fulllineitems}

\index{make\_piece\_index() (agents.self\_learning.utils 모듈)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.self_learning:agents.self_learning.utils.make_piece_index}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{agents.self\_learning.utils.}}\sphinxbfcode{\sphinxupquote{make\_piece\_index}}}{}{}
알파벳으로 표시된 체스 기물을 정수로 변환하는 dict를 반환
\begin{quote}\begin{description}
\item[{반환}] \leavevmode
(dict) -

\end{description}\end{quote}

\end{fulllineitems}



\section{agents.stockfish package}
\label{\detokenize{agents.stockfish:agents-stockfish-package}}\label{\detokenize{agents.stockfish::doc}}

\subsection{Submodules}
\label{\detokenize{agents.stockfish:submodules}}

\subsection{Module contents}
\label{\detokenize{agents.stockfish:module-contents}}\label{\detokenize{agents.stockfish:module-agents.stockfish}}\index{agents.stockfish (모듈)}

\subsection{agents.stockfish.agent module}
\label{\detokenize{agents.stockfish:module-agents.stockfish.agent}}\label{\detokenize{agents.stockfish:agents-stockfish-agent-module}}\index{agents.stockfish.agent (모듈)}
Stockfish Wrapper
\index{Stockfish (agents.stockfish.agent 종류)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.stockfish:agents.stockfish.agent.Stockfish}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{agents.stockfish.agent.}}\sphinxbfcode{\sphinxupquote{Stockfish}}}{\emph{name}, \emph{color}}{}
Bases: {\hyperref[\detokenize{agents:agents.BaseAgent}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{agents.BaseAgent}}}}}

UCI 인터페이스를 이용해 Stockfish를 래핑함, 실제 구현은 stockfish\_8\_*.exe 임

Microchees에 맞게 Stockfish의 기능을 제한함
\begin{itemize}
\item {} 
탐색번위 제한: 5x4 번위 이외의 공간을 탐색하지 못하게 함

\item {} 
castling 제한: castling 위치가 다르기 때문에 Stockfish에서는 에러 발생

\end{itemize}

TODO: 간헐적으로 에러가 발생함, 원인은 아직 불명
\index{act() (agents.stockfish.agent.Stockfish method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.stockfish:agents.stockfish.agent.Stockfish.act}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{act}}}{\emph{state}}{}
AI 초기화
\begin{quote}\begin{description}
\item[{매개 변수}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{state}} ({\hyperref[\detokenize{scripts:scripts.run_game.State}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{State}}}}}) -- 현재 게임 상태

\item[{반환}] \leavevmode

다음 수 반환
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{chess.Move}} -- AI의 다음수

\item {} 
None -- 다음 수를 반환할 필요가 없는 경우

\end{itemize}


\end{description}\end{quote}

\end{fulllineitems}

\index{available\_squares (agents.stockfish.agent.Stockfish attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.stockfish:agents.stockfish.agent.Stockfish.available_squares}}\pysigline{\sphinxbfcode{\sphinxupquote{available\_squares}}\sphinxbfcode{\sphinxupquote{ = {[}32, 33, 34, 35, 24, 25, 26, 27, 16, 17, 18, 19, 8, 9, 10, 11, 0, 1, 2, 3{]}}}}
\end{fulllineitems}

\index{close() (agents.stockfish.agent.Stockfish method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.stockfish:agents.stockfish.agent.Stockfish.close}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{close}}}{}{}
AI 종료
마지막에 한 번 실행

\end{fulllineitems}

\index{engine (agents.stockfish.agent.Stockfish attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.stockfish:agents.stockfish.agent.Stockfish.engine}}\pysigline{\sphinxbfcode{\sphinxupquote{engine}}\sphinxbfcode{\sphinxupquote{ = None}}}
\end{fulllineitems}

\index{engine\_path (agents.stockfish.agent.Stockfish attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.stockfish:agents.stockfish.agent.Stockfish.engine_path}}\pysigline{\sphinxbfcode{\sphinxupquote{engine\_path}}\sphinxbfcode{\sphinxupquote{ = 'stockfish'}}}
\end{fulllineitems}

\index{level (agents.stockfish.agent.Stockfish attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.stockfish:agents.stockfish.agent.Stockfish.level}}\pysigline{\sphinxbfcode{\sphinxupquote{level}}\sphinxbfcode{\sphinxupquote{ = 20}}}
\end{fulllineitems}

\index{reset() (agents.stockfish.agent.Stockfish method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{agents.stockfish:agents.stockfish.agent.Stockfish.reset}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{reset}}}{}{}
AI 초기화
게임 초기에 한 번 실행

\end{fulllineitems}


\end{fulllineitems}

\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python 모듈 목록}
\begin{sphinxtheindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{a}
\item {\sphinxstyleindexentry{agents}}\sphinxstyleindexpageref{agents:\detokenize{module-agents}}
\item {\sphinxstyleindexentry{agents.basic}}\sphinxstyleindexpageref{agents.basic:\detokenize{module-agents.basic}}
\item {\sphinxstyleindexentry{agents.basic.debug\_agent}}\sphinxstyleindexpageref{agents.basic:\detokenize{module-agents.basic.debug_agent}}
\item {\sphinxstyleindexentry{agents.basic.human}}\sphinxstyleindexpageref{agents.basic:\detokenize{module-agents.basic.human}}
\item {\sphinxstyleindexentry{agents.basic.random\_agent}}\sphinxstyleindexpageref{agents.basic:\detokenize{module-agents.basic.random_agent}}
\item {\sphinxstyleindexentry{agents.search}}\sphinxstyleindexpageref{agents.search:\detokenize{module-agents.search}}
\item {\sphinxstyleindexentry{agents.search.abp\_negamax\_search\_agent}}\sphinxstyleindexpageref{agents.search:\detokenize{module-agents.search.abp_negamax_search_agent}}
\item {\sphinxstyleindexentry{agents.search.mcts\_agent}}\sphinxstyleindexpageref{agents.search:\detokenize{module-agents.search.mcts_agent}}
\item {\sphinxstyleindexentry{agents.search.negamax\_search\_agent}}\sphinxstyleindexpageref{agents.search:\detokenize{module-agents.search.negamax_search_agent}}
\item {\sphinxstyleindexentry{agents.search.one\_step\_search\_agent}}\sphinxstyleindexpageref{agents.search:\detokenize{module-agents.search.one_step_search_agent}}
\item {\sphinxstyleindexentry{agents.search.two\_step\_search\_agent}}\sphinxstyleindexpageref{agents.search:\detokenize{module-agents.search.two_step_search_agent}}
\item {\sphinxstyleindexentry{agents.self\_learning}}\sphinxstyleindexpageref{agents.self_learning:\detokenize{module-agents.self_learning}}
\item {\sphinxstyleindexentry{agents.self\_learning.agent}}\sphinxstyleindexpageref{agents.self_learning:\detokenize{module-agents.self_learning.agent}}
\item {\sphinxstyleindexentry{agents.self\_learning.mcts}}\sphinxstyleindexpageref{agents.self_learning:\detokenize{module-agents.self_learning.mcts}}
\item {\sphinxstyleindexentry{agents.self\_learning.memory}}\sphinxstyleindexpageref{agents.self_learning:\detokenize{module-agents.self_learning.memory}}
\item {\sphinxstyleindexentry{agents.self\_learning.models}}\sphinxstyleindexpageref{agents.self_learning:\detokenize{module-agents.self_learning.models}}
\item {\sphinxstyleindexentry{agents.self\_learning.utils}}\sphinxstyleindexpageref{agents.self_learning:\detokenize{module-agents.self_learning.utils}}
\item {\sphinxstyleindexentry{agents.stockfish}}\sphinxstyleindexpageref{agents.stockfish:\detokenize{module-agents.stockfish}}
\item {\sphinxstyleindexentry{agents.stockfish.agent}}\sphinxstyleindexpageref{agents.stockfish:\detokenize{module-agents.stockfish.agent}}
\indexspace
\bigletter{s}
\item {\sphinxstyleindexentry{scripts}}\sphinxstyleindexpageref{scripts:\detokenize{module-scripts}}
\item {\sphinxstyleindexentry{scripts.chess\_board}}\sphinxstyleindexpageref{scripts:\detokenize{module-scripts.chess_board}}
\item {\sphinxstyleindexentry{scripts.run\_game}}\sphinxstyleindexpageref{scripts:\detokenize{module-scripts.run_game}}
\item {\sphinxstyleindexentry{scripts.utils}}\sphinxstyleindexpageref{scripts:\detokenize{module-scripts.utils}}
\end{sphinxtheindex}

\renewcommand{\indexname}{색인}
\printindex
\end{document}