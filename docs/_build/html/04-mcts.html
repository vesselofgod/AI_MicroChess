
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="ko">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Monte Carlo Tree Search &#8212; Microchess AI Competition 0.1.0 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/translations.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="색인" href="genindex.html" />
    <link rel="search" title="검색" href="search.html" />
    <link rel="next" title="Self Learning AI" href="05-self_learning.html" />
    <link rel="prev" title="기본 AI 예제" href="03-basic_ai_examples.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>탐색</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="전체 색인"
             accesskey="I">색인</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python 모듈 목록"
             >모듈</a> |</li>
        <li class="right" >
          <a href="05-self_learning.html" title="Self Learning AI"
             accesskey="N">다음</a> |</li>
        <li class="right" >
          <a href="03-basic_ai_examples.html" title="기본 AI 예제"
             accesskey="P">이전</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Microchess AI Competition 0.1.0 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="monte-carlo-tree-search">
<h1>Monte Carlo Tree Search<a class="headerlink" href="#monte-carlo-tree-search" title="제목 주소">¶</a></h1>
<p>지금까지는 상태공간 탐색에 대한 기본적인 사항을 알아보았고, 탐색기반 예제 AI들과 일반 체스에서 가장 강력한 AI인 Stockfish를 살펴보았다.
Microchess가 간단함 게임에도 불구하고, 탐색공간이 매우 커질 수 있기 때문에, 간단한 예제 AI들의 성능은 충분하지 않았다.
심지어 기존에 가장 강력한 체스 AI인 Stockfish라고 하더라도
평가기준이 달라진 경진대회에서는 상당히 정밀한 수정이 없이는 기존 성능을 보여주지 못하기 때문에,
이 이상의 성능을 위해서는 새로운 방식으로 AI를 구현해야만 한다.
지금부터는 MCTS (Monte-Carlo Tree Search)와 Self Learning으로 Stockfish 이상의 성능을 달성하는 방법을 보인다.</p>
<div class="section" id="mcts">
<h2>MCTS 개요<a class="headerlink" href="#mcts" title="제목 주소">¶</a></h2>
<p>MCTS (Monte-Carlo Tree Search) <a class="footnote-reference" href="#id5" id="id1">[1]</a> 의 기본 아이디어는 탐색 공간이 거대하여 모두 탐색이 불가능하다면,
무작위로 상태공간을 표본추출(sampling)하여 통계적인 근사치를 구하고, 그 근사치에 근거한 의사결정을 하겠다는 것이다.
만약 충분히 많은 표본추출을 할 수 있다면 어떤 행동을 하는 것이 가장 승률이 높은지를 추정할 수 있다.
하나의 표본을 추출하는 과정은 현재 상태부터 마지막(예. 게임종료)까지 무작위로 행동을 결정하여 첫 번째 행동의 보상(평가값)을 알아내는 것이며,
이것을 시뮬레이션이라고 한다.</p>
<div class="figure" id="id6" style="width: 300px">
<span id="simple-mcts"></span><img alt="_images/simple_mcts.png" src="_images/simple_mcts.png" />
<p class="caption"><span class="caption-text">간단한 MCTS</span></p>
</div>
<p><a class="reference internal" href="#simple-mcts"><span class="std std-ref">간단한 MCTS</span></a> 는 현재 상태( <span class="math notranslate">\(S_0\)</span> )에서 여러 번 무작위로 시뮬레이션(점선)을 하고,
현재 선택 할 수 있는 행동들의 기대 보상을 추정하는 것이다.
무작위 시뮬레이션을 충분히 많이 해볼 수 있다면, 특정 상태에서 행동마다 기대보상을 추정할 수 있기 때문에,
전체 공간을 탐색하지 않더라도, 현재 상태에서 의사결정을 할 수 있다.
이 방법의 성능은 충분한 시뮬레이션에 달려있기 때문에, 기본 계산비용이 큰 경우가 많다.</p>
<p>단순 MCTS를 개선한 것이 UCT (Upper Confidence bound Tree search) <a class="footnote-reference" href="#id5" id="id2">[1]</a> 알고리즘이다. 이 알고리즘은 시뮬레이션을 할 때
완전히 무작위로 행동을 결정하는 대신, 이전에 시뮬레이션한 정보를 이용하여 시뮬레이션의 효율을 개선한 것이다.
요즘은 보통 UCT를 그냥 MCTS라고 부른다.</p>
</div>
<div class="section" id="multi-armed-bandit">
<h2>Multi-Armed Bandit<a class="headerlink" href="#multi-armed-bandit" title="제목 주소">¶</a></h2>
<div class="figure" id="id7" style="width: 400px">
<span id="mcts-mab-fig"></span><img alt="_images/mcts_multi_armed_bandit.png" src="_images/mcts_multi_armed_bandit.png" />
<p class="caption"><span class="caption-text">MCTS → Multi-Armed Bandit</span></p>
</div>
<p>MCTS에서 특정 한 상태를 떼어놓고 보면( <a class="reference internal" href="#mcts-mab-fig"><span class="std std-ref">MCTS → Multi-Armed Bandit</span></a> ), 이것은 비결정적인 보상을 반환하는 환경에서 특정 상태에서 어떤 행동을 하는것이
더 좋은지를 알아내는 문제이다. 지금까지 살펴본 탐색 기반 AI에서 취하는 기본 접근방법은 결정적인 환경을 가정하고,
특정 상태에서 가능한 행동들을 모두 시뮬레이션 해본다음, 그 결과를 바탕으로 어떤 행동이 더 좋은지 알아내는 것이다.
그러나, 만약 시뮬레이션 결과가 (MCTS의 무작위 탐색 때문에) 시뮬레이션을 수행 할 때마다 달라진다면,
가능한 행동들 중에서 더 좋은 행동을 찾기위해서는 모든 행동들을 여러 번 반복하여 시뮬레이션 해보고,
통계적인 추정을 할 필요가 있다. 시뮬레이션을 하면 할 수록 추정이 정확해 지겠지만, 계산비용을 절약하기 위해서는
최소한의 시뮬레이션 만으로 정확한 추정을 할 필요가 있다.</p>
<p>이런 문제를 탐색과 활용 딜레마(exploration-exploitation dilemma)라고 한다.
탐색과 활용 딜레마가 발생하는 가장 간단한 문제는 MAB (Multi-Armed Bandit) 문제이다.
여기서 Bandit란 슬롯머신을 의미한다. 만약 상금이 나올 확률이 서로 다른 여러 대의 슬롯머신이 있다면,
가장 높은 상금을 타는 방법은 어떤 슬롯머신이 더 확률이 높은지를 알아내고, 그 슬롯머신에서 게임을 계속 시도하는 것이다.
그러나 문제는 슬롯머신에서 상금을 탈 확률을 알아내기 위해서는 충분한 게임(실험)을 해서,
통계적인 추정치를 알아내야만 한다는 것이다. 투자할 수 있는 총 자금에는 한계가 있고,
실험에는 비용이 들기 때문에 지나친 게임은 최종 기대이익을 낮출 수 있다.</p>
<p>이 문제에 대한 가장 간단한 해법 중 하나는 <span class="math notranslate">\(\epsilon-greedy\)</span> 알고리즘이다.
이 알고리즘은 전체 자원 중 일정 비율(<span class="math notranslate">\(\epsilon\)</span>)을 실험을 위해 사용하는 알고리즘이다.
예를 들어 현재 가진 돈으로 1,000게임을 할 수 있다면, 그 중 500게임을 실험을 위해 사용하는 식이다.
2대의 슬롯머신이 있다면, 500게임을 250게임씩 균등하게 나눠서 실험해보고, 그 중에서 상금이 더 높은 슬롯머신에서
나머지 500게임을 하는 것이다. 흔히 처음에 실험을 위해 하는 500게임을 탐색(exploration)이라고 하고,
나중에 상금을 얻기 위해 하는 행동을 활용(exploitation)이라고 한다.</p>
<p>이 방법은 직관적으로 알 수 있다시피, 탐색과 활용의 비율을 잘 조절하는 것이 기대이익을 높이는 핵심이다.
탐색에 너무 많은 게임을 하면, 이익을 극대화 시키기 위한 활동인 활용의 비율이 낮아져 최종 상금이 낮아진다.
반면, 탐색에 너무 적은 게임을 할당하면, 부정확한 추정에 근거해 활용을 하기 때문에, 최종 기대보상이 낮아진다.
따라서, 탐색과 활용의 비율을 적절히 조정하는 것이 이 문제를 잘 해결하는 방법이다.
이 문제, 탐색과 활용 딜레마(dilemma)는 강화학습에서 중요한 문제 중 하나이다.</p>
<p>UCB (Upper Confidence Bound)는 <span class="math notranslate">\(\epsilon\)</span> 보다 좀 더 정교한 방식으로 탐색과 활용을 조정한다.
반복해서 시뮬레이션을 하면서, 얻은 예측의 신뢰도(upper confidence)를 이용해 현재 추정치가 얼마나 신뢰할 만한지를 판단한다.
<span class="math notranslate">\(\epsilon\)</span> 처럼 명확하게 탐색과 활용을 분리하지 않고, 언제나 각 행동에 대해 현재 보상과 신뢰도에 의해 결정된
UCB 값이 가장 높은 행동을 결정한다. 기본적으로 기대 보상이 높으며, 신뢰도가 낮은 행동일 수록 선택될 가능성이 높다.
<a class="reference internal" href="#equation-ucb1">(1)</a> 은 가장 대표적인 UCB 공식이다. 첫 번째 항은 활용에 대응되는 기대보상(평균)이며,
두 번째 항은 탐색에 대응되는 값이다. 두 번째 항의 N은 전체 시뮬레이션 횟수이고, n_a는 그 중에서 특정 행동 a를 시뮬레이션한
횟수이다. 따라서, 전체 시뮬레이션 횟수에 비해서 a를 시뮬레이션 한 횟수가 클 수록 두번째 항은 자연스럽게 줄어든다.</p>
<div class="math notranslate" id="equation-ucb1">
<span class="eqno">(1)<a class="headerlink" href="#equation-ucb1" title="Permalink to this equation">¶</a></span>\[UCB_a = \frac{v_a}{n_a} + \sqrt{\frac{2log{N}}{n_a}}\]</div>
<p><span class="math notranslate">\(\epsilon\)</span> -greedy 는 적절한 <span class="math notranslate">\(\epsilon\)</span> 를 알고 있다면, 좋은 결과를 얻을 수 있지만,
좋은 <span class="math notranslate">\(\epsilon\)</span> 값을 모를 경우 UCB가 보다 좋은 성능을 보이는 경우가 많다. 다만, 끝까지 탐색을 멈추지 않기 때문에
충분히 탐색을 한 뒤에도 예상과 달리 전혀 엉뚱한 행동을 선택하기도 한다.</p>
</div>
<div class="section" id="upper-confidence-tree">
<h2>Upper Confidence Tree<a class="headerlink" href="#upper-confidence-tree" title="제목 주소">¶</a></h2>
<p>MCTS의 시뮬레이션은 결국 각 단계에서 MAB 문제를 연속적으로 풀고 있는 것으로 볼 수 있다.
각 깊이에서 아직 충분히 시도해 보지 않은 행동을 실험을(탐색) 할 것인지, 아니면 지금까지 시뮬레이션 한 결과 가장 좋았던
행동에서 연결되는 미래상태를 더 시뮬레이션하여 추정치를 정교하게 갱신(활용)할 것인지를 끊임없이 결정하는 것이다.
UCT (Upper Confidence Tree)는 이 과정에 UCB를 사용하여 MCTS의 성능을 높인 것이다.</p>
<div class="figure" id="id8">
<span id="uct-algorithm"></span><img alt="_images/uct.png" src="_images/uct.png" />
<p class="caption"><span class="caption-text">UCT 알고리즘</span></p>
</div>
<p>무작위 탐색(Monte-Carlo simulation)만을 하는 기존 MCTS와 달리 UCT는 UCB를 사용하는 Tree Policy 와
Default Policy (무작위 탐색) 단계로 구분된다. <a class="reference internal" href="#uct-algorithm"><span class="std std-ref">UCT 알고리즘</span></a> 에 UCT의 네 단계를 설명하였다.
첫 번째 선택 단계에서는 <a class="reference internal" href="#equation-ucb1">(1)</a> 을 이용하여 현재 상태에서 UCB값이 가장 큰 행동을 선택한다.
제일 처음에 시뮬레이션을 시작할 때나 현재까지 탐색을 해본 상태를 따라 깊게 내려가다보면,
아직 한번도 시도해보지 않은 행동이 나타난다.
이때, 두 번째 확장 단계로 넘어간다. 이 단계에서는 아직 시도해보지 않은 행동들 중에서 무작위로 하나를 골라
현재까지 탐색해본 트리의 끝부분에 상태 노드를 하나 추가한다. 이 두 단계를 Tree Policy라고 한다.
세 번째 단계에서는 기존 MCTS 처럼 게임이 종료되는 상태까지 무작위로 트리를 탐색한다. 이것을 Default Policy라고 한다.
마지막 네 번째 단계에서는 게임이 종료되었을 때의 결과를 역전파하여 트리 노드의 기대 보상값을 갱신한다.
갱신된 보상값은 다음 반복(iteration)때, Tree Policy에서 사용된다.</p>
<div class="literal-block-wrapper docutils container" id="id9">
<span id="mcts-code"></span><div class="code-block-caption"><span class="caption-text">MCTS 기본코드</span><a class="headerlink" href="#id9" title="Permalink to this code">¶</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">v0</span> <span class="o">=</span> <span class="n">Node</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">turn</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span>
<span class="n">simulation_count</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
   <span class="k">if</span> <span class="n">simulation_count</span> <span class="o">&gt;</span> <span class="n">n_simulations</span> <span class="ow">or</span> <span class="n">time</span><span class="o">.</span><span class="n">perf_counter</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span> <span class="o">&gt;</span> <span class="mf">0.99</span> <span class="o">*</span> <span class="n">timeout</span><span class="p">:</span>
      <span class="c1"># 제약조건(시뮬레이션 횟수와 시간제한)을 초과하면 바로 시뮬레이션 종료</span>
      <span class="k">break</span>

   <span class="bp">self</span><span class="o">.</span><span class="n">_depth</span> <span class="o">=</span> <span class="mi">0</span>
   <span class="c1"># Tree Policy: 선택 및 확장</span>
   <span class="n">vl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_policy</span><span class="p">(</span><span class="n">v0</span><span class="p">)</span>
   <span class="c1"># Default Policy: Monte-Carlo 시뮬레이션</span>
   <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">default_policy</span><span class="p">(</span><span class="n">vl</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
   <span class="c1"># Backup: 보상 역전파</span>
   <span class="bp">self</span><span class="o">.</span><span class="n">backup</span><span class="p">(</span><span class="n">vl</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
   <span class="n">simulation_count</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<p>MCTS의 전체 흐름은 <a class="reference internal" href="#mcts-code"><span class="std std-ref">MCTS 기본코드</span></a> 에서 볼 수 있다. 주어진 자원(시뮬레이션 횟수 또는 시간제한)동안
계속 시뮬레이션을 반복하면서, Tree를 확장해 간다.</p>
<p>이 네 단계를 한번 수행하는 것을 시뮬레이션이라고 하고, 이 과정을 충분히 반복하면 현재 상태(root node)에서 각 행동들의
기대 보상값을 추정할 수 있다. 최종 의사결정을 할 때는 행동의 기대보상값(보상 / 실험 횟수)가 가장 높은 행동을 고르기도 하지만,
시도 횟수가 가장많은 것을 고르는 경우가 많다.</p>
<p>Minimax 같은 기존 탐색 알고리즘은 기본적으로 수평선 효과를 완화하기 위해 깊은 탐색이 필요하다.
탐색의 깊이가 깊어질 수록 탐색해야 할 상태가 지수적으로 증가하기 때문에,
계산복잡도를 대강 <span class="math notranslate">\(O(b^d)\)</span> 로 생각할 수 있다 (<em>b</em>: 평군 선택가능한 행동, <em>d</em>: 탐색깊이).
반면에 MCTS는 시뮬레이션 횟수(Monte-Carlo 샘플링 횟수)에 따라 계산비용이 증가한다.
대강의 계산복잡도는 <span class="math notranslate">\(O(nd)\)</span> 로 볼 수 있다(<em>n</em>: 시뮬레이션 횟수, <em>d</em>: 탐색깊이).
따라서 상태공간의 규모가 커질 수록 상대적으로 적은 계산 비용만을 필요로 한다.</p>
<p>주의할 점은, MCTS로 충분한 성능을 얻으려면, 충분힌 시뮬레이션이 필요하다는 것이다..
매우 오랜시간이 걸리지만 최적해를 찾는 것이 모장된 기존 탐색알고리즘과 달리,
MCTS는 근사해를 구하는 알고리즘이기 때문에 최적해를 찾는다고 보장할 수 없다.
이것을 해결하는 직접적인 방법은 충분한 시뮬레이션 횟수를 확보하는 것 뿐이다.
그러나, 너무 큰 시뮬레이션 횟수도 불필요하다.
비록 이론적으로는 MCTS의 시간복잡도가 선형으로 증가하기는 하지만,
실제구현에서는 그 외 요소도 있기 때문에 너무 큰 시뮬레이션 횟수는 큰 부담이 되는 경우가 많다.
때문에 대부분의 구현에서는 시뮬레이션을 몇 번할 것인지 미리 지정해두기 보다는,
주어진 시간에 맞춰 최대한 많이 시뮬레이션을 하는 방식을 사용한다.
이 경우 만약 주어진 시간이 변경되더라도 그에 맞춰 언제나 최선의 성능(근사해)을 보일 수 있다.</p>
</div>
<div class="section" id="id3">
<h2>MCTS 예제<a class="headerlink" href="#id3" title="제목 주소">¶</a></h2>
<p>MCTS 예제는 <a class="reference internal" href="agents.search.html#agents.search.mcts_agent.MCTSAgent" title="agents.search.mcts_agent.MCTSAgent"><code class="xref py py-class docutils literal notranslate"><span class="pre">MCTSAgent</span></code></a> 에 구현되어 있다.
기본 MCTS 구현은 Microchess 정도의 문제이서도 충분한 성능을 보이지 못하기 때문에, 두 가지를 수정 했다.</p>
<p><strong>탐색깊이 제한</strong></p>
<p>원래 MCTS는 게임의 종료상태까지 탐색을 시도하고, 종료상태에서 승리했는지 패배했는지에 따라 보상을 받고,
그 정보를 이용해 의사결정을 하는 것이지만, 실질적으로는 너무 깊은 공간을 탐색하는 것은 큰 계산비용이 소모된다.
뿐만아니라, 탐색할 공간이 커질 수록 충분히 정확한 결과를 얻기 위해 시뮬레이션의 횟수가 많이 필요하기 때문에
탐색깊이를 최대 6으로 제한하고, 마지막에서 평가함수를 사용하여 계산비용을 절감하였다.
비록 정교하지 못한 평가함수로 인해 보상의 추정이 왜곡될 위험이 있기는 하지만,
주어진 시뮬레이션 횟수에서 탐색깊이를 제한하지 않으면, 부정확한 추정으로 인해 명백히 나쁜 수를 둘 확률이 높았다.
체스 같은 턴제 게임에서는 한번의 나쁜 수가 승패를 가를 정도로 중요하기 때문에, 이것을 막기위해 탐색깊이를 제한했다.</p>
<p><strong>보상신호 강화</strong></p>
<p>탐색깊이를 제한했지만, 깊이가 깊어질 수록 노드의 개수가 10배씩 증가하고, 보상신호는 1/10씩 감소하기 때문에,
깊이 6정도에서도 보상신호는 매우 약해졌다. 즉 현재 상태에서 좋은 행동과 나쁜 행동의 차이를 구분하기 어려워졌다.
게다가, 탐색깊이를 제한하고, 평가함수를 사용하면서 가장 좋은 상태와 가장 나쁜 상태의 차이는 더 작아졌다.
이 문제를 완화하기 위해 보상을 평가함수의 출력을 그대로 사용하지 않고,
마지막 상태가 현재 상태보다 좋은을 때는 보상 1, 나쁠 때는 보상 0으로 사용하였다.
이 변경으로 인해 상대적인 보상의 크기가 훨씬 커지게 되었고, AI는 보다 그럴듯하게 작동하게 되었다.
다만, 마지막 상태가 현재 상태보다 좋기만하다면 모두 동일하게 보상 1을 받기 때문에 더 좋은 수와 덜 좋은 수를 구분하지는
못하게 되었다.</p>
<p>탐색깊이 제한과 보상신호를 강화하기 전에 MCTS는 너무 낮은 보상신호 때문에, 종종 매우 나쁜 수를 두고는 했다.
한 게임 동안에 반드시 몇 번을 그런 수를 두기 때문에 승률을 높이기 어려웠다.
하지만, 개선한 MCTS는 훨씬 안정적이고 그럴듯하게 작동했다.</p>
<p>구현한 MCTS 예제의 성능을 평가하기 위해 Stockfish와 비교하였다.
Stockfish는 비록 Microchess 용으로 개발된 AI도 아니고, 승리/패배 규칙도 경진대회 규칙과 다르지만,
상당히 강력한 성능을 보인다. 다른 예제 AI는 Stockfish를 상대로 0.3 이상의 승률을 보이기 어렵다.
게임을 잘 플레이한 경우에도 Stockfish를 상대로 승리하기는 어렵고, 겨우 비기는 경우였다.</p>
<p>하지만, MCTS는 Stockfish를 상대로 평균(40게임) 0.562의 승률을 보였다. White로 20게임 Black으로 20게임을 했는데,
White일 때는 0.85, Black일 때는 0.275를 기록했다. 일정 수준의 실력을 보이는 AI끼리는 누가봐도 명백한 실수를 하지 않기
때문에 먼저시작하는 White로 할 때 이기고, 나중에 두는 Black일 때 진다.
따라서 평균 승률을 높이기 위해서는 White일때 확실히 이겨야 하고, Black일 때 비기거나 져야 한다.
MCTS도 Black일 때 이견 경우는 20게임 중 불과 2게임에 불과하다.</p>
<p>Stockfish는 체스에 대한 전문적인 지식을 가지고 설계된 매우 정교한 평가함수와
매우 빠른 탐색 알고리즘을 사용하기 때문에 매우 가벼우면서 높은 성능을 가지고 있다.
하지만, 이 경진대회 처럼 게임의 규칙이 바뀐 경우, 여기에 대처하도록 개선하는 것은 쉽지 않다.
Microchess는 일반 체스와 유사하기 때문에, 어느 정도 성능을 보일 수 있었지만, 만약 전혀 다른 보드 게임이라면
Stockfish가 사용하는 정교한 평가함수는 전혀 사용할 수 없을 것이기 때문에, 일정 규모 이상의 문제에서는 빠르고 효율적인
탐색 알고리즘으로도 해결하기 어려울 것이다.</p>
<p>반면 MCTS는 전문적인 지식에 거의 의존하지 않는다. 비록 예제 MCTS에서는 문제의 규모를 축소하기 위해
간단한 평가함수를 도입했지만, 이것은 매우 간단하기 때문에 다른 문제에서도 쉽게 이 수준의 평가함수를 구현할 수 있다.
그 대신에 MCTS는 수 많은 시뮬레이션으로 근사해를 찾아낸다.
실제로 예제 MCTS가 높은 성능을 내기 위해서는 사용할 수 있는 모든 시간동안 계속 시뮬레이션을 해야만 했다.
경진대회에서 한 턴에 약 10초를 가정했기 때문에, MCTS는 10초동안 약 7,000~12,000번의 시뮬레이션을 수행했다(Intel i7-7700).
만약 시간이 반 이하로 주어지거나, 실행하는 PC의 사양이 낮다면, MCTS가 Stockfish를 상대로 높은 승률을 보이기는 어려울 것이다.</p>
<div class="section" id="id4">
<h3>MCTS 시각화<a class="headerlink" href="#id4" title="제목 주소">¶</a></h3>
<p>MCTS AI는 10초 시간제한으로 <a class="reference internal" href="agents.search.html#agents.search.mcts_agent.MCTSAgent" title="agents.search.mcts_agent.MCTSAgent"><code class="xref py py-class docutils literal notranslate"><span class="pre">agents.search.mcts_agent.MCTSAgent</span></code></a> 와 5000회 시뮬레이션 횟수 제한으로 실행되는
<a class="reference internal" href="agents.search.html#agents.search.mcts_agent.MCTSAgentDev" title="agents.search.mcts_agent.MCTSAgentDev"><code class="xref py py-class docutils literal notranslate"><span class="pre">agents.search.mcts_agent.MCTSAgentDev</span></code></a> 가 있다.
MCTSAgentDev가 작동하기 전에 visdom server를 실행시켜두면 <a class="reference internal" href="#mcts-visualization"><span class="std std-ref">MCTS 의사결정과정</span></a> 을 볼 수 있다.
시각화 기능에도 무시하기 어려운 계산비용이 필요하기 때문에, 기본적으로 개발용 버전인 MCTSAgentDev만 기능이 활성화 되어있다.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># visdom 서버 실행
(mchess) ~/ python -m visdom.server

# 다른 콘솔 창에서
(mchess) ~/ python scripts/run_game.py --white=mcts_dev --black=mcts
</pre></div>
</div>
<div class="figure" id="id10" style="width: 600px">
<span id="mcts-visualization"></span><img alt="_images/mcts_visualization.png" src="_images/mcts_visualization.png" />
<p class="caption"><span class="caption-text">MCTS 의사결정과정</span></p>
</div>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td><em>(<a class="fn-backref" href="#id1">1</a>, <a class="fn-backref" href="#id2">2</a>)</em> Browne, Cameron B., et al. &quot;A survey of monte carlo tree search methods.&quot; IEEE Transactions on Computational Intelligence and AI in games 4.1 (2012): 1-43.</td></tr>
</tbody>
</table>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">목차</a></h3>
  <ul>
<li><a class="reference internal" href="#">Monte Carlo Tree Search</a><ul>
<li><a class="reference internal" href="#mcts">MCTS 개요</a></li>
<li><a class="reference internal" href="#multi-armed-bandit">Multi-Armed Bandit</a></li>
<li><a class="reference internal" href="#upper-confidence-tree">Upper Confidence Tree</a></li>
<li><a class="reference internal" href="#id3">MCTS 예제</a><ul>
<li><a class="reference internal" href="#id4">MCTS 시각화</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>이전 항목</h4>
  <p class="topless"><a href="03-basic_ai_examples.html"
                        title="이전 장">기본 AI 예제</a></p>
  <h4>다음 항목</h4>
  <p class="topless"><a href="05-self_learning.html"
                        title="다음 장">Self Learning AI</a></p>
  <div role="note" aria-label="source link">
    <h3>현재 문서</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/04-mcts.rst.txt"
            rel="nofollow">소스 코드를 보려면</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>빠른 검색</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="바로 가기" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>탐색</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="전체 색인"
             >색인</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python 모듈 목록"
             >모듈</a> |</li>
        <li class="right" >
          <a href="05-self_learning.html" title="Self Learning AI"
             >다음</a> |</li>
        <li class="right" >
          <a href="03-basic_ai_examples.html" title="기본 AI 예제"
             >이전</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Microchess AI Competition 0.1.0 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2018, NCSOFT, Game AI Lab, Game AI Team.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.7.2.
    </div>
  </body>
</html>